{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "49f653d1-6cad-40d1-ad20-fdd8ea975e02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in /opt/anaconda3/lib/python3.11/site-packages (0.3.20)\n",
      "Requirement already satisfied: tiktoken in /opt/anaconda3/lib/python3.11/site-packages (0.9.0)\n",
      "Requirement already satisfied: langchain-openai in /opt/anaconda3/lib/python3.11/site-packages (0.3.11)\n",
      "Requirement already satisfied: langchainhub in /opt/anaconda3/lib/python3.11/site-packages (0.1.21)\n",
      "Requirement already satisfied: chromadb in /opt/anaconda3/lib/python3.11/site-packages (0.6.3)\n",
      "Requirement already satisfied: langchain in /opt/anaconda3/lib/python3.11/site-packages (0.3.23)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /opt/anaconda3/lib/python3.11/site-packages (from langchain_community) (0.3.51)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.11/site-packages (from langchain_community) (2.0.25)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.11/site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from langchain_community) (3.9.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from langchain_community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/lib/python3.11/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from langchain_community) (2.8.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /opt/anaconda3/lib/python3.11/site-packages (from langchain_community) (0.3.24)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /opt/anaconda3/lib/python3.11/site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.11/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-openai) (1.69.0)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/lib/python3.11/site-packages (from langchainhub) (24.2)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from langchainhub) (2.32.0.20250328)\n",
      "Requirement already satisfied: build>=1.0.3 in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (2.11.0)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (0.115.12)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/anaconda3/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (3.23.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (4.13.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (1.31.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (4.65.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (0.15.2)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (32.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (3.10.16)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from chromadb) (13.3.5)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/anaconda3/lib/python3.11/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.3)\n",
      "Requirement already satisfied: pyproject_hooks in /opt/anaconda3/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /opt/anaconda3/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (0.46.1)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (3.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (0.58.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /opt/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
      "Requirement already satisfied: durationpy>=0.7 in /opt/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain_community) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: coloredlogs in /opt/anaconda3/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/anaconda3/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.8.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.31.1 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.31.1 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.52b1 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.52b1 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.52b1 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
      "Requirement already satisfied: asgiref~=3.0 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (0.4.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (2.0.4)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/anaconda3/lib/python3.11/site-packages (from tokenizers>=0.13.2->chromadb) (0.29.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/anaconda3/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/anaconda3/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/anaconda3/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.11/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain_community) (2.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/anaconda3/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74c545c-dea4-4c69-9f56-6312b2a1a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"ResearchPro2\" \n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91031f9b-2e75-4c2e-80c3-4b869e737174",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = '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'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbd3bda-aabe-4f71-a3df-0628d4cc6059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a00bc98-63f1-4fbb-a8e3-54dac5308dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-huggingface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c1196-f3fa-4068-a7ff-a9ad615a48a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cb0806f-262c-45c0-915a-94795dbc9468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hell.\n"
     ]
    }
   ],
   "source": [
    "a=\"hell.pdf\"\n",
    "print(a.replace(\"pdf\",\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170e3927-529a-457f-8429-c9f646d2f73b",
   "metadata": {},
   "source": [
    "\n",
    "<font size=\"4\">Getting 20 research papers and creating a vector DB</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1666d07c-28a8-4d6c-ab74-b0cbccde19c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5n/8m4t00412h97v592vh5y0pgc0000gn/T/ipykernel_1849/1707532346.py:26: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  results = list(search.results())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 20 papers\n",
      " Already downloaded: 2503.02141v1.pdf\n",
      " Already downloaded: 2403.09418v1.pdf\n",
      " Already downloaded: 2306.02224v1.pdf\n",
      " Already downloaded: 2501.18648v2.pdf\n",
      "Downloading A Survey on Large Language Models from Concept to Implementation\n",
      "Failed to download http://arxiv.org/pdf/2403.18969v2: 404 Client Error: NOT FOUND for url: http://arxiv.org/pdf/2403.18969v2\n",
      " Already downloaded: 2410.16349v1.pdf\n",
      " Already downloaded: 2402.00066v1.pdf\n",
      " Already downloaded: 2310.01415v3.pdf\n",
      "Downloading GPT Carry-On: Training Foundation Model for Customization Could Be Simple, Scalable and Affordable\n",
      "Saved: 2504.07513v1.pdf\n",
      " Already downloaded: 2408.04646v2.pdf\n",
      " Already downloaded: 2310.07713v3.pdf\n",
      " Already downloaded: 2409.02111v1.pdf\n",
      " Already downloaded: 2311.07621v1.pdf\n",
      " Already downloaded: 2407.13228v1.pdf\n",
      " Already downloaded: 2311.05842v1.pdf\n",
      " Already downloaded: 2311.06025v3.pdf\n",
      " Already downloaded: 2412.12163v1.pdf\n",
      " Already downloaded: 2411.06691v1.pdf\n",
      " Already downloaded: 2310.10449v2.pdf\n",
      " Already downloaded: 2310.01434v1.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from langchain_community.document_loaders import ArxivLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# -------------- SETUP --------------\n",
    "import arxiv\n",
    "\n",
    "PDF_DIR = \"./data/papers\"\n",
    "\n",
    "os.makedirs(PDF_DIR, exist_ok=True)\n",
    "\n",
    "# -------------- STEP 1: SEARCH ARXIV --------------\n",
    "# Search up to 20 results\n",
    "search = arxiv.Search(\n",
    "    query=\"('Large Language Models' OR LLM OR GPT OR Transformers OR 'Foundation Models') \"\n",
    "    \"OR ('Deep Learning' OR 'Neural Networks') \"\n",
    "    \"OR ('Machine Learning')\",\n",
    "    max_results=20,\n",
    "    sort_by=arxiv.SortCriterion.Relevance,\n",
    ")\n",
    "\n",
    "results = list(search.results())\n",
    "print(f\"Retrieved {len(results)} papers\")\n",
    "\n",
    "\n",
    "# -------------- STEP 2: DOWNLOAD PDFs --------------\n",
    "downloaded_files = []\n",
    "for doc in results:\n",
    "    title = doc.title\n",
    "    pdf_url = doc.pdf_url\n",
    "    arxiv_id = doc.entry_id.split(\"/\")[-1]  # Extracts ID from https://arxiv.org/abs/2404.12345\n",
    "    filename = f\"{arxiv_id}.pdf\"\n",
    "    pdf_path = os.path.join(PDF_DIR, filename)\n",
    "\n",
    "    if os.path.exists(pdf_path):\n",
    "        print(f\" Already downloaded: {filename}\")\n",
    "        downloaded_files.append(pdf_path)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"Downloading {title}\")\n",
    "        response = requests.get(pdf_url)\n",
    "        response.raise_for_status()\n",
    "        with open(pdf_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        downloaded_files.append(pdf_path)\n",
    "        print(f\"Saved: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {pdf_url}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f9ddf4-14c6-45df-84b5-ab3d38950849",
   "metadata": {},
   "source": [
    "<font size=\"4\">Loading Docs and creating Vector DB</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "575639c3-5700-4c40-a893-7cc523ce0a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading all PDFs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 61 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents...\n",
      "Embedding and creating Chroma DB...\n",
      "\n",
      " VectorDB created with 1494 chunks!\n"
     ]
    }
   ],
   "source": [
    "# -------------- STEP 3: LOAD ALL PDFs --------------\n",
    "print(\"\\nLoading all PDFs...\")\n",
    "all_docs = []\n",
    "downloaded_folder = \"./data/papers\"\n",
    "\n",
    "# Get full paths of all PDF files in the folder\n",
    "downloaded_files = [os.path.join(downloaded_folder, f) for f in os.listdir(downloaded_folder) if f.endswith(\".pdf\")]\n",
    "\n",
    "for path in downloaded_files:\n",
    "    try:\n",
    "        loader = PyPDFLoader(path)\n",
    "        docs = loader.load()\n",
    "        \n",
    "        filename = os.path.basename(path)\n",
    "        arxiv_id = filename.replace(\".pdf\", \"\")\n",
    "        arxiv_url = f\"https://arxiv.org/pdf/{arxiv_id}\"\n",
    "\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = arxiv_url\n",
    "        all_docs.extend(docs)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {path}: {e}\")\n",
    "\n",
    "# -------------- STEP 4: SPLIT TEXT --------------\n",
    "print(\"Splitting documents...\")\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer - choose one closest to your Ollama model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-hf\")  # or the model you used\n",
    "\n",
    "# Define a tokenizer function as expected by LangChain\n",
    "def token_length_function(text):\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# Now use LangChain's TokenTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    length_function=token_length_function,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(all_docs)\n",
    "\n",
    "\n",
    "# -------------- STEP 5: EMBEDDINGS + VECTOR DB --------------\n",
    "print(\"Embedding and creating Chroma DB...\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_db = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=\"./s2_chroma_db\")\n",
    "\n",
    "# Optional: Persist the DB\n",
    "vector_db.persist()\n",
    "\n",
    "print(f\"\\n VectorDB created with {len(chunks)} chunks!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581294c5-a346-4012-acfc-11e49756ca32",
   "metadata": {},
   "source": [
    "\n",
    "<font size=\"4\">Loading from Stored DB</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "125102fe-758f-4a87-8b85-6337d2c55de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# Load the persisted DB\n",
    "vector_db = Chroma(persist_directory=\"./arxiv_vector1_db\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f069ff4-6ca0-4a82-8298-6a405759060e",
   "metadata": {},
   "source": [
    "<font size=\"4\">Adding a doc to db </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbd4c3f7-372b-4f42-a966-0fdc64f1efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "def add_pdf_to_vector_db(pdf_path: str):\n",
    "    \"\"\"\n",
    "    Loads a PDF from the given path, splits it into chunks,\n",
    "    and adds the chunks to the existing vector_db.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the PDF\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents = loader.load()\n",
    "\n",
    "        # Split using existing text_splitter\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "        # Add to vector database\n",
    "        vector_db.add_documents(chunks)\n",
    "\n",
    "        print(f\"Successfully added {pdf_path} to the vector database.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to add {pdf_path}: {e}\")\n",
    "\n",
    "add_pdf_to_vector_db(\"./data/papers/2501.09136.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1631c860-1f80-4900-bf75-c8c7ed37f70f",
   "metadata": {},
   "source": [
    "<font size=\"4\">Implement MultiQueryRetriever</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a9a1835-359b-4005-bbf1-faf38866f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MultiQueryRetriever\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import BaseOutputParser, StrOutputParser\n",
    "from typing import List\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "import re\n",
    "\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        # Find all lines that start with a digit followed by a period (1. 2. etc.)\n",
    "        lines = re.findall(r\"^\\d+\\.\\s+(.*)\", text, re.MULTILINE)\n",
    "        return [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "\n",
    "query_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI assistant. Rephrase the question in 5 different ways to improve retrieval from a document store. Separate each by a new line.\\nQuestion: {question}\"\"\"\n",
    ")\n",
    "\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "llm_chain = query_prompt | llm | LineListOutputParser()\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "retriever = MultiQueryRetriever(\n",
    "    retriever=vector_db.as_retriever(),\n",
    "    llm_chain=llm_chain,\n",
    "    parser_key=\"lines\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "953c3a12-fb48-405b-8da6-56d024da7cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "# 📄 Format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# 📝 RAG Prompt\n",
    "rag_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are an AI assistant answering user questions based on retrieved documents.\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# 🔗 RAG Chain\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,  # Retrieval with formatting\n",
    "        \"question\": RunnablePassthrough(),   # Pass through the original question\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def run_rag(question: str) -> str:\n",
    "    return rag_chain.invoke(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d21329d2-e3c6-4496-90a7-9d728858933e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agentic RAG refers to an integrated approach to AI systems that combines the principles of systems thinking with the capabilities of agentic AI. It advocates for embedding AI agents within a cohesive system where every component interacts synergistically with others, promoting a collaborative environment where human intuition and ethical reasoning complement AI's analytical capabilities.\n"
     ]
    }
   ],
   "source": [
    "response = run_rag(\"What is Agentic RAG?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e3a0e-ccfb-4cb6-9e9d-ca5b4b029835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embd = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "query_result = embd.embed_query(question)\n",
    "document_result = embd.embed_query(document)\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "03cb3412-698b-498c-a808-bc6d9ccbde40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.5782658187502142\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result)\n",
    "print(\"Cosine Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee339518-34cd-4115-a040-185cbc2f5c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING ####\n",
    "\n",
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4d7bf-7567-462c-9182-bb7092726002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bbb5e6-6e95-40e6-934a-3b77c39f1198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de46854b-b222-4606-ac33-0fd48300eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18345b16-f565-4603-8a1b-f8d000c0b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62577af-65a0-46aa-921e-c566b80b0d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e11e7d5-d9f4-4771-94b3-38153cc5a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f6cef6-8fb3-43fb-8bf2-d7fdde4dd864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "chain.invoke({\"context\":docs,\"question\":\"What is Task Decomposition?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb03258-00fd-4cbd-bdf2-b5a899ea7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f3c658-3925-4502-87c5-c8f309f3c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_hub_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0e4de08-bf51-4dae-bdf3-7a0f62fb3f04",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformat_docs\u001b[39m(docs):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs)\n\u001b[1;32m      7\u001b[0m rag_chain \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      8\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: retriever \u001b[38;5;241m|\u001b[39m format_docs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: RunnablePassthrough()}\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;241m|\u001b[39m prompt\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m|\u001b[39m llm\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m rag_chain\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is Task Decomposition?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c90d857d-a248-4a21-a713-33ca8c002092",
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Multi Query\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import OllamaLLM\n",
    "llm = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | llm \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f1bb0d38-24bb-4a76-b8ce-3e9af75aeebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "question = \"who am I  ?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0c9bce87-edfb-43cf-8226-ae14a7e49b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Queries:\n",
      "1. Here are five alternative versions of the original question \"who am I?\" to retrieve relevant documents from a vector database:\n",
      "2. \n",
      "3. 1. What are my characteristics and attributes?\n",
      "4. \n",
      "5. 2. Can you provide information about my identity and individuality?\n",
      "6. \n",
      "7. 3. How can I learn more about myself and my place in the world?\n",
      "8. \n",
      "9. 4. What are some common questions that people ask themselves when trying to understand their own identity?\n",
      "10. \n",
      "11. 5. Are there any resources or tools available that can help me discover more about who I am?\n"
     ]
    }
   ],
   "source": [
    "# Generate and print the multiple queries\n",
    "queries = generate_queries.invoke({\"question\": question})\n",
    "\n",
    "# Display each generated query\n",
    "print(\"Generated Queries:\")\n",
    "for i, q in enumerate(queries, 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7de500da-978f-4029-84cc-fccf5834796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abc65521-06c5-4caf-a28c-4ca2b21099dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MultiQueryRetriever\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from typing import List\n",
    "\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        # Find all lines that start with a digit followed by a period (1. 2. etc.)\n",
    "        lines = re.findall(r\"^\\d+\\.\\s+(.*)\", text, re.MULTILINE)\n",
    "        return [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "query_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI assistant. Rephrase the question in 5 different ways to improve retrieval from a document store. Separate each by a new line.\\nQuestion: {question}\"\"\"\n",
    ")\n",
    "\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.2\")\n",
    "llm_chain = query_prompt | llm | LineListOutputParser()\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "retriever = MultiQueryRetriever(\n",
    "    retriever=vector_db.as_retriever(),\n",
    "    llm_chain=llm_chain,\n",
    "    parser_key=\"lines\"  # matches the output parser\n",
    ")\n",
    "question = \"What are the key concepts behind Agentic RAG systems?\"\n",
    "\n",
    "# Get relevant documents using multi-query retrieval\n",
    "relevant_docs = retriever.invoke(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "883bdba8-c2ac-487b-a4f5-0af5717ad368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'creationdate': '2024-10-15T01:15:48+00:00', 'creator': 'LaTeX with acmart 2023/03/30 v1.90 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'moddate': '2024-10-15T01:15:48+00:00', 'page': 1, 'page_label': '2', 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'https://arxiv.org/pdf/2410.09942v1', 'subdomain': 'information_retrieval', 'subject': '', 'title': 'Learning to Rank for Multiple Retrieval-Augmented Models through Iterative Utility Maximization', 'total_pages': 12, 'trapped': '/False'}, page_content='𝑖 = {(𝑥′\\n𝑗,𝑦′\\n𝑗)}|𝐷test\\n𝑖 |\\n𝑗=1\\nin the same order. The end-to-end performance of the agent𝑀𝑖 can\\nbe measured by a utility function (metric) 𝜇𝑖.\\nEach RAG agent can be simply formulated as ¯𝑦𝑀𝑖 = 𝑀𝑖(𝑥; 𝑅𝜃).\\nIn more detail, given an input 𝑥, each RAG agent 𝑀𝑖 submits a\\nquery to the retrieval model 𝑅𝜃 for information access, and gener-\\nates the output by consuming the top 𝑘 retrieved documents (i.e.,\\nR𝑘 = {𝑟1,...,𝑟 𝑘}). As suggested in [42], the search engine and RAG\\nagents can engage in an offline optimization process, in which each\\nRAG agent 𝑀𝑖 produces a feedback list for a retrieval list of size 𝑘,\\ndenoted as 𝑓𝑀𝑖 = {𝑓𝑗}𝑘\\n𝑗=1 ∈{0,1}1×𝑘. This feedback list indicates\\nthe usefulness of each retrieved document in the retrieval list from\\nthe agent’s perspective. Feedback can be computed in various ways;\\nbased on the performance of the generated output when utilizing\\nthe retrieved documents on the downstream task, e.g., using the\\nutility function 𝜇𝑖, or based on ratings received from the users or\\nevaluators of the RAG agent, among other methods. Without loss of'),\n",
       " Document(metadata={'author': 'IEEE', 'company': 'IEEE', 'creationdate': '2010-02-25T11:55:07+07:00', 'creator': 'Acrobat PDFMaker 8.1 for Word', 'moddate': '2010-03-04T22:32:39+05:00', 'page': 1, 'page_label': '2', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'source': './arxiv_data/data_preprocessing/1003.1792v1_A_Hybrid_System_based_on_Multi-Agent_System_in_the.pdf', 'sourcemodified': 'D:20100225045425', 'sub_domain': 'data_preprocessing', 'title': 'Paper Title (use style: paper title)', 'total_pages': 4}, page_content='(IJCSIS) International Journal of Computer Science and Information Security,  \\nVol. 7, No. 2, 2010 \\n \\n[15]. \\nwith business tasks such as planning, reasoning and learning. \\nAlso, data mining techniques is the important way to make a \\nreason for agent under uncertainty and with incomplete \\ninformation situations. Notwithstanding, data preprocessing \\nstep acts as an crucial task to filter and select suitable \\ninformation before processing any mining algorithms.     \\nIII. THE METHODOLOGIES \\nIn this section, we illustra te the specified methodologies \\nused in this project but it is only focused on the approaches \\nusing in the data preprocessing st age specified in dealing with \\nmissing values.  \\nA. Multi-Agent System  \\nAgent is the software program that enables to autonomous \\naction in some environment so as to meet its design objectives. \\nAccording to N. R. Jennings and M. Wooldridge [12], the \\nessential characters of each agent are following: reactive, pro-\\nactive, autonomous, object-oriented and social ability. Each \\nagent can play as a behalf of the user and execute the \\nparticular task. Also, Padghan and Winikopff [13] described \\nthat the concept of Agent refers to an entity acting on behalf of \\nother entities or organizations and having the ability to \\nperceive relevant information, and following and handling the \\nobjectives to be accomplished. However, in open and dynamic \\nenvironment like internet, a multi-agent system is one of the \\npromising means to help reduce cost, increase efficiency, \\nreduce errors and achieve optimal deal.  \\n There are two issues related to the design of MAS: Agent \\nCommunication Language and agent development platform.'),\n",
       " Document(metadata={'author': '', 'creationdate': '2024-12-25T01:19:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-12-25T01:19:55+00:00', 'page': 4, 'page_label': '3', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'https://arxiv.org/pdf/2412.18143v1', 'subdomain': 'databases', 'subject': '', 'title': '', 'total_pages': 29, 'trapped': '/False'}, page_content='e = v, w∈ E is simply a set of two vertices, whereas if we want to have a directed graph we\\ntake e to be a tuple whose first element is the source and the second element is the target vertex.\\nThe definition of a simple graph can be extended to model more complex kinds of graphs. A\\nweighted graph can be modeled by a triple V, E, w, where w : E 7→ R is function that gives the\\nweight of a given edge.2 A hypergraph H generalizes a simple graphG allowing any of its edges\\nto be a set of any number of vertices.\\nWe will give here the abstract definitions of the most popular graph models used by NoSQL\\nstores in practice: the labeled property graph (LPG) and the resource description framework\\n(RDF). We follow [4, §2.1], but [5, §2] gives similar definitions.\\nThe LPG model is an extension of the simple graph model that adds metadata (labels and\\nproperties) to vertices and edges. Properties are key-value pairs, while labels are simple scalar\\nvalues. A LPG G is a tuple\\n(V, E, L, lV , lE, K, W, pV , pE)\\nwhere:\\n• L is the set of labels;\\n• lV : V 7→ P(L) and lE : E 7→ P(L) are the labeling functions for vertices and edges\\nrespectively (P is the powerset function);\\n• K is the set of property keys;\\n• W is the set of property values;\\n• a property p is a pair (k, v), with k ∈ K and v ∈ W;\\n• pV : V 7→ P(K × W) and pE : E 7→ P(K × W) are the functions that give the set of\\nproperties of a given node and edge, respectively.\\nRDF 3 is a data model specified by the W3C and is at its core a collection of triples. A triple\\nis composed by a subject, a predicate, and an object. A subject s may be an identifier or a blank\\nnode (essentially a dummy identifier), while a predicate p is always an identifier, and an object\\no may be an identifier, a blank nodes, or a literal (a value like a string or a number). A triple is'),\n",
       " Document(metadata={'author': 'Administrator', 'company': 'Northumbria University', 'creationdate': '2012-05-17T12:43:17+01:00', 'creator': 'Acrobat PDFMaker 10.0 for Word', 'moddate': '2012-05-17T12:44:33+01:00', 'page': 75, 'page_label': '76', 'producer': 'Adobe PDF Library 10.0', 'source': 'https://arxiv.org/pdf/1510.00819v1', 'sourcemodified': 'D:20120517113725', 'subdomain': 'information_retrieval', 'title': 'Intelligent Search Optimization using Artificial fuzzy logic', 'total_pages': 184}, page_content='wrapper induction’, Agents. Available \\nat: http://www.isi.edu/integration/papers/muslea99-agents.pdf (Accessed: 13 \\nMarch 2012). \\n \\n\\uf0a7 Munibalaji, T. and Balamurugan, C. (2012) ‘Analysis of link algorithms for \\nweb mining’, International Journal of Engineering and Innovative \\nTechnology. 1( 2). Available \\nat: http://www.ijeit.com/vol%201/Issue%202/IJEIT1412201202_16.pdf \\n(Accessed: 23 March 2012). \\n \\n\\uf0a7 Odom, S. and Allisson, L. (2012) SEO for 2012: Search engine Optimizat ion \\nSecrets.  4th edn. London: MediaWorks Publishing. \\n \\n\\uf0a7 Perkowitz, M. and  Etaioni, O . (1995) ‘Category translation: learning to \\nunderstand information on the internet ’, In Proceeding 15th International \\nJoint Conference on AI.  Montral, Canada.  Available \\nat: http://www.cs.washington.edu/homes/etzioni/papers/ijcai95.pdf  \\n(Accessed: 14 March 2012).'),\n",
       " Document(metadata={'creationdate': '2024-10-15T01:15:48+00:00', 'creator': 'LaTeX with acmart 2023/03/30 v1.90 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'moddate': '2024-10-15T01:15:48+00:00', 'page': 7, 'page_label': '8', 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'https://arxiv.org/pdf/2410.09942v1', 'subdomain': 'information_retrieval', 'subject': '', 'title': 'Learning to Rank for Multiple Retrieval-Augmented Models through Iterative Utility Maximization', 'total_pages': 12, 'trapped': '/False'}, page_content='captures how the ranking order of documents correlates between\\ntwo lists, providing insights into how similarly or differently the\\ndocuments are ranked for various agents. Jaccard’s similarity, on\\nthe other hand, is a set-based metric that quantifies the overlap\\nbetween two sets of retrieved documents, indicating the percentage\\nof shared documents across retrieval lists for different agents. These\\nmetrics allow us to analyze both the ranking and the content of the\\nretrieved documents across personalized search engine settings.\\nThe results of this experiment are shown in Figure 3, yielding sev-\\neral key insights. First, the findings suggest that, on average, about\\n20% of the retrieved documents differ between RAG agents for the\\nsame query. This highlights the personalization effect, where each\\nRAG agent receives a distinct set of documents despite querying\\nwith identical input. Furthermore, the low Kendall’s tau correlation\\nindicates significant differences in the ranking of the documents\\nretrieved for different RAG agents, demonstrating that the search\\nengine adapts document ranking based on agent-specific prefer-\\nences and behaviors. Additionally, the Jaccard’s similarity between\\nagents employing FiD and those using in-prompt augmentation\\nis notably lower than between agents both utilizing in-prompt\\naugmentation. This demonstrates that the system has effectively\\nlearned to tailor document retrieval strategies according to the\\ndifferent retrieval-augmentation methods used by the RAG agents.\\nAnother interesting observation is that the Kendall’s tau cor-\\nrelation is higher between agents that both using T5 or FiD with\\nT5 than between agents where one employs BART and the other\\nutilizes T5 or FiD with T5. This suggests that the search engine,\\nthrough model ID, has identified shared information needs between\\nagents that utilize the same backbone language model (T5), result-\\ning in more similar retrieval outputs. In summary, these results\\nconfirm that personalization significantly affects the retrieval lists\\nprovided to each agent, as the system learns to adapt document\\nrankings and selections based on both the retrieval-augmentation\\nmethod and the backbone LLM.\\n6 CONCLUSION & FUTURE WORK\\nIn this paper, we address the challenge of building a search en-\\ngine tailored for multiple RAG agents, functioning similarly to how\\nsearch engines serve human users, considering the paradigm shift\\nwhere nowadays LLMs are the main users of the search engines.\\nWe propose IUM, an iterative framework with expectation maxi-\\nmization to iteratively gather feedback from RAG agents and adjust'),\n",
       " Document(metadata={'author': 'w10037359', 'creationdate': '2015-09-28T22:24:12+05:30', 'creator': 'Microsoft® Office Word 2007', 'moddate': '2015-09-28T22:24:12+05:30', 'page': 6, 'page_label': '7', 'producer': 'Microsoft® Office Word 2007', 'source': 'https://arxiv.org/pdf/1509.08396v1', 'subdomain': 'information_retrieval', 'total_pages': 7}, page_content='The 6th Conference on Software, Knowledge, Information Management and Applications, Chengdu, China, September 9-11 2012, #57.  \\n \\n \\n7 \\n[8] Cohen, S., J. Mamou, et al. (2003). XSEarch: a semantic search \\nengine for XML. Proceedings of the 29th international conference \\non Very large data bases - Volume 29 . Berlin, Germany, VLDB \\nEndowment: 45-56. \\n[9] David Vi ney, http://www.amazon.com/Get -Top-Google-\\nTechniques-Rankings/dp/1857885023#reader_1857885023 \\n[10] Dreilinger, D. and A. E. Howe (1997). \"Experiences with selecting \\nsearch engines using metasearch.\" ACM Trans. Inf. Syst.  15(3): \\n195-222. \\n[11] Emtage, A. (1993). Archie Intelligent Information Retrieval: The \\nCase of Astronomy and Related Space Sciences. A. Heck and F. \\nMurtagh, Springer Netherlands. 182: 103-111. \\n[12] Fan, Y. and Gauch, S. (1999), “Adaptive agents for information \\ngathering from multiple, distributed informati on sources”, \\navailable at:www.ittc.ukans.edu/,sgauch/papers/AAAI99.doc \\n[13] Ferragina, P. and A. Gulli (2008). \"A personalized search engine \\nbased on Web -snippet hierarchical clustering.\" Software: Practice \\nand Experience 38(2): 189-225. \\n[14] Funkhouser, T., P. Min, et al. (2003). \"A search engine for 3D \\nmodels.\" ACM Trans. Graph. 22(1): 83-105. \\n[15] Gauch “ProFusion: Intelligent Fusion from Multiple, Distributed \\nSearch Engines,” Susan Gauch, Guijun Wang and Mario Gomez, \\nJournal of Universal Computer Science , Vol. 2, No. 9, September \\n1997, 637-649. \\n[16] Gray, J. (2012). \"Interview with Dr Stephen Wolfram.\" Linux J. \\n2012(214): 5. \\n[17] Harley Hahn and Rick Stout. The Internet Complete Reference. \\nOsborne McGraw-Hill, Berkeley, California, 1994.'),\n",
       " Document(metadata={'creationdate': '2024-10-15T01:15:48+00:00', 'creator': 'LaTeX with acmart 2023/03/30 v1.90 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'moddate': '2024-10-15T01:15:48+00:00', 'page': 7, 'page_label': '8', 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'https://arxiv.org/pdf/2410.09942v1', 'subdomain': 'information_retrieval', 'subject': '', 'title': 'Learning to Rank for Multiple Retrieval-Augmented Models through Iterative Utility Maximization', 'total_pages': 12, 'trapped': '/False'}, page_content='search engines serve human users, considering the paradigm shift\\nwhere nowadays LLMs are the main users of the search engines.\\nWe propose IUM, an iterative framework with expectation maxi-\\nmization to iteratively gather feedback from RAG agents and adjust\\nthe search engine based on them in an offline and online phase.\\nOur findings demonstrate that the proposed approach statistically\\nsignificantly outperforms established baselines in terms of average\\nagents performance. We also conducted extensive studies to analyze\\nthe impact of key factors such as the number of training iterations\\nin Offline IUM, batch size in Online IUM, and the role of per-\\nsonalization in search results for each agent. Overall, the proposed\\nmethod exhibits promising results, showcasing its effectiveness in\\ndesigning search engines for multiple RAG agents.\\nThere are several potential future directions for this work: (1)\\nextending the current setup to optimize retrieval models rather\\nthan just reranking; (2) considering multiple utility functions per\\nagent; (3) investigating novel regularization techniques to enhance'),\n",
       " Document(metadata={'author': 'w10037359', 'creationdate': '2015-09-28T22:24:12+05:30', 'creator': 'Microsoft® Office Word 2007', 'moddate': '2015-09-28T22:24:12+05:30', 'page': 1, 'page_label': '2', 'producer': 'Microsoft® Office Word 2007', 'source': 'https://arxiv.org/pdf/1509.08396v1', 'subdomain': 'information_retrieval', 'total_pages': 7}, page_content='Using intelligent agents to retrieve search results were \\ndiscussed in [23]. Selecting most promising search engine for \\ninformation retrieval, Savvy Search [10]  was developed in \\nwhich, query was send parallel to 2 -3 search engines but \\nresults displayed were not merged. Neural network based \\nintelligent Meta search engines [1 6], [25]. The ProFusion \\nSystem [12] supports automatic and manual query \\ndispatching, combing several features of other Meta search \\nengines.  \\n \\nExperimental Meta search engines focus on system \\narchitecture su ch as sorting algorithms [21 ] and adaptive \\nbehavior [12]. Work of [3] uses two models of Meta search \\nengine. Their study was based  to enhancing the performance \\nof Meta search engines using two different algorithms, Bord a-\\nfuse and Bayes -fuse. ProThes by Braslavaski et al., (2004) \\nrevamped the design of Meta  search engine by including a \\nthesaurus component.'),\n",
       " Document(metadata={'author': 'Thorsten Händler', 'creationdate': '2023-10-06T00:29:27+00:00', 'creator': 'LaTeX with hyperref', 'keywords': 'Taxonomy, autonomous agents, multi-agent collaboration, large language models (LLMs), AI systems classification, alignment, software architecture, architectural viewpoints, software-design rationale, context interaction, artificial intelligence, domain-ontology diagram, feature diagram, radar chart.', 'moddate': '2023-10-06T00:29:27+00:00', 'page': 8, 'page_label': '9', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'https://arxiv.org/pdf/2310.03659v1', 'subdomain': 'artificial_intelligence', 'subject': 'cs.AI, cs.SE, cs.MA', 'title': 'Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-powered Multi-Agent Architectures', 'total_pages': 40, 'trapped': '/False'}, page_content='as formal ontologies [23], they are also devised as conceptual models to support human understanding of the\\naddressed domain [36, 24, 25].\\nOur domain ontology is represented as a conceptual model in terms of a class diagram of the Unified Modeling\\nLanguage (UML2) [55], which allows for organizing the identified concepts as classes and their relationships\\nin terms of generalizations and kinds of associations with indicated multiplicities (i.e. amount of objects\\ninvolved in a relationship). For further details on the applied diagram notations, the UML specification serves\\nas a comprehensive guide [55].\\nThe primary objective of the presented model in Fig. 4 is to mirror architectural concepts especially relevant to\\nour taxonomy’s scope. In doing so, it deliberately adopts a high-level view, abstracting from technical details\\nand specifics typical of individual systems to support clarity and accessibility. For example, while diving into\\nthe complexities of the agent’s memory usage, as for reflecting and combining task instructions or for planning\\nsteps and actions, is undoubtedly worthy of thorough exploration [59, 85], it falls outside the narrow scope of\\nour taxonomy. This approach ensures that actual multi-agent systems can be regarded as potential instances of\\nthis conceptual framework. It’s worth noting that this doesn’t preclude the addition of more specific technical\\ncomponents and mechanisms as systems evolve.\\nThe domain-ontology model derives from an examination of the code and architectural documentation of\\nseveral representative multi-agent architectures, especially AUTO GPT [77], SUPER AGI [79], and METAGPT\\n[28], the Generative Agents project [59], as well as the LANG CHAIN framework [14]. The latter serves as the\\nfoundational infrastructure for some of the assessed multi-agent systems (refer to Section 2.2). Through an\\niterative process, we analyzed these systems and frameworks to understand their components, interactions, and\\noverarching structures. This analysis facilitated the identification and abstraction of recurrent architectural\\ncharacteristics prevalent among these architectures.\\nThe concepts of the model are arranged into thematic blocks corresponding to the system characteristics\\nidentified in Section 3.1, such as G . In the following, we delve into these concepts and their main interrelations.\\nFurther details are provided in the domain-ontology diagram illustrated in Fig. 4.\\nG Concepts of Goal-driven Task Management. Typically, aHuman User initiates system operations via a'),\n",
       " Document(metadata={'author': '', 'creationdate': '2018-05-15T09:45:55+00:00', 'creator': 'LaTeX with hyperref package', 'keywords': '', 'moddate': '2018-05-15T09:45:55+00:00', 'page': 6, 'page_label': '7', 'producer': 'pdfTeX-1.40.12', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'source': 'https://arxiv.org/pdf/1307.2603v1', 'subdomain': 'databases', 'subject': '', 'title': '', 'total_pages': 16, 'trapped': '/False'}, page_content='In the literature, there are several alignment methods that can be catego-\\nrized according to techniques employed to produce alignments. The most early\\nmethods are based on the comparison of linguistic expressions [7]. Another\\naligner presented in [5] has taken into account annotations of entities deﬁned\\nin ontologies. More recently, the methods introduced in [9], [13], [10] have ex-\\nploited ontological structures related to concepts in question. These methods,\\nnamely simple alignment methods, are the most prevalent at present. They de-\\ntect simple correspondences between atomic entities (or simple concepts) (e.g.\\nHuman ⊑Person, Female ⊑Person ). As a result, some kinds of semantic het-\\nerogeneity in diﬀerent ontologies can be solved by using these classical alignment\\nmethods.\\nHowever, simple correspondences are not suﬃcient to express relationships\\nthat represent correspondences between complex concepts since (i) it may be\\ndiﬃcult to discover simple correspondences (or they do not exist) in certain cases,\\nor (ii) simple correspondences do not allow for expressing accurately relationships\\nbetween entities.\\nA second important issue is that generating a complex alignment has a certain\\nimpact during the consistency checking of the system. Indeed, a reasoner such as\\nPellet [?] or FaCT++ [ ?], running on a system consisting of two ontologies O1\\nand O2 and a simple alignment As, may reply that the system is not consistent.\\nBut, this same reasoner, with the same ontologiesO1 and O2, and with a complex\\nalignment Ac can deduce that the system is consistent.\\nConsequently, new works follow the way of complex alignment solutions such\\nas [12]. But, currently, they address the alignment of simple concept with a\\ncomplex concept, at best.\\n4 Architecture overview\\nIn this section, we present the main components of our system and highlight\\non the approaches used at each steps of the data integration processing. These\\nsteps, depicted in Fig. 3, consist of the (1) creation of an ontology associated to\\neach data sources, (2) aligning these ontologies and (3) creating a global ontology\\ngiven these correspondences.\\nFinally, we present a query language enabling to retrieve information stored\\nin the sources from a query expressed over the global ontology.\\n4.1 Source ontology generation\\nAs explained earlier, NOSQL databases are generally schemaless. Although this'),\n",
       " Document(metadata={'author': 'Thorsten Händler', 'creationdate': '2023-10-06T00:29:27+00:00', 'creator': 'LaTeX with hyperref', 'keywords': 'Taxonomy, autonomous agents, multi-agent collaboration, large language models (LLMs), AI systems classification, alignment, software architecture, architectural viewpoints, software-design rationale, context interaction, artificial intelligence, domain-ontology diagram, feature diagram, radar chart.', 'moddate': '2023-10-06T00:29:27+00:00', 'page': 13, 'page_label': '14', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'https://arxiv.org/pdf/2310.03659v1', 'subdomain': 'artificial_intelligence', 'subject': 'cs.AI, cs.SE, cs.MA', 'title': 'Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-powered Multi-Agent Architectures', 'total_pages': 40, 'trapped': '/False'}, page_content='not, or at least only indirectly, concern the utilized large language models (LLMs) [3, 92] or other contextual\\nresources, such as foundation models. However, the agent-interaction layer extends alignment possibilities\\nby integrating rules and mechanisms to control agent interactions, for example, by incorporating real-time\\nmonitoring via interceptors [4]. Such measures, as delineated by [40, 27] enable precise control over agent\\ninteractions as well as their interactions with LLMs and contextual resources, ensuring that they adhere to\\npredetermined conditions and behaviors. Moreover, employing methodologies like design by contract [46]\\n14'),\n",
       " Document(metadata={'author': 'Roy', 'creationdate': '2014-01-21T13:51:52-05:00', 'creator': 'Microsoft® Word 2010', 'moddate': '2014-01-21T14:10:43-05:00', 'page': 1, 'page_label': '2', 'producer': 'Microsoft® Word 2010', 'source': 'https://arxiv.org/pdf/1401.5424v1', 'subdomain': 'artificial_intelligence', 'total_pages': 16}, page_content='Leveraging domain knowledge limits the amount of information the autonomous agent needs to \\nlearn and may result in better performance. Additionally, it does not violate the core goal of AI \\nresearch, which is to develop agents that learns and utilizes knowledge autonomously. An agent \\nwill still need to learn and utilize game specific information. To develop such an agent a RTS \\nlanguage needs to be created. The next section proposes a Real Time Strategy language. \\nFaction Description \\nThe game description can be split broadly into two segments. The first segment described \\nhere is the faction description. This describes the factions that can comprise the game, along with \\ntheir buildings and units. As the description below demonstrates, Factions contain Human and \\nOrc. Meaning there consist two factions named Human and Orc respectively. It is important to \\nnote that the Human and Orc is game specific information and can easily be renamed. The \\nResources denote what resources are available in the game. At the beginning of the game a \\nplayer will have 100 pieces of wood for use. As with factions resources are game specific \\ninformation \\n \\n<Factions> \\n Human \\n Orc'),\n",
       " Document(metadata={'creationdate': '2024-10-15T01:15:48+00:00', 'creator': 'LaTeX with acmart 2023/03/30 v1.90 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'moddate': '2024-10-15T01:15:48+00:00', 'page': 1, 'page_label': '2', 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'https://arxiv.org/pdf/2410.09942v1', 'subdomain': 'information_retrieval', 'subject': '', 'title': 'Learning to Rank for Multiple Retrieval-Augmented Models through Iterative Utility Maximization', 'total_pages': 12, 'trapped': '/False'}, page_content='domain question answering, fact verification, slot filling, and entity\\nlinking, providing a benchmark for knowledge-intensive tasks.\\nRetrieval-Augmented Generation (RAG). RAG [26] represents\\na framework that merges information retrieval with natural lan-\\nguage generation to improve the quality of generated outputs by\\nintegrating external knowledge in the generation process [3, 47].\\nUnlike traditional LLMs that rely solely on pre-trained knowledge,\\nRAG can dynamically retrieve information from external sources\\nvia a retriever, enabling them to produce content that is more accu-\\nrate [20, 58]. This flexibility allows RAG to be applied in various\\ndomains, including knowledge grounding in textual [ 14, 26, 31]\\nand multimodal [5, 9, 36, 39], personalization [22, 37, 38, 40], and\\nreducing hallucinations [2, 44]. The retriever in RAG plays a piv-\\notal role as it sources the necessary information for the LLM to\\nperform its task [ 26]. This is typically done using either sparse\\nretrieval methods (e.g., TF-IDF, BM25 [34]) or dense retrieval mod-\\nels (e.g., DPR [19], Contriever [12], ColBERTv2 [43], E5 [54]). The\\nretrieved information is then utilized by the large language model\\nto complete the task. Prominent methods in this context include\\nIn-Prompt Augmentation (IPA) and Fusion-in-Decoder (FiD) [14].\\n1Our code can be found at: https://github.com/alirezasalemi7/uRAG\\nIn IPA, the retrieved data is appended to the prompt, allowing the\\nlanguage model to incorporate it during generation. FiD encodes\\neach retrieved document separately alongside the prompt within\\nthe encoder of an encoder-decoder architecture, combining them\\nin the decoder to generate a cohesive answer based on the available\\ninformation, as explained in Izacard and Grave [14].\\nA Search Engine for Machines. Research on search engine de-\\nsign demonstrates that successful retrieval systems rely on large-\\nscale user feedback for optimization [1, 50]. With the emergence of\\nLLMs as primary users of search engines [20, 58], Salemi and Za-\\nmani [41] showed that the LLMs’ preferences about relevance of a'),\n",
       " Document(metadata={'creationdate': '2024-10-15T01:15:48+00:00', 'creator': 'LaTeX with acmart 2023/03/30 v1.90 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'moddate': '2024-10-15T01:15:48+00:00', 'page': 1, 'page_label': '2', 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'https://arxiv.org/pdf/2410.09942v1', 'subdomain': 'information_retrieval', 'subject': '', 'title': 'Learning to Rank for Multiple Retrieval-Augmented Models through Iterative Utility Maximization', 'total_pages': 12, 'trapped': '/False'}, page_content='scale user feedback for optimization [1, 50]. With the emergence of\\nLLMs as primary users of search engines [20, 58], Salemi and Za-\\nmani [41] showed that the LLMs’ preferences about relevance of a\\nquery and document differs from humans. Salemi and Zamani [42]\\nintroduced a new problem that is training a unified search engine\\ncapable of serving multiple diverse RAG agents. They introduced\\nuRAG, a unified ranking model designed to serve multiple RAG\\nmodels while learning and optimizing based on feedback provided\\nby these diverse RAG models. Recently, several methods have been\\nproposed for training retrieval models tailored to LLMs, including\\ndistillation from LLMs to retrievers [13, 15, 55], end-to-end training\\nof retrievers and LLMs [35, 57], and bandit algorithms [53]. How-\\never, the majority of these approaches focus on leveraging feedback\\nfrom a single LLM, aiming to align the retrieval model with that\\nparticular LLM [42]. In this work, we introduce an approach based\\non iterative utility maximization to train a unified retrieval model\\nfor serving multiple RAG agents, applied in both offline and online\\nsettings [6, 11] to optimize the search engine for the agents.\\n3 PROBLEM FORMULATION\\nConsider the retrieval model 𝑅𝜃, parameterized by 𝜃, whose main\\nrole is to facilitate information access from a corpus 𝐶 for a set of\\nRAG models (a.k.a, RAG agents) denoted as 𝑀 = {𝑀𝑖}𝑛\\n𝑖=1. Each 𝑀𝑖\\nacts as a black-box agent for𝑅𝜃, which means that𝑅𝜃 does not have\\naccess to the models’ architecture, configuration, or parameters.\\nEach 𝑀𝑖 is designed to perform a knowledge-intensive task 𝑇𝑖 =\\n(𝐷train\\n𝑖 ,𝐷test\\n𝑖 ,𝜇𝑖)that requires external information from the cor-\\npus 𝐶 as the knowledge source. There is a training dataset 𝐷train\\n𝑖 =\\n{(𝑥𝑗,𝑦𝑗)}|𝐷train\\n𝑖 |\\n𝑗=1 for each agent 𝑀𝑖, which can be used by the re-\\ntrieval model 𝑅𝜃 for offline optimization. At inference, each agent\\n𝑀𝑖 operates sequentially on a test dataset 𝐷test\\n𝑖 = {(𝑥′\\n𝑗,𝑦′\\n𝑗)}|𝐷test\\n𝑖 |\\n𝑗=1\\nin the same order. The end-to-end performance of the agent𝑀𝑖 can\\nbe measured by a utility function (metric) 𝜇𝑖.'),\n",
       " Document(metadata={'creationdate': '2024-10-15T01:15:48+00:00', 'creator': 'LaTeX with acmart 2023/03/30 v1.90 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'moddate': '2024-10-15T01:15:48+00:00', 'page': 1, 'page_label': '2', 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'https://arxiv.org/pdf/2410.09942v1', 'subdomain': 'information_retrieval', 'subject': '', 'title': 'Learning to Rank for Multiple Retrieval-Augmented Models through Iterative Utility Maximization', 'total_pages': 12, 'trapped': '/False'}, page_content='Preprint, Preprint Alireza Salemi and Hamed Zamani\\nthe personalization of the search engine for that specific agent,\\ntailoring the system to better meet their unique information needs.\\nWe evaluate our approach using diverse tasks from the Knowledge-\\nIntensive Language Tasks (KILT) benchmark [31]. Our evaluation\\nincludes three open-domain question answering datasets: Natural\\nQuestions (NQ) [23], TriviaQA [18], and HotPotQA [56]; one fact\\nverification dataset: FEVER [49]; and two relation extraction and\\nslot-filling datasets: zsRE [24] and T-REx [8]. Following Salemi and\\nZamani [42], we test our approach with 18 different RAG agents,\\neach performing a specific task and utilizing distinct augmentation\\napproaches and LLMs, to serve as users of the search engine. Our\\nresults demonstrate that the proposed approach for offline iterative\\ntraining of the search engine significantly outperforms the state-\\nof-the-art baseline. Additionally, combining this offline approach\\nwith our online learning yields an even greater improvement over\\nthe baselines. We also conduct an extensive ablation study on vari-\\nous configurations of the proposed approaches to provide further\\ninsights into their effectiveness and impact. Furthermore, we show\\nthat our approach enhances the personalization of the search engine\\nover time, addressing a limitation noted in Salemi and Zamani [42].\\nWe observe that the correlation between the retrieval results of the\\nagents employing different LLMs but performing the same task is\\nvery low, indicating that the results are effectively personalized.\\nTo support future research, we have open-sourced our code and\\nmodels for the community. 1\\n2 RELATED WORK\\nKnowledge-Intensive Language Tasks (KILT) . Contrary to\\nstandard NLP tasks like natural language understanding [51, 52]\\nand question answering [28], where the input alone is enough to\\nperform the task, knowledge-intensive NLP tasks rely heavily on ex-\\nternal knowledge sources to extract necessary information. Petroni\\net al. [31] introduces KILT, a benchmark designed for evaluating\\nsuch tasks. KILT encompasses a variety of tasks, including open-\\ndomain question answering, fact verification, slot filling, and entity\\nlinking, providing a benchmark for knowledge-intensive tasks.\\nRetrieval-Augmented Generation (RAG). RAG [26] represents\\na framework that merges information retrieval with natural lan-')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a7b2315-b360-40fe-84e1-c97458c4df7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key concepts behind Agentic RAG (Robust, Adaptable, Goal-orientated) systems include:\n",
      "\n",
      "1. Autonomy: The ability of an agent to make decisions and take actions without human intervention.\n",
      "2. Alignment: The degree to which an agent's goals align with those of the user or system it is designed to serve.\n",
      "3. Architectural Aspects: 12 architectural aspects inherent to LLM-powered multi-agent systems, including viewpoints from Kruchten's viewpoint model for software architecture.\n",
      "\n",
      "Additionally, there are four applied architectural viewpoints:\n",
      "\n",
      "1. Autonomy Scope\n",
      "2. Alignment Scope\n",
      "3. Scope of Architectural Aspects\n",
      "4. Expressiveness of Taxonomic Classification\n",
      "\n",
      "The taxonomy also considers levels as strengths and weaknesses, with higher levels potentially leading to increased autonomy but also more complexity and potential errors.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "rag_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are an AI assistant answering user questions based on retrieved documents.\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Step 2: Format Retrieved Docs\n",
    "def format_docs(docs):\n",
    "    \"\"\"Extracts and joins text from retrieved documents\"\"\"\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Step 3: Generate Answer\n",
    "context_text = format_docs(relevant_docs)\n",
    "llm_chain = rag_prompt | llm  # Chain the prompt with LLM\n",
    "answer = llm_chain.invoke({\"context\": context_text, \"question\": question})\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c97dd496-1025-4b1c-81cd-c2ca6a4a8a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x16de74550>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "79b101b0-f6fa-4429-bde0-096c02b537e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document 1 ---\n",
      "\n",
      "Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\n",
      "\n",
      "Each element is an observation, an event directly provided by the agent.\n",
      "- Inter-agent communication can trigger new natural language statements.\n",
      "\n",
      "\n",
      "Retrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\n",
      "\n",
      "Recency: recent events have higher scores\n",
      "Importance: distinguish mundane from core memories. Ask LM directly.\n",
      "Relevance: based on how related it is to the current situation / query.\n",
      "\n",
      "\n",
      "Reflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display results\n",
    "for i, doc in enumerate(relevant_docs):\n",
    "    print(f\"\\n--- Document {i+1} ---\\n\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2ad43c76-8d67-49e8-922c-f10519d15ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "#Decomposition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13425cef-42f7-4a72-bebf-149ea0a085f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bbb364-0468-476f-8ff2-004c46eb0a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Searching Arxiv for relevant research papers and downloading the pdfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8500980d-18f5-4536-bb6c-64c96a59551b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting requests~=2.32.0 (from arxiv)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (2024.2.2)\n",
      "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m414.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=d8e2a6e8db1aca75477c20c71e69f2339a2be651da4b6f5cd73e5cbd73a894df\n",
      "  Stored in directory: /Users/aditi/Library/Caches/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, requests, feedparser, arxiv\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.75 requires requests_mock, which is not installed.\n",
      "streamlit 1.30.0 requires packaging<24,>=16.8, but you have packaging 24.2 which is incompatible.\n",
      "streamlit 1.30.0 requires protobuf<5,>=3.20, but you have protobuf 5.29.4 which is incompatible.\n",
      "streamlit 1.30.0 requires tenacity<9,>=8.1.0, but you have tenacity 9.0.0 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
      "anaconda-cloud-auth 0.1.4 requires pydantic<2.0, but you have pydantic 2.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed arxiv-2.1.3 feedparser-6.0.11 requests-2.32.3 sgmllib3k-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b57bad9b-cdaf-416f-900d-b2643f3c07f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Published', 'Title', 'Authors', 'Summary'])\n",
      "{'Published': '2025-02-04', 'Title': 'Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG', 'Authors': 'Aditi Singh, Abul Ehtesham, Saket Kumar, Tala Talaei Khoei', 'Summary': 'Large Language Models (LLMs) have revolutionized artificial intelligence (AI)\\nby enabling human like text generation and natural language understanding.\\nHowever, their reliance on static training data limits their ability to respond\\nto dynamic, real time queries, resulting in outdated or inaccurate outputs.\\nRetrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs\\nby integrating real time data retrieval to provide contextually relevant and\\nup-to-date responses. Despite its promise, traditional RAG systems are\\nconstrained by static workflows and lack the adaptability required for\\nmultistep reasoning and complex task management.\\n  Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these\\nlimitations by embedding autonomous AI agents into the RAG pipeline. These\\nagents leverage agentic design patterns reflection, planning, tool use, and\\nmultiagent collaboration to dynamically manage retrieval strategies,\\niteratively refine contextual understanding, and adapt workflows to meet\\ncomplex task requirements. This integration enables Agentic RAG systems to\\ndeliver unparalleled flexibility, scalability, and context awareness across\\ndiverse applications.\\n  This survey provides a comprehensive exploration of Agentic RAG, beginning\\nwith its foundational principles and the evolution of RAG paradigms. It\\npresents a detailed taxonomy of Agentic RAG architectures, highlights key\\napplications in industries such as healthcare, finance, and education, and\\nexamines practical implementation strategies. Additionally, it addresses\\nchallenges in scaling these systems, ensuring ethical decision making, and\\noptimizing performance for real-world applications, while providing detailed\\ninsights into frameworks and tools for implementing Agentic RAG.'}\n",
      "Found paper: Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG\n",
      "Authors: Aditi Singh, Abul Ehtesham, Saket Kumar, Tala Talaei Khoei\n",
      "Published: 2025-02-04\n",
      "PDF URL: None\n",
      "PDF URL: https://arxiv.org/pdf/2501.09136.pdf\n",
      "Downloaded PDF as 2501.09136.pdf\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "\n",
    "# STEP 1: Define your search query\n",
    "query = \"Agentic RAG\"\n",
    "\n",
    "# STEP 2: Search arXiv for top paper\n",
    "loader = ArxivLoader(query=query, load_max_docs=1)\n",
    "docs = loader.load()\n",
    "\n",
    "print(docs[0].metadata.keys())\n",
    "print(docs[0].metadata)\n",
    "\n",
    "# STEP 3: Inspect metadata for the PDF URL\n",
    "metadata = docs[0].metadata\n",
    "print(\"Found paper:\", metadata.get(\"Title\"))\n",
    "print(\"Authors:\", metadata.get(\"Authors\"))\n",
    "print(\"Published:\", metadata.get(\"Published\"))\n",
    "print(\"PDF URL:\", metadata.get(\"pdf_url\"))\n",
    "page_content = docs[0].page_content\n",
    "\n",
    "# STEP 4: Extract arXiv ID using regex from page_content\n",
    "match = re.search(r'arXiv:(\\d{4}\\.\\d{5})', page_content)\n",
    "if match:\n",
    "    arxiv_id = match.group(1)\n",
    "    pdf_url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "    print(\"PDF URL:\", pdf_url)\n",
    "\n",
    "    # Step 5: Download PDF\n",
    "    pdf_filename = f\"{arxiv_id}.pdf\"\n",
    "    response = requests.get(pdf_url)\n",
    "    with open(pdf_filename, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded PDF as {pdf_filename}\")\n",
    "else:\n",
    "    print(\"❌ Could not extract arXiv ID.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "57526323-4a4b-4497-96ea-dc9b8bd4b5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2501.09136.pdf'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_id + \".pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1199bd2a-7de8-48ad-856d-ca88ca763f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-5.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "575eba49-0ba8-471a-bbe1-8ecf3f2b02ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pdf_path = arxiv_id + \".pdf\"  # Replace with your actual file name\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# STEP 2: Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# STEP 3: Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# STEP 4: Temporary in-memory vector DB\n",
    "tempDB = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "# STEP 5: Retriever\n",
    "retriever = tempDB.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9a7bc8e0-1ce9-4f33-a953-bd94c86c0ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The text mentions three RAG paradigms that evolved into Agentic RAG:\n",
      "\n",
      "1. Naïve RAG\n",
      "2. Modular RAG\n",
      "3. Graph RAG\n",
      "\n",
      "These are the phases of RAG evolution mentioned in the context, but it doesn't provide information on the exact sequence or order of their development before Agentic RAG.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)\n",
    "\n",
    "query = \"What are the phases of RAG evoulution before Agentic RAG?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(\"Answer:\", result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "104ae2b3-1e22-45a3-b0ae-c2c6eade677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MultiQueryRetriever\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from typing import List\n",
    "\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        # Find all lines that start with a digit followed by a period (1. 2. etc.)\n",
    "        lines = re.findall(r\"^\\d+\\.\\s+(.*)\", text, re.MULTILINE)\n",
    "        return [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "query_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI assistant. Rephrase the question in 5 different ways to improve retrieval from a document store. Separate each by a new line.\\nQuestion: {question}\"\"\"\n",
    ")\n",
    "\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.2\")\n",
    "llm_chain = query_prompt | llm | LineListOutputParser()\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "retriever = MultiQueryRetriever(\n",
    "    retriever=vector_db.as_retriever(),\n",
    "    llm_chain=llm_chain,\n",
    "    parser_key=\"lines\"  # matches the output parser\n",
    ")\n",
    "\n",
    "question = \"What are the RAG evoulution paradigms before Agentic RAG?\"\n",
    "\n",
    "# Get relevant documents using multi-query retrieval\n",
    "relevant_docs = retriever.invoke(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075613a3-e2e9-4766-b749-b32421c1ba17",
   "metadata": {},
   "source": [
    "<font size=\"4\"> Basic LLM Response </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f876e0-1647-4fef-814c-8521a178308e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def basic_llm_response(topic: str, model_name: str = \"llama3.2\") -> str:\n",
    "    \"\"\"Returns a basic LLM response for a given topic using a simple prompt.\"\"\"\n",
    "    llm = OllamaLLM(model=model_name)\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\"What is {topic}?\")\n",
    "    chain = prompt | llm\n",
    "\n",
    "    response = chain.invoke({\"topic\": topic})\n",
    "    return response.replace(\"\\\\n\", \"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cf8bed6-20bc-422b-8a22-3861e3101e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "I couldn't find any specific information on \"agentic RAg\". It's possible that it's a specialized term or an acronym used within a particular context or field.\n",
      "\n",
      "However, I can try to help you break down the words and see if we can make an educated guess about what it might refer to:\n",
      "\n",
      "* \"Agentic\" is an adjective that means having power, authority, or control over something or someone. It's often used in psychology and sociology to describe individuals who are autonomous, self-directed, and proactive.\n",
      "* \"Rag\" (or \"raging\") is a verb that can have different meanings depending on the context. In general, it can mean being angry, upset, or violent.\n",
      "\n",
      "Given these definitions, it's possible that \"agentic RAg\" could refer to something like:\n",
      "\n",
      "* A person who is agential (autonomous and self-directed) and exhibits strong emotions such as anger or frustration.\n",
      "* A concept or idea related to agency and emotional regulation.\n",
      "* A term used in a specific field, such as psychology or sociology, to describe a particular phenomenon or theory.\n",
      "\n",
      "If you have more context or information about what \"agentic RAg\" refers to, I'd be happy to try and help you further.\n"
     ]
    }
   ],
   "source": [
    "print(type(response))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "720876db-b099-4038-be0c-d7841c91e822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter question what is agentic RAg\n"
     ]
    }
   ],
   "source": [
    "question = input(\"enter question\")\n",
    "response =basic_llm_response(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40141b67-5c6c-406c-947e-c80ffbc1a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = input(\"enter question\")\n",
    "response =basic_llm_response(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfdf368-d562-4859-a1c4-d51e9d12fba2",
   "metadata": {},
   "source": [
    "<font size=\"4\"> RAG over LLM Response </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "affb36ca-7bf4-4dc2-aaae-fc04254a9566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve the relevant docs from the database\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Setup your Chroma DB path and embedding model\n",
    "DB_DIR = \"./arxiv_vector1_db\"\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_db = Chroma(persist_directory=DB_DIR, embedding_function=embedding_model)\n",
    "\n",
    "\n",
    "def retrieve_simple_docs(question: str, top_k: int = 5):\n",
    "    print(f\"\\n Simple Retrieval\\nQuery: {question}\\n\")\n",
    "    \n",
    "    results = vector_db.similarity_search_with_relevance_scores(question, k=top_k)\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"--- Result {i} ---\")\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        print(f\"Chunk:\\n{doc.page_content[:300]}...\")\n",
    "        print(f\"Source PDF: {doc.metadata.get('source')}\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "#MultiQuery Retrieval\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from typing import List\n",
    "\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        # Find all lines that start with a digit followed by a period (1. 2. etc.)\n",
    "        lines = re.findall(r\"^\\d+\\.\\s+(.*)\", text, re.MULTILINE)\n",
    "        return [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "def retrieve_multiquery_docs(question: str):\n",
    "    print(f\"\\n Multi-Query Retrieval\\nQuery: {question}\\n\")\n",
    "\n",
    "    query_prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"\"\"You are an AI assistant. Rephrase the question in 5 different ways to improve retrieval from a document store. Separate each by a new line.\\nQuestion: {question}\"\"\"\n",
    "    )   \n",
    "    llm = OllamaLLM(model=\"llama3.2\")\n",
    "    llm_chain = query_prompt | llm | LineListOutputParser()\n",
    "\n",
    "    rephrased_queries = llm_chain.invoke({\"question\": question})\n",
    "    print(\" Rephrased Queries:\")\n",
    "    for i, q in enumerate(rephrased_queries, 1):\n",
    "        print(f\"{i}. {q}\")\n",
    "    print()\n",
    "\n",
    "    retriever = MultiQueryRetriever(\n",
    "        retriever=vector_db.as_retriever(),\n",
    "        llm_chain=llm_chain,\n",
    "        parser_key=\"lines\"\n",
    "    )\n",
    "    docs = retriever.invoke(question)\n",
    "    \n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        print(f\"--- Result {i} ---\")\n",
    "        # print(f\"Score: {score:.4f}\")\n",
    "        print(f\"Chunk:\\n{doc.page_content[:300]}...\")\n",
    "        print(f\"Source PDF: {doc.metadata.get('source')}\")\n",
    "        print()\n",
    "    \n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea4c18e9-b909-4b19-909a-92cbd65f753c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Simple Retrieval\n",
      "Query: What are the key concepts behind Agentic RAG systems\n",
      "\n",
      "--- Result 1 ---\n",
      "Score: 0.3880\n",
      "Chunk:\n",
      "resulting in Agentic RAG. By incorporating autonomous agents capable of dynamic decision-making, iterative reasoning,\n",
      "and adaptive retrieval strategies, Agentic RAG builds on the modularity of earlier paradigms while overcoming their\n",
      "inherent constraints. This evolution enables more complex, multi-d...\n",
      "Source PDF: https://arxiv.org/pdf/2501.09136\n",
      "\n",
      "--- Result 2 ---\n",
      "Score: 0.3749\n",
      "Chunk:\n",
      "agentic intelligence, these systems introduce capabilities such as dynamic decision-making, iterative reasoning, and\n",
      "collaborative workflows, enabling them to tackle complex, real-world tasks with enhanced precision and adaptability.\n",
      "This survey explored the evolution of RAG systems, from their init...\n",
      "Source PDF: https://arxiv.org/pdf/2501.09136\n",
      "\n",
      "--- Result 3 ---\n",
      "Score: 0.3731\n",
      "Chunk:\n",
      "into Agentic RAG systems. Key contributions include a detailed taxonomy of Agentic RAG frameworks, applications\n",
      "across domains such as healthcare [17, 18], finance, and education [19], and insights into implementation strategies,\n",
      "benchmarks, and ethical considerations.\n",
      "The structure of this paper is...\n",
      "Source PDF: https://arxiv.org/pdf/2501.09136\n",
      "\n",
      "--- Result 4 ---\n",
      "Score: 0.3446\n",
      "Chunk:\n",
      "tion, planning, tool use, and multi-agent collaboration to dynamically manage retrieval strategies,\n",
      "iteratively refine contextual understanding, and adapt workflows through clearly defined operational\n",
      "structures ranging from sequential steps to adaptive collaboration. This integration enables Agenti...\n",
      "Source PDF: https://arxiv.org/pdf/2501.09136\n",
      "\n",
      "--- Result 5 ---\n",
      "Score: 0.3304\n",
      "Chunk:\n",
      "10 Conclusion\n",
      "Agentic Retrieval-Augmented Generation (RAG) represents a transformative advancement in artificial intelligence,\n",
      "addressing the limitations of traditional RAG systems through the integration of autonomous agents. By leveraging\n",
      "33...\n",
      "Source PDF: https://arxiv.org/pdf/2501.09136\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## question = \"What are the key concepts behind Agentic RAG systems?\"\n",
    "simple_retrieved_docs = retrieve_simple_docs(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa34280f-8d89-45f3-8f4b-db98c26ac6cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Multi-Query Retrieval\n",
      "Query: What are the key concepts behind Agentic RAG systems?\n",
      "\n",
      " Rephrased Queries:\n",
      "1. What is the underlying philosophy that drives the development of Agentic RAG (Relevance-Adjusted Ranking of Goals) systems?\n",
      "2. Can you explain the core principles and motivations that underpin the design of Agentic RAG systems?\n",
      "3. How do Agentic RAG systems prioritize tasks and goals, and what key concepts are at play in this process?\n",
      "4. What theoretical foundations or assumptions support the use of Agentic RAG systems in decision-making and task allocation?\n",
      "5. In the context of goal-oriented systems, what are the essential aspects that Agentic RAG systems aim to capture or optimize?\n",
      "\n",
      "--- Result 1 ---\n",
      "Chunk:\n",
      "the agent does not know about the correct action to be performed for a given input. Instead, it tries to learn the optimal action based on the reward it receives. Problems solved by RL algorithms are very similar to the Markov decision process (MDP), involving the following information: • Finite set...\n",
      "Source PDF: https://arxiv.org/pdf/2501.08655v1\n",
      "\n",
      "--- Result 2 ---\n",
      "Chunk:\n",
      "comprehensive survey of multiagent reinforcement learning.\n",
      "IEEE Transactions on Systems, Man, and Cybernetics, Part\n",
      "C (Applications and Reviews), 38(2): 156–172.\n",
      "Heckhausen, J.; and Heckhausen, H. 2018. Motivation and\n",
      "action. Springer.\n",
      "Hernandez-Leal, P.; Kartal, B.; and Taylor, M. E. 2019. A\n",
      "survey...\n",
      "Source PDF: https://arxiv.org/pdf/2401.05572v1\n",
      "\n",
      "--- Result 3 ---\n",
      "Chunk:\n",
      "perceived by agents combines the views and knowledge of other\n",
      "agents around them, and can better reflect the real situation of the\n",
      "current agent.\n",
      "Ant Nest\n",
      "Obstacle\n",
      "Hole\n",
      "Food\n",
      "(a) Table Q -Learning (b) Pheromone Table Q -Learning\n",
      "Figure 1: Intuition behind PooL (a) Most agents trained\n",
      "by Table Q-Learn...\n",
      "Source PDF: https://arxiv.org/pdf/2202.09722v1\n",
      "\n",
      "--- Result 4 ---\n",
      "Chunk:\n",
      "Schembri, M.; Mirolli, M.; and Baldassarre, G. 2007. Evolu-\n",
      "tion and learning in an intrinsically motivated reinforcement\n",
      "learning robot. In Advances in Artificial Life: 9th European\n",
      "Conference, ECAL 2007, Lisbon, Portugal, September 10-\n",
      "14, 2007. Proceedings 9, 294–303. Springer.\n",
      "Schiefele, U. 1996...\n",
      "Source PDF: https://arxiv.org/pdf/2401.05572v1\n",
      "\n",
      "--- Result 5 ---\n",
      "Chunk:\n",
      "10 Eger et al.\n",
      "Paper chat and QA. Paper chat and question-answering (QA) systems such as ChatGPT, Deepseek, NotebookLM,\n",
      "ExplainPaper, ChatPDF, and DocAnalyzer.AI allow users to interact with scientific papers by asking questions and\n",
      "receiving responses based on the document’s content. They typically...\n",
      "Source PDF: https://arxiv.org/pdf/2502.05151v1\n",
      "\n",
      "--- Result 6 ---\n",
      "Chunk:\n",
      "Learning to Rank for Multiple Retrieval-Augmented Models\n",
      "through Iterative Utility Maximization\n",
      "Alireza Salemi\n",
      "University of Massachusetts Amherst\n",
      "United States\n",
      "asalemi@cs.umass.edu\n",
      "Hamed Zamani\n",
      "University of Massachusetts Amherst\n",
      "United States\n",
      "zamani@cs.umass.edu\n",
      "ABSTRACT\n",
      "This paper investigates th...\n",
      "Source PDF: https://arxiv.org/pdf/2410.09942v1\n",
      "\n",
      "--- Result 7 ---\n",
      "Chunk:\n",
      "scale user feedback for optimization [1, 50]. With the emergence of\n",
      "LLMs as primary users of search engines [20, 58], Salemi and Za-\n",
      "mani [41] showed that the LLMs’ preferences about relevance of a\n",
      "query and document differs from humans. Salemi and Zamani [42]\n",
      "introduced a new problem that is traini...\n",
      "Source PDF: https://arxiv.org/pdf/2410.09942v1\n",
      "\n",
      "--- Result 8 ---\n",
      "Chunk:\n",
      "domain question answering, fact verification, slot filling, and entity\n",
      "linking, providing a benchmark for knowledge-intensive tasks.\n",
      "Retrieval-Augmented Generation (RAG). RAG [26] represents\n",
      "a framework that merges information retrieval with natural lan-\n",
      "guage generation to improve the quality of ge...\n",
      "Source PDF: https://arxiv.org/pdf/2410.09942v1\n",
      "\n",
      "--- Result 9 ---\n",
      "Chunk:\n",
      "to address this limitation is to enhance LLMs by retrieving infor-\n",
      "mation from external knowledge sources, a technique known as\n",
      "retrieval-augmented generation (RAG) [ 3, 14, 26]. This marks a\n",
      "Permission to make digital or hard copies of part or all of this work for personal or\n",
      "classroom use is grant...\n",
      "Source PDF: https://arxiv.org/pdf/2410.09942v1\n",
      "\n",
      "--- Result 10 ---\n",
      "Chunk:\n",
      "𝑖 = {(𝑥′\n",
      "𝑗,𝑦′\n",
      "𝑗)}|𝐷test\n",
      "𝑖 |\n",
      "𝑗=1\n",
      "in the same order. The end-to-end performance of the agent𝑀𝑖 can\n",
      "be measured by a utility function (metric) 𝜇𝑖.\n",
      "Each RAG agent can be simply formulated as ¯𝑦𝑀𝑖 = 𝑀𝑖(𝑥; 𝑅𝜃).\n",
      "In more detail, given an input 𝑥, each RAG agent 𝑀𝑖 submits a\n",
      "query to the retrieval model 𝑅𝜃 f...\n",
      "Source PDF: https://arxiv.org/pdf/2410.09942v1\n",
      "\n",
      "--- Result 11 ---\n",
      "Chunk:\n",
      "optimizer and learning rate to train the model for two epochs in\n",
      "each iteration. In this case, we set the variable 𝑘 to be consistent\n",
      "with the downstream RAG agent configuration for each model in\n",
      "Table 3 in Appendix B, in contrast with the offline phase, where we\n",
      "set a constant number of 𝑘 = 32 for ...\n",
      "Source PDF: https://arxiv.org/pdf/2410.09942v1\n",
      "\n",
      "--- Result 12 ---\n",
      "Chunk:\n",
      "captures how the ranking order of documents correlates between\n",
      "two lists, providing insights into how similarly or differently the\n",
      "documents are ranked for various agents. Jaccard’s similarity, on\n",
      "the other hand, is a set-based metric that quantifies the overlap\n",
      "between two sets of retrieved documen...\n",
      "Source PDF: https://arxiv.org/pdf/2410.09942v1\n",
      "\n",
      "--- Result 13 ---\n",
      "Chunk:\n",
      "Preprint, Preprint Alireza Salemi and Hamed Zamani\n",
      "Agents Configuration. Following Salemi and Zamani [42], we\n",
      "utilize 18 diverse RAG agents in our experiments, as listed in Table\n",
      "3 in Appendix B. In this setting, each agent is trained on a separate\n",
      "dataset, with a distinct set of resources, a differ...\n",
      "Source PDF: https://arxiv.org/pdf/2410.09942v1\n",
      "\n",
      "--- Result 14 ---\n",
      "Chunk:\n",
      "search engines serve human users, considering the paradigm shift\n",
      "where nowadays LLMs are the main users of the search engines.\n",
      "We propose IUM, an iterative framework with expectation maxi-\n",
      "mization to iteratively gather feedback from RAG agents and adjust\n",
      "the search engine based on them in an offlin...\n",
      "Source PDF: https://arxiv.org/pdf/2410.09942v1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the key concepts behind Agentic RAG systems?\"\n",
    "multiquery_retrieved_docs = retrieve_multiquery_docs(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd45a3eb-bf6b-4a12-8796-a022bb9d21d9",
   "metadata": {},
   "source": [
    "<font size=\"3\"> Getting Response</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c214710-4cbd-467b-98e3-1babac9fa40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Step 1: Define the prompt\n",
    "rag_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are an AI assistant answering user questions based on retrieved documents.\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# Step 2: Format the retrieved documents\n",
    "def format_docs(docs):\n",
    "    \"\"\"Extracts and joins text from retrieved documents, supports (doc, score) format.\"\"\"\n",
    "    cleaned = []\n",
    "    for doc in docs:\n",
    "        if isinstance(doc, tuple):  # (Document, score)\n",
    "            doc = doc[0]\n",
    "        cleaned.append(doc.page_content)\n",
    "    return \"\\n\\n\".join(cleaned)\n",
    "\n",
    "# Step 3: Run the LLM chain on formatted docs\n",
    "def generate_rag_answer(docs, question, llm):\n",
    "    context_text = format_docs(docs)\n",
    "    llm_chain = rag_prompt | llm\n",
    "    return llm_chain.invoke({\"context\": context_text, \"question\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c75f07c5-d537-4cc1-b495-2e23a586ac03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " According to the retrieved documents, the key concepts behind Agentic RAG systems include:\n",
      "\n",
      "1. Autonomous Agents: These agents comprise a Large Language Model (LLM), Memory, and Planning components, enabling dynamic decision-making, iterative reasoning, and collaborative workflows.\n",
      "2. Dynamic Decision-Making: Autonomous agents can make decisions based on real-time data retrieval, contextually accurate information, and adaptable workflows.\n",
      "3. Iterative Reasoning: Agents use reflection, query routing, or self-critique to break down complex tasks into manageable steps.\n",
      "4. Collaborative Workflows: Agentic RAG systems enable multi-agent collaboration, allowing agents to work together to achieve common goals.\n",
      "5. Contextual Understanding: The system's ability to capture and adapt to context, ensuring that outputs are relevant, accurate, and up-to-date.\n",
      "6. Dynamic Adaptability: Agents can adjust their strategies in response to changing conditions or new information.\n",
      "7. Scalability: Agentic RAG systems aim to reduce latency through optimized workflows and refine outputs iteratively.\n",
      "\n",
      "These concepts enable Agentic RAG systems to tackle complex tasks with enhanced precision, adaptability, and context-awareness, positioning them as a cornerstone for next-generation AI applications.\n"
     ]
    }
   ],
   "source": [
    "answer = generate_rag_answer(simple_retrieved_docs, question, llm)\n",
    "print(\"\\nAnswer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d852d4d4-a256-4e1b-a4ce-7c557547cee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " The key concepts behind Agentic RAG systems include:\n",
      "\n",
      "1. Autonomous Agents: Agentic RAG systems integrate autonomous agents that perform specialized tasks, such as retrieval, augmentation, and generation.\n",
      "2. Dynamic Adaptability: These systems are designed to adapt to changing queries and contexts, ensuring high response accuracy and efficiency.\n",
      "3. Modular Architecture: Agentic RAG systems consist of multiple agents working together to achieve a common goal, promoting modularity, scalability, and fault tolerance.\n",
      "4. Contextual Understanding: The system's ability to understand the context of the query and generate responses that are relevant and coherent.\n",
      "5. Retrieval from External Sources: Agentic RAG systems can dynamically retrieve information from external sources, such as knowledge bases or APIs, when the initial retrieval results are insufficient.\n",
      "6. Response Synthesis: The final response is generated by synthesizing all validated and refined information, ensuring high accuracy and coherence.\n",
      "7. Reflection, Planning, Tool Use, and Multi-Agent Collaboration: These agents work together to dynamically manage retrieval strategies, refine contextual understanding, and adapt workflows through clearly defined operational structures.\n",
      "\n",
      "These concepts combine to provide Agentic RAG systems with unparalleled flexibility, scalability, and context-awareness across diverse applications.\n"
     ]
    }
   ],
   "source": [
    "answer = generate_rag_answer(multiquery_retrieved_docs, question, llm)\n",
    "print(\"\\nAnswer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74202d8d-5860-4a10-a7a0-6d94a809bfb2",
   "metadata": {},
   "source": [
    "<font size=\"4\"> Comparing the results <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bda8126d-218e-4c51-8967-f01d9d84574e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter question How can I transform cyclical features to improve model accuracy?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Transforming cyclical features can significantly improve model accuracy, especially in tasks such as image classification, object detection, and generation. Cyclical features refer to patterns or structures that repeat over time or space, often causing difficulties for deep learning models.\\n\\nHere are some common techniques used to transform cyclical features:\\n\\n1. **Data Augmentation**: Apply random transformations to the input data, such as rotation, scaling, flipping, and color jittering, to break the cyclical pattern.\\n2. **Periodic Convolution**: Use periodic convolutional layers with a period equal to the length of the cyclical feature to extract patterns at different scales.\\n3. **Residual Connections**: Incorporate residual connections between layers to capture long-range dependencies in cyclical features.\\n4. **Self-Attention Mechanism**: Utilize self-attention mechanisms, such as spatial attention or channel attention, to focus on specific parts of the input data that are relevant for classification.\\n5. **Long Short-Term Memory (LSTM) Networks**: Employ LSTM networks to model long-term dependencies in cyclical features.\\n6. **Generative Adversarial Networks (GANs)**: Use GANs to generate new samples that can help break the cyclical pattern and improve model accuracy.\\n7. **Cycle-Consistency Loss**: Incorporate a cycle-consistency loss term into the objective function to encourage the network to preserve the cyclical structure in the input data.\\n\\nBy applying these techniques, you can transform cyclical features and improve model accuracy for tasks such as:\\n\\n* Image classification: Cyclical patterns in images, like texture or shading, can be challenging for models. Techniques like periodic convolution and self-attention mechanisms can help capture these patterns.\\n* Object detection: Cyclical patterns in object appearances, like rotation or scaling, can affect detection performance. Residual connections and LSTMs can help model these dependencies.\\n* Image generation: GANs can generate images with realistic cyclical features, such as texture or shading.\\n\\nWhen choosing a technique, consider the specific nature of your data and the task at hand. Experimenting with different approaches will help you find the best method for transforming cyclical features and improving model accuracy.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How can I transform cyclical features to improve model accuracy?\n",
    "question = input(\"enter question\")\n",
    "basic_llm_response(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e19255fb-8518-45bc-a491-542cae756bcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Simple Retrieval\n",
      "Query: How can I transform cyclical features to improve model accuracy?\n",
      "\n",
      "--- Result 1 ---\n",
      "Score: 0.1466\n",
      "Chunk:\n",
      "also considered in the implementation of DR techniques.\n",
      "Features of the selected FS and DR techniques are summarized\n",
      "in Table IV.\n",
      "Each FS or DR technique is then combined with a classiﬁca-\n",
      "tion model (kNN, SVM and RF). In this regard, the selected set\n",
      "of features is fed into the aforementioned class...\n",
      "Source PDF: ./arxiv_data/feature_engineering/2303.08300v1_Learning_From_High-Dimensional_Cyber-Physical_Data.pdf\n",
      "\n",
      "--- Result 2 ---\n",
      "Score: 0.1421\n",
      "Chunk:\n",
      "High-dim.feature vectorsSelecting afeature subsetLow-dim.feature vectorsClassiﬁcationModel Classiﬁer erroras a criterion\n",
      "(a) Feature selection via wrapper methods\n",
      "High-dim.feature vectorsFeature rankingwith a criterionSelecting the topbestfeaturesLow-dim.feature vectorsClassiﬁcationModel\n",
      "(b) Feature...\n",
      "Source PDF: ./arxiv_data/feature_engineering/2105.00191v1_Stochastic_Mutual_Information_Gradient_Estimation_.pdf\n",
      "\n",
      "--- Result 3 ---\n",
      "Score: 0.1292\n",
      "Chunk:\n",
      "the total number of features from 500 to 5000 and considered three synthetic datasets with easy,\n",
      "intermediate, and hard classiﬁcation diﬃculty (can be controlled by the variableclass_sep). We\n",
      "randomly split each dataset into 70% training, 20% validation, and 10% testing with 20 replicates.\n",
      "We perfor...\n",
      "Source PDF: ./arxiv_data/feature_engineering/2211.14144v1_Graph_Convolutional_Network-based_Feature_Selectio.pdf\n",
      "\n",
      "--- Result 4 ---\n",
      "Score: 0.1278\n",
      "Chunk:\n",
      "and correspond to the best accuracy shown above.\n",
      "Based on the results, the RFMS and FA methods outperformed both PCA and k-best screening in accuracy. The highest\n",
      "accuracy was achieved by using the RFMS–SVC and FA–RF pairs (61.4%); however, the latter combination required\n",
      "considerably lower screenin...\n",
      "Source PDF: ./arxiv_data/feature_engineering/2305.15793v1_Feature_space_reduction_method_for_ultrahigh-dimen.pdf\n",
      "\n",
      "--- Result 5 ---\n",
      "Score: 0.1274\n",
      "Chunk:\n",
      "Our work is supported by NSF (IIS-1149570, CNS-1544895, IIS-1715858),\n",
      "DHHS (90RE5017-02-01), and NIH (R01DC009834).\n",
      "Copyright (c) 2017 IEEE. Personal use of this material is permitted.\n",
      "However, permission to use this material for any other purposes must be\n",
      "obtained from the IEEE by sending an email ...\n",
      "Source PDF: ./arxiv_data/feature_engineering/1903.12235v2_Information_Theoretic_Feature_Transformation_Learn.pdf\n",
      "\n",
      "\n",
      "Answer:\n",
      " Based on the provided text, it appears that the authors are discussing various methods for transforming high-dimensional features to improve model accuracy. However, none of the methods specifically mentioned in the text are directly related to transforming cyclical features.\n",
      "\n",
      "The text does mention \"feature transformation approaches\" (option c) which aim to learn a mapping function based on an optimality criterion independent of the classification model. This might be a possible approach to consider, but it would require further research and development to determine if it can effectively transform cyclical features.\n",
      "\n",
      "It's also worth noting that the authors mention \"information theoretic linear projections or rotations\" (option c) which could potentially be used to transform high-dimensional features. However, without more specific information on how these methods might be applied to cyclical features, it's difficult to say if they would be effective.\n"
     ]
    }
   ],
   "source": [
    "# question = input(\"enter question\")\n",
    "simple_retrieved_docs = retrieve_simple_docs(question)\n",
    "answer = generate_rag_answer(simple_retrieved_docs, question, llm)\n",
    "print(\"\\nAnswer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9b092f9-8069-4afb-a9b4-72680d25ec03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Multi-Query Retrieval\n",
      "Query: How can I transform cyclical features to improve model accuracy?\n",
      "\n",
      " Rephrased Queries:\n",
      "1. What strategies can be employed to convert cyclical attributes into numerical representations that enhance the performance of machine learning models?\n",
      "2. How might I adapt or modify cyclical data to enable more effective learning and prediction in complex systems?\n",
      "3. What methods can be used to transform cyclical variables into a format that is better suited for analysis and training by machine learning algorithms?\n",
      "4. Can you provide suggestions on how to reformulate cyclical features to improve the interpretability and accuracy of models used in various applications?\n",
      "5. How might I translate cyclical data into a numerical or categorical format that is more conducive to model optimization, feature engineering, and overall improved performance?\n",
      "\n",
      "--- Result 1 ---\n",
      "Chunk:\n",
      "AutoAI-TS: AutoAI for Time Series Forecasting Conference’17, July 2017, Washington, DC, USA\n",
      "preprint arXiv:1703.07015(2017).\n",
      "[24] Alysha Livera, Rob Hyndman, and Ralph Snyder. 2010. Forecasting Time Series\n",
      "With Complex Seasonal Patterns Using Exponential Smoothing. J. Amer. Statist.\n",
      "Assoc. 106 (01 2...\n",
      "Source PDF: ./arxiv_data/model_selection/2102.12347v2_AutoAI-TS:_AutoAI_for_Time_Series_Forecasting.pdf\n",
      "\n",
      "--- Result 2 ---\n",
      "Chunk:\n",
      "longer.\n",
      "3.4. Forecasting techniques\n",
      "In order to conduct the necessary experiments, a total of 17 forecasting techniques\n",
      "were selected, which are summarized in Table 4. These techniques are categorized ac-\n",
      "cording to three diﬀerent types of attributes in order to answer our research questions.\n",
      "Firstl...\n",
      "Source PDF: ./arxiv_data/model_selection/2002.00949v1_Profit-oriented_sales_forecasting:_a_comparison_of.pdf\n",
      "\n",
      "--- Result 3 ---\n",
      "Chunk:\n",
      "window and its length has significant impact on performance of the\n",
      "models. Efficient value of look-back window length is somewhat\n",
      "dependent on the input data, therefore it can either be set based on\n",
      "prior knowledge about the data or should be discovered based on\n",
      "characteristics of the input data. Au...\n",
      "Source PDF: ./arxiv_data/model_selection/2102.12347v2_AutoAI-TS:_AutoAI_for_Time_Series_Forecasting.pdf\n",
      "\n",
      "--- Result 4 ---\n",
      "Chunk:\n",
      "AutoAI-TS: AutoAI for Time Series Forecasting\n",
      "Syed Yousaf Shah, Dhaval Patel, Long Vu, Xuan-Hong Dang, Bei Chen, Peter Kirchner, Horst\n",
      "Samulowitz, David Wood, Gregory Bramble, Wesley M. Gifford, Giridhar Ganapavarapu, Roman\n",
      "Vaculin, and Petros Zerfos\n",
      "IBM Thomas J. Watson Research Center\n",
      "Yorktown Hei...\n",
      "Source PDF: ./arxiv_data/model_selection/2102.12347v2_AutoAI-TS:_AutoAI_for_Time_Series_Forecasting.pdf\n",
      "\n",
      "--- Result 5 ---\n",
      "Chunk:\n",
      "oftheinputdatausinganadditionaltaskdrivenlayer.Apopularexampleisthecase\n",
      "of classiﬁcations, where this block is usually a linear operation followed by the\n",
      "cross-entropyloss function (detailed in Section 3).\n",
      "When approaching the analysis of data with varying length, such as sequential\n",
      "data, a variant ...\n",
      "Source PDF: ./arxiv_data/deep_learning_fundamentals/2003.03253v1_Introduction_to_deep_learning.pdf\n",
      "\n",
      "--- Result 6 ---\n",
      "Chunk:\n",
      "Kriegeskorte & Golan (2019) Neural network models and deep learning \n",
      " \n",
      "8 \n",
      " \n",
      " \n",
      "Figure 3 | Recurrent neural networks.  (a) A recurrent neural network models with two input \n",
      "units (in blue box), three hidden units (green box), and two output units (pink box). The hidden \n",
      "units here are fully recurrentl...\n",
      "Source PDF: ./arxiv_data/deep_learning_fundamentals/1902.04704v2_Neural_network_models_and_deep_learning_-_a_primer.pdf\n",
      "\n",
      "--- Result 7 ---\n",
      "Chunk:\n",
      "Introduction to deep learning 5\n",
      "More complex RNN structures include performing bi-directional calculations or\n",
      "addinggatingtothefeedbackandtheinputreceivedbythenetwork.Themostknown\n",
      "complexRNNarchitectureistheLong-Term-Short-Memory(LSTM)[46,37],which\n",
      "adds gates to the RNN. These gates decide what info...\n",
      "Source PDF: ./arxiv_data/deep_learning_fundamentals/2003.03253v1_Introduction_to_deep_learning.pdf\n",
      "\n",
      "--- Result 8 ---\n",
      "Chunk:\n",
      "input patterns form a two -dimensional space. The hidden and output units here use a sigmoid \n",
      "(logistic) activation function. Surface plots on the left show the activation of each unit as a function \n",
      "of the input pattern (horizontal plane spanned by inputs x 1 and x 2). For the output units, the \n",
      "pr...\n",
      "Source PDF: ./arxiv_data/deep_learning_fundamentals/1902.04704v2_Neural_network_models_and_deep_learning_-_a_primer.pdf\n",
      "\n",
      "--- Result 9 ---\n",
      "Chunk:\n",
      "Comparing machine learning models to choose the variable or dering for CAD 3\n",
      "and smaller dimensions; and then uses these to lift − to incrementally build\n",
      "decompositions of larger and larger spaces according to the polyno mials at that\n",
      "level. For full details on the original CAD algorithm see [3].\n",
      "QE...\n",
      "Source PDF: ./arxiv_data/model_selection/1904.11061v2_Comparing_machine_learning_models_to_choose_the_va.pdf\n",
      "\n",
      "--- Result 10 ---\n",
      "Chunk:\n",
      "into the higher classification accuracy. The proposed method obtains the highest accuracy in more than twelve medical dataset\n",
      "s and the \n",
      "possible reason b\n",
      "eing the \n",
      "diverse\n",
      " \n",
      "solution found by Pareto Optimal set. \n",
      "Moreover\n",
      ",\n",
      " \n",
      "the result indicates the proposed ensemble method \n",
      "has stronger performan...\n",
      "Source PDF: ./arxiv_data/feature_engineering/2004.07478v1_Multi-Objective_Evolutionary_approach_for_the_Perf.pdf\n",
      "\n",
      "--- Result 11 ---\n",
      "Chunk:\n",
      "Comparing machine learning models to choose the variable or dering for CAD 15\n",
      "gorithms for Scientiﬁc Computing (SYNASC '16). pp. 45–52. IEEE (2016),\n",
      "https://doi.org/10.1109/SYNASC.2016.020\n",
      "36. Huang, Z., England, M., Wilson, D., Bridge, J., Davenpor t, J.H., Paulson,\n",
      "L.: Using machine learning to im...\n",
      "Source PDF: ./arxiv_data/model_selection/1904.11061v2_Comparing_machine_learning_models_to_choose_the_va.pdf\n",
      "\n",
      "--- Result 12 ---\n",
      "Chunk:\n",
      "Geometric deep learning: Grids, groups, graphs, geodesics, and gauges.\n",
      "ArXiv, abs/2104.13478, 2021.\n",
      "[8] Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur D. Szlam,\n",
      "and Pierre Vandergheynst. Geometric deep learning: Going beyond\n",
      "euclidean data. IEEE Signal Processing Magazine , 34:18–42, 2016.\n",
      "[9]...\n",
      "Source PDF: ./arxiv_data/deep_learning_fundamentals/2305.05601v1_Deep_Learning_and_Geometric_Deep_Learning:_an_intr.pdf\n",
      "\n",
      "--- Result 13 ---\n",
      "Chunk:\n",
      "[392] Simonyan, K. and Zisserman, A.Very Deep Convolutional Networks for Large-\n",
      "Scale Image Recognition.arXiv:1409.1556 (2014). url: arxiv.org/abs/1409.1556.\n",
      "[393] Sirignano, J. and Spiliopoulos, K.DGM: A deep learning algorithm for solving\n",
      "partial differential equations.J. Comput. Phys.375 (2018), ...\n",
      "Source PDF: ./arxiv_data/deep_learning_fundamentals/2310.20360v2_Mathematical_Introduction_to_Deep_Learning:_Method.pdf\n",
      "\n",
      "--- Result 14 ---\n",
      "Chunk:\n",
      "relevant features autonomously. These features are automatically extracted from the training data by the CNN, with only the architecture of the neural network being pre-defined. Despite the effectiveness of these neural networks in adapting to new problems, some drawbacks are present. Deep neural ne...\n",
      "Source PDF: ./arxiv_data/deep_learning_fundamentals/1809.09645v1_Deep_Neural_Networks_for_Pattern_Recognition.pdf\n",
      "\n",
      "--- Result 15 ---\n",
      "Chunk:\n",
      "term Deep Learning is describing the procedure of performing\n",
      "machine learning tasks with deep artiﬁcial neural networks\n",
      "[15]. In reality, the best performing deep neural networks are\n",
      "nowadays consisting of hundreds of layers. Since it is often\n",
      "hard to understand what a speciﬁc neuron is recognizing,...\n",
      "Source PDF: ./arxiv_data/deep_learning_fundamentals/1803.02129v1_A_Non-Technical_Survey_on_Deep_Convolutional_Neura.pdf\n",
      "\n",
      "--- Result 16 ---\n",
      "Chunk:\n",
      "data analyses ( Li et al. , 2017).\n",
      "The necessity of feature selection has increased in recent years: Whereas in the last decades, a number\n",
      "of 50 to 100 features was called a “large” feature set ( Kudo and Sklansky , 2000, p. 25), today we are\n",
      "confronted with hundreds or even thousands ( Hua et al. ,...\n",
      "Source PDF: ./arxiv_data/model_selection/2111.12140v1_Filter_Methods_for_Feature_Selection_in_Supervised.pdf\n",
      "\n",
      "--- Result 17 ---\n",
      "Chunk:\n",
      "1 Introduction\n",
      "Machine Learning (ML) is a core technology of artiﬁcial intelligence (\n",
      "Russell and Norvig , 2016), which has\n",
      "rendered current outstanding applications of speech processing , image recognition, self-driving cars, and\n",
      "others possible. Next to these remarkable technological developm ents...\n",
      "Source PDF: ./arxiv_data/model_selection/2111.12140v1_Filter_Methods_for_Feature_Selection_in_Supervised.pdf\n",
      "\n",
      "--- Result 18 ---\n",
      "Chunk:\n",
      "which could also be implemented by transforming the features instea d). Moreover, in\n",
      "contrast to algorithm HPs, preprocessing HPs often cannot be se t by a single software\n",
      "function argument (for example, all HPs of the CART algorithm name d in the previous\n",
      "section can be speciﬁed within a single R f...\n",
      "Source PDF: ./arxiv_data/data_preprocessing/2412.03491v1_Beyond_algorithm_hyperparameters:_on_preprocessing.pdf\n",
      "\n",
      "--- Result 19 ---\n",
      "Chunk:\n",
      "learning algorithm is aﬀected by certain data set characteristics (e .g., whether the al-\n",
      "gorithm is sensitive to outliers in features, which requires some form of transformation;\n",
      "12...\n",
      "Source PDF: ./arxiv_data/data_preprocessing/2412.03491v1_Beyond_algorithm_hyperparameters:_on_preprocessing.pdf\n",
      "\n",
      "\n",
      "Answer:\n",
      " The question about transforming cyclical features to improve model accuracy is related to feature engineering and preprocessing techniques. Cyclical features refer to variables that follow a cyclical pattern, such as time series data or seasonal patterns in economic indicators.\n",
      "\n",
      "To transform cyclical features, you can consider the following methods:\n",
      "\n",
      "1. **Normalization**: Normalize the cyclical variable by subtracting its mean and dividing by its standard deviation. This helps to prevent overfitting and improves model interpretability.\n",
      "2. **Differencing**: Apply differencing techniques, such as first-order or second-order differences, to break the cyclical pattern and reduce the dimensionality of the data.\n",
      "3. **Seasonal decomposition**: Use seasonal decomposition methods, like STL decomposition (Trend, Seasonal, and Residual Decomposition), to separate the cyclical component from the non-cyclical components.\n",
      "4. **Exponential smoothing**: Apply exponential smoothing techniques, such as Simple Exponential Smoothing (SES) or Holt's method, to capture the cyclical pattern in the data.\n",
      "5. **Time series forecasting**: Use time series forecasting methods, like ARIMA, SARIMA, or Prophet, to model and forecast the cyclical patterns.\n",
      "\n",
      "These transformations can help improve model accuracy by:\n",
      "\n",
      "* Reducing overfitting caused by the cyclical pattern\n",
      "* Improving model interpretability\n",
      "* Capturing non-linear relationships in the data\n",
      "* Enhancing model generalizability\n",
      "\n",
      "However, it's essential to note that the choice of transformation method depends on the specific characteristics of your dataset and the type of problem you're trying to solve. Experimenting with different techniques and evaluating their impact on model performance can help determine the most effective approach for your particular use case.\n"
     ]
    }
   ],
   "source": [
    "# question = input(\"enter question\")\n",
    "multiquery_retrieved_docs = retrieve_multiquery_docs(question)\n",
    "answer = generate_rag_answer(multiquery_retrieved_docs, question, llm)\n",
    "print(\"\\nAnswer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a591a9ae-b58b-4d55-a765-67b69bbd2162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "57607a48-2ff7-4c9b-b45b-a946ec213f7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided documents, there were three main evolution paradigms of RAG (Retrieval-Augmented Generation) systems before Agentic RAG:\n",
      "\n",
      "1. Naïve RAG: This paradigm is mentioned in reference [16] as an early form of RAG. However, no further details are available.\n",
      "2. Modular RAG: Another evolution paradigm mentioned in the same reference [16]. Again, no specific details about this version are provided.\n",
      "3. Graph RAG: Also discussed in reference [16], another precursor to Agentic RAG.\n",
      "\n",
      "These three paradigms have evolved and eventually led to the development of Agentic RAG (Agentric Retrieval-Augmented Generation) systems, which integrate agents into the retrieval-augmentation pipeline for more dynamic and adaptive information processing.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "rag_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are an AI assistant answering user questions based on retrieved documents.\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Step 2: Format Retrieved Docs\n",
    "def format_docs(docs):\n",
    "    \"\"\"Extracts and joins text from retrieved documents\"\"\"\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Step 3: Generate Answer\n",
    "context_text = format_docs(relevant_docs)\n",
    "llm_chain = rag_prompt | llm  # Chain the prompt with LLM\n",
    "answer = llm_chain.invoke({\"context\": context_text, \"question\": question})\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc4fcd1-03b9-48cf-9df2-1c3dcf359845",
   "metadata": {},
   "source": [
    "<font size=\"4\"> Checking connection with LangSmith </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50de5127-f2c9-4bbc-b65b-1128b3c510db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n",
      "{'detail': 'Not Found'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {os.environ['LANGCHAIN_API_KEY']}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "response = requests.get(\"https://api.smith.langchain.com/v1/projects\", headers=headers)\n",
    "print(response.status_code)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54b728d-15ff-4286-9095-08c94af1bfad",
   "metadata": {},
   "source": [
    "<font size=\"4\"> Enhanching DB and create sub domains in DB </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da869dd9-cf1b-4bc4-99af-01a22974d83a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import arxiv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ------------------------------ SETUP ------------------------------\n",
    "\n",
    "BASE_DIR = \"./arxiv_data\"\n",
    "DB_DIR = \"./arxiv_vector1_db\"\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "# topics = {\n",
    "#   \"artificial_intelligence\": \"artificial intelligence OR AI OR intelligent systems OR autonomous agents\",\n",
    "#   \"hardware_architecture\": \"hardware architecture OR computer architecture OR processor design OR FPGA OR ASIC\",\n",
    "#   \"computational_complexity\": \"computational complexity OR complexity theory OR P vs NP OR time complexity\",\n",
    "#   \"computational_engineering_finance_science\": \"computational engineering OR computational finance OR computational science OR scientific computing\",\n",
    "#   \"computational_geometry\": \"computational geometry OR geometric algorithms OR convex hull OR Voronoi diagrams\",\n",
    "#   \"computation_and_language\": \"natural language processing OR NLP OR computational linguistics OR language models\",\n",
    "#   \"cryptography_and_security\": \"cryptography OR cybersecurity OR encryption OR secure communication\",\n",
    "#   \"computer_vision_and_pattern_recognition\": \"computer vision OR image recognition OR object detection OR pattern recognition\",\n",
    "#   \"computers_and_society\": \"ethics in computing OR social impact of AI OR digital privacy OR algorithmic bias\",\n",
    "#   \"databases\": \"databases OR database systems OR SQL OR NoSQL OR data storage OR data indexing\",\n",
    "#   \"distributed_parallel_and_cluster_computing\": \"distributed computing OR parallel computing OR cluster computing OR distributed systems\",\n",
    "#   \"digital_libraries\": \"digital libraries OR information archiving OR digital preservation OR metadata standards\",\n",
    "#   \"discrete_mathematics\": \"discrete mathematics OR graph theory OR combinatorics OR number theory\",\n",
    "#   \"data_structures_and_algorithms\": \"data structures OR algorithms OR algorithm analysis OR dynamic programming\",\n",
    "#   \"emerging_technologies\": \"emerging technologies OR quantum computing OR nanotechnology OR 6G\",\n",
    "#   \"formal_languages_and_automata_theory\": \"automata theory OR formal languages OR finite state machines OR grammars\",\n",
    "#   \"general_literature\": \"computer science surveys OR literature review OR overview of technologies\",\n",
    "#   \"graphics\": \"computer graphics OR rendering OR visualization OR 3D modeling\",\n",
    "#   \"cs_and_game_theory\": \"game theory OR mechanism design OR auctions OR strategic behavior\",\n",
    "#   \"human_computer_interaction\": \"human-computer interaction OR HCI OR usability OR user interfaces\",\n",
    "#   \"information_retrieval\": \"information retrieval OR search engines OR ranking algorithms OR relevance feedback\",\n",
    "#   \"information_theory\": \"information theory OR entropy OR data compression OR channel capacity\",\n",
    "#   \"machine_learning\": \"machine learning OR supervised learning OR unsupervised learning OR reinforcement learning\",\n",
    "#   \"logic_in_computer_science\": \"logic in computer science OR formal verification OR model checking OR theorem proving\",\n",
    "#   \"multiagent_systems\": \"multi-agent systems OR agent-based modeling OR cooperative agents OR swarm intelligence\",\n",
    "#   \"multimedia\": \"multimedia systems OR audio processing OR video streaming OR interactive media\",\n",
    "#   \"mathematical_software\": \"mathematical software OR symbolic computation OR computer algebra\",\n",
    "#   \"numerical_analysis\": \"numerical analysis OR numerical methods OR finite element methods OR PDE solvers\",\n",
    "#   \"neural_and_evolutionary_computing\": \"neural computing OR evolutionary algorithms OR genetic algorithms OR neuroevolution\",\n",
    "#   \"networking_and_internet_architecture\": \"networking OR internet architecture OR TCP/IP OR network protocols\",\n",
    "#   \"other_computer_science\": \"interdisciplinary computing OR unconventional computing OR computer science overview\",\n",
    "#   \"operating_systems\": \"operating systems OR kernel design OR file systems OR process scheduling\",\n",
    "#   \"performance\": \"system performance OR benchmarking OR latency optimization OR throughput\",\n",
    "#   \"programming_languages\": \"programming languages OR language design OR compiler construction OR type systems\",\n",
    "#   \"robotics\": \"robotics OR autonomous robots OR robot perception OR robot control\",\n",
    "#   \"symbolic_computation\": \"symbolic computation OR algebraic computation OR formal math reasoning\",\n",
    "#   \"sound\": \"digital audio OR sound processing OR speech analysis OR audio synthesis\",\n",
    "#   \"software_engineering\": \"software engineering OR software development OR agile OR software testing\",\n",
    "#   \"social_and_information_networks\": \"social networks OR information networks OR network analysis OR influence propagation\",\n",
    "#   \"systems_and_control\": \"control systems OR feedback control OR systems theory OR state-space models\"\n",
    "# }\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-hf\")\n",
    "\n",
    "def token_length_function(text):\n",
    "    try:\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        return len(tokenizer.encode(text))\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenization error for text: {text[:100]} - {e}\")\n",
    "        return 0  # or raise, depending on how you want to handle it\n",
    "\n",
    "\n",
    "def split_documents(docs):\n",
    "    print(\"Splitting documents into chunks...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=50,\n",
    "        length_function=token_length_function,\n",
    "    )\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "def download_arxiv_papers(query, max_results, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    print(f\" Searching arXiv for: {query}\")\n",
    "    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)\n",
    "    \n",
    "    pdf_paths = []\n",
    "    for result in search.results():\n",
    "        paper_id = result.get_short_id()\n",
    "        title = result.title.replace(\" \", \"_\").replace(\"/\", \"_\")[:50]\n",
    "        filename = f\"{paper_id}_{title}.pdf\"\n",
    "        filepath = os.path.join(save_dir, filename)\n",
    "\n",
    "        if not os.path.exists(filepath):\n",
    "            try:\n",
    "                print(f\" Downloading: {title}\")\n",
    "                result.download_pdf(dirpath=save_dir, filename=filename)\n",
    "            except Exception as e:\n",
    "                print(f\" Failed to download {title}: {e}\")\n",
    "                continue\n",
    "\n",
    "        pdf_paths.append(filepath)\n",
    "    return pdf_paths\n",
    "\n",
    "def process_topic(topic_query, topic_name, max_results=20):\n",
    "    print(f\"\\n🔍 Processing Topic: {topic_name}\")\n",
    "    topic_dir = os.path.join(BASE_DIR, topic_name)\n",
    "    pdf_paths = download_arxiv_papers(topic_query, max_results, topic_dir)\n",
    "\n",
    "    all_docs = []\n",
    "    for path in pdf_paths:\n",
    "        try:\n",
    "            loader = PyPDFLoader(path)\n",
    "            docs = loader.load()\n",
    "            for doc in docs:\n",
    "                doc.metadata[\"sub_domain\"] = topic_name\n",
    "            all_docs.extend(docs)\n",
    "        except Exception as e:\n",
    "            print(f\" Failed to load {path}: {e}\")\n",
    "    return all_docs\n",
    "\n",
    "def load_or_create_vector_db():\n",
    "    print(\" Loading or creating vector DB...\")\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    if os.path.exists(DB_DIR) and os.listdir(DB_DIR):\n",
    "        print(\" Existing DB found. Loading...\")\n",
    "        vector_db = Chroma(persist_directory=DB_DIR, embedding_function=embedding_model)\n",
    "    else:\n",
    "        print(\" No DB found. Will create a new one.\")\n",
    "        vector_db = None\n",
    "    return vector_db, embedding_model\n",
    "\n",
    "def add_chunks_to_db(chunks, vector_db, embedding_model):\n",
    "    if not chunks:\n",
    "        print(\" No chunks to add to DB. Skipping.\")\n",
    "        return vector_db\n",
    "\n",
    "    if vector_db is None:\n",
    "        print(\" Creating new DB from chunks...\")\n",
    "        vector_db = Chroma.from_documents(documents=chunks, embedding=embedding_model, persist_directory=DB_DIR)\n",
    "    else:\n",
    "        print(f\" Adding {len(chunks)} new chunks to existing DB...\")\n",
    "        vector_db.add_documents(chunks)\n",
    "    vector_db.persist()\n",
    "    return vector_db\n",
    "\n",
    "# ------------------------------ MAIN ------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e134d40d-d323-4ae5-b796-7c844d934f92",
   "metadata": {},
   "source": [
    "<font size=\"5\">Loading all sub dir together</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb08939d-f8a5-4285-8f71-2189e0d26284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def load_and_process_pdfs(\n",
    "    main_folder=\"./arxiv_data\",\n",
    "    persist_directory=\"./s2_chroma_db\",\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    tokenizer_model_name=\"NousResearch/Llama-2-7b-hf\"\n",
    "):\n",
    "    print(\"\\nLoading all PDFs from subdirectories...\")\n",
    "    all_docs = []\n",
    "\n",
    "    for subdir, dirs, files in os.walk(main_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pdf\"):\n",
    "                path = os.path.join(subdir, file)\n",
    "                try:\n",
    "                    loader = PyPDFLoader(path)\n",
    "                    docs = loader.load()\n",
    "                    \n",
    "                    filename = os.path.basename(path)\n",
    "                    arxiv_id = filename.replace(\".pdf\", \"\").split(\"_\")[0]\n",
    "                    arxiv_url = f\"https://arxiv.org/pdf/{arxiv_id}\"\n",
    "                    subdomain = os.path.basename(subdir)\n",
    "\n",
    "                    for doc in docs:\n",
    "                        doc.metadata[\"source\"] = arxiv_url\n",
    "                        doc.metadata[\"subdomain\"] = subdomain\n",
    "\n",
    "                    all_docs.extend(docs)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to load {path}: {e}\")\n",
    "\n",
    "    print(\"Splitting documents...\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)\n",
    "    def token_length_function(text):\n",
    "        if isinstance(text, str):\n",
    "            return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "        else:\n",
    "            raise TypeError(\"Expected input to be a string.\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=token_length_function,\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(all_docs)\n",
    "\n",
    "    print(\"Embedding and creating Chroma DB...\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "    vector_db = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    vector_db.persist()\n",
    "\n",
    "    print(f\"\\n VectorDB created with {len(chunks)} chunks!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6da3f9-ef83-4200-817c-822215c42b87",
   "metadata": {},
   "source": [
    "<font size=\"5\">Try loading one by one sub dir</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4aa149-0619-4d02-9f9d-1e46264f770e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: ./arxiv_data/software_engineering\n",
      "\n",
      "Loading all PDFs from subdirectories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 62 0 (offset 0)\n",
      "Ignoring wrong pointing object 64 0 (offset 0)\n",
      "Ignoring wrong pointing object 69 0 (offset 0)\n",
      "Ignoring wrong pointing object 82 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 81 0 (offset 0)\n",
      "Ignoring wrong pointing object 83 0 (offset 0)\n",
      "Ignoring wrong pointing object 85 0 (offset 0)\n",
      "Ignoring wrong pointing object 100 0 (offset 0)\n",
      "Ignoring wrong pointing object 102 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (979 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and creating Chroma DB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 77 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " VectorDB created with 424 chunks!\n",
      "\n",
      "Processing: ./arxiv_data/computer_vision_and_pattern_recognition\n",
      "\n",
      "Loading all PDFs from subdirectories...\n",
      "Splitting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (688 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and creating Chroma DB...\n",
      "\n",
      " VectorDB created with 528 chunks!\n",
      "\n",
      "Processing: ./arxiv_data/computational_geometry\n",
      "\n",
      "Loading all PDFs from subdirectories...\n",
      "Splitting documents...\n",
      "Embedding and creating Chroma DB...\n",
      "\n",
      " VectorDB created with 1008 chunks!\n",
      "\n",
      "Processing: ./arxiv_data/human_computer_interaction\n",
      "\n",
      "Loading all PDFs from subdirectories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 61 0 (offset 0)\n",
      "Ignoring wrong pointing object 67 0 (offset 0)\n",
      "Ignoring wrong pointing object 73 0 (offset 0)\n",
      "Ignoring wrong pointing object 83 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (762 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and creating Chroma DB...\n",
      "\n",
      " VectorDB created with 681 chunks!\n",
      "\n",
      "Processing: ./arxiv_data/multiagent_systems\n",
      "\n",
      "Loading all PDFs from subdirectories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 76 0 (offset 0)\n",
      "Ignoring wrong pointing object 78 0 (offset 0)\n",
      "Ignoring wrong pointing object 80 0 (offset 0)\n",
      "Ignoring wrong pointing object 85 0 (offset 0)\n",
      "Ignoring wrong pointing object 87 0 (offset 0)\n",
      "Ignoring wrong pointing object 96 0 (offset 0)\n",
      "Ignoring wrong pointing object 125 0 (offset 0)\n",
      "Ignoring wrong pointing object 127 0 (offset 0)\n",
      "Ignoring wrong pointing object 129 0 (offset 0)\n",
      "Ignoring wrong pointing object 131 0 (offset 0)\n",
      "Ignoring wrong pointing object 133 0 (offset 0)\n",
      "Ignoring wrong pointing object 135 0 (offset 0)\n",
      "Ignoring wrong pointing object 137 0 (offset 0)\n",
      "Ignoring wrong pointing object 139 0 (offset 0)\n",
      "Ignoring wrong pointing object 141 0 (offset 0)\n",
      "Ignoring wrong pointing object 146 0 (offset 0)\n",
      "Ignoring wrong pointing object 148 0 (offset 0)\n",
      "Ignoring wrong pointing object 153 0 (offset 0)\n",
      "Ignoring wrong pointing object 155 0 (offset 0)\n",
      "Ignoring wrong pointing object 160 0 (offset 0)\n",
      "Ignoring wrong pointing object 162 0 (offset 0)\n",
      "Ignoring wrong pointing object 164 0 (offset 0)\n",
      "Ignoring wrong pointing object 177 0 (offset 0)\n",
      "Ignoring wrong pointing object 194 0 (offset 0)\n",
      "Ignoring wrong pointing object 196 0 (offset 0)\n",
      "Ignoring wrong pointing object 198 0 (offset 0)\n",
      "Ignoring wrong pointing object 200 0 (offset 0)\n",
      "Ignoring wrong pointing object 202 0 (offset 0)\n",
      "Ignoring wrong pointing object 207 0 (offset 0)\n",
      "Ignoring wrong pointing object 209 0 (offset 0)\n",
      "Ignoring wrong pointing object 211 0 (offset 0)\n",
      "Ignoring wrong pointing object 225 0 (offset 0)\n",
      "Ignoring wrong pointing object 232 0 (offset 0)\n",
      "Ignoring wrong pointing object 244 0 (offset 0)\n",
      "Ignoring wrong pointing object 254 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (954 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and creating Chroma DB...\n",
      "\n",
      " VectorDB created with 616 chunks!\n",
      "\n",
      "Processing: ./arxiv_data/databases\n",
      "\n",
      "Loading all PDFs from subdirectories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n",
      "Ignoring wrong pointing object 88 0 (offset 0)\n",
      "Ignoring wrong pointing object 109 0 (offset 0)\n",
      "Ignoring wrong pointing object 112 0 (offset 0)\n",
      "Ignoring wrong pointing object 114 0 (offset 0)\n",
      "Ignoring wrong pointing object 120 0 (offset 0)\n",
      "Ignoring wrong pointing object 173 0 (offset 0)\n",
      "Ignoring wrong pointing object 175 0 (offset 0)\n",
      "Ignoring wrong pointing object 177 0 (offset 0)\n",
      "Ignoring wrong pointing object 191 0 (offset 0)\n",
      "Ignoring wrong pointing object 193 0 (offset 0)\n",
      "Ignoring wrong pointing object 195 0 (offset 0)\n",
      "Ignoring wrong pointing object 200 0 (offset 0)\n",
      "Ignoring wrong pointing object 202 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (677 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and creating Chroma DB...\n",
      "\n",
      " VectorDB created with 1008 chunks!\n",
      "\n",
      "Processing: ./arxiv_data/mathematical_software\n",
      "\n",
      "Loading all PDFs from subdirectories...\n",
      "Splitting documents...\n",
      "Embedding and creating Chroma DB...\n",
      "\n",
      " VectorDB created with 714 chunks!\n",
      "\n",
      "Processing: ./arxiv_data/discrete_mathematics\n",
      "\n",
      "Loading all PDFs from subdirectories...\n",
      "Splitting documents...\n",
      "Embedding and creating Chroma DB...\n",
      "\n",
      " VectorDB created with 1113 chunks!\n",
      "\n",
      "Processing: ./arxiv_data/general_literature\n",
      "\n",
      "Loading all PDFs from subdirectories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 210 0 (offset 0)\n",
      "EOF marker not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load ./arxiv_data/general_literature/2005.04681v2_Simulating_quantum_dynamics:_Evolution_of_algorith.pdf: Stream has ended unexpectedly\n",
      "Splitting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and creating Chroma DB...\n",
      "\n",
      " VectorDB created with 413 chunks!\n",
      "\n",
      "Processing: ./arxiv_data/systems_and_control\n",
      "\n",
      "Loading all PDFs from subdirectories...\n",
      "Splitting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (641 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and creating Chroma DB...\n",
      "\n",
      " VectorDB created with 810 chunks!\n",
      "\n",
      "Processing: ./arxiv_data/robotics\n",
      "\n",
      "Loading all PDFs from subdirectories...\n",
      "Splitting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (660 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and creating Chroma DB...\n",
      "\n",
      " VectorDB created with 691 chunks!\n",
      "\n",
      "Processing: ./arxiv_data/machine_learning\n",
      "\n",
      "Loading all PDFs from subdirectories...\n",
      "Splitting documents...\n",
      "Embedding and creating Chroma DB...\n",
      "\n",
      " VectorDB created with 632 chunks!\n",
      "\n",
      "Processing: ./arxiv_data/programming_languages\n",
      "\n",
      "Loading all PDFs from subdirectories...\n",
      "Splitting documents...\n",
      "Embedding and creating Chroma DB...\n",
      "\n",
      " VectorDB created with 1302 chunks!\n",
      "\n",
      "Processing: ./arxiv_data/other_computer_science\n",
      "\n",
      "Loading all PDFs from subdirectories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Ignoring wrong pointing object 81 0 (offset 0)\n",
      "Ignoring wrong pointing object 93 0 (offset 0)\n",
      "Ignoring wrong pointing object 95 0 (offset 0)\n",
      "Ignoring wrong pointing object 117 0 (offset 0)\n",
      "Ignoring wrong pointing object 119 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 87 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (600 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and creating Chroma DB...\n",
      "\n",
      " VectorDB created with 560 chunks!\n",
      "\n",
      "Processing: ./arxiv_data/computational_engineering_finance_science\n",
      "\n",
      "Loading all PDFs from subdirectories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 81 0 (offset 0)\n",
      "Ignoring wrong pointing object 93 0 (offset 0)\n",
      "Ignoring wrong pointing object 95 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1655 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and creating Chroma DB...\n",
      "\n",
      " VectorDB created with 470 chunks!\n",
      "\n",
      "Processing: ./arxiv_data/symbolic_computation\n",
      "\n",
      "Loading all PDFs from subdirectories...\n",
      "Splitting documents...\n",
      "Embedding and creating Chroma DB...\n",
      "\n",
      " VectorDB created with 879 chunks!\n",
      "\n",
      "Processing: ./arxiv_data/artificial_intelligence\n",
      "\n",
      "Loading all PDFs from subdirectories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "EOF marker not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load ./arxiv_data/artificial_intelligence/2412.01992v1_ChatCollab:_Exploring_Collaboration_Between_Humans.pdf: Stream has ended unexpectedly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EOF marker not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load ./arxiv_data/artificial_intelligence/2204.10358v1_Creative_Problem_Solving_in_Artificially_Intellige.pdf: Stream has ended unexpectedly\n",
      "Splitting documents...\n",
      "Embedding and creating Chroma DB...\n",
      "\n",
      " VectorDB created with 385 chunks!\n",
      "\n",
      "Processing: ./arxiv_data/digital_libraries\n",
      "\n",
      "Loading all PDFs from subdirectories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (638 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and creating Chroma DB...\n",
      "\n",
      " VectorDB created with 436 chunks!\n",
      "\n",
      "Processing: ./arxiv_data/information_retrieval\n",
      "\n",
      "Loading all PDFs from subdirectories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "could not convert string to float: b'0.00-24254184' : FloatObject (b'0.00-24254184') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.00-91074704' : FloatObject (b'0.00-91074704') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.00-19984013' : FloatObject (b'0.00-19984013') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.00-30147724' : FloatObject (b'0.00-30147724') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.00-9107471' : FloatObject (b'0.00-9107471') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.00-22356359' : FloatObject (b'0.00-22356359') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.00-91074704' : FloatObject (b'0.00-91074704') invalid; use 0.0 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents...\n",
      "Embedding and creating Chroma DB...\n",
      "\n",
      " VectorDB created with 860 chunks!\n",
      "\n",
      "Processing: ./arxiv_data/operating_systems\n",
      "\n",
      "Loading all PDFs from subdirectories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Advanced encoding /SymbolSetEncoding not implemented yet\n",
      "Advanced encoding /SymbolSetEncoding not implemented yet\n",
      "Advanced encoding /SymbolSetEncoding not implemented yet\n",
      "Advanced encoding /SymbolSetEncoding not implemented yet\n",
      "Advanced encoding /SymbolSetEncoding not implemented yet\n",
      "Advanced encoding /SymbolSetEncoding not implemented yet\n",
      "Advanced encoding /SymbolSetEncoding not implemented yet\n",
      "Advanced encoding /SymbolSetEncoding not implemented yet\n",
      "Advanced encoding /SymbolSetEncoding not implemented yet\n",
      "Advanced encoding /SymbolSetEncoding not implemented yet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents...\n",
      "Embedding and creating Chroma DB...\n",
      "\n",
      " VectorDB created with 683 chunks!\n",
      "\n",
      "Processing: ./arxiv_data/cryptography_and_security\n",
      "\n",
      "Loading all PDFs from subdirectories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 78 0 (offset 0)\n",
      "Ignoring wrong pointing object 87 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1629 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and creating Chroma DB...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "main_folder = \"./arxiv_data\"\n",
    "subdirs = [os.path.join(main_folder, d) for d in os.listdir(main_folder) if os.path.isdir(os.path.join(main_folder, d))]\n",
    "\n",
    "for subdir in subdirs:\n",
    "    print(f\"\\nProcessing: {subdir}\")\n",
    "    try:\n",
    "        load_and_process_pdfs(\n",
    "            main_folder=subdir,\n",
    "            persist_directory=\"arxiv_vector1_db\",\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=50,\n",
    "            embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            tokenizer_model_name=\"bert-base-uncased\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {subdir}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d9d560-b57e-4535-8266-c0316982f087",
   "metadata": {},
   "source": [
    "<font size=\"4\"> Retrieving docs</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9457bcb7-cc4d-42ca-aab3-b217f25394a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Setup your Chroma DB path and embedding model\n",
    "DB_DIR = \"./arxiv_vector1_db\"\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_db = Chroma(persist_directory=DB_DIR, embedding_function=embedding_model)\n",
    "\n",
    "def retrieve_docs(question, top_k=5):\n",
    "    print(f\"\\nQuery: {question}\\n\")\n",
    "\n",
    "    results = vector_db.similarity_search_with_relevance_scores(question, k=top_k)\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"--- Result {i} ---\")\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        print(f\"Chunk:\\n{doc.page_content[:300]}...\")  # first 300 chars\n",
    "        print(f\"Source PDF: {doc.metadata.get('source')}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc28621f-1d24-4689-a178-66ac814f7410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter question:  What are the key concepts behind Agentic RAG systems?\n"
     ]
    }
   ],
   "source": [
    "query = input(\"enter question: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "97b38cbe-4e9f-4088-8c05-e5aa2b113a27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What are the key concepts behind Agentic RAG systems?\n",
      "\n",
      "--- Result 1 ---\n",
      "Score: 0.1430\n",
      "Chunk:\n",
      "The idea of humans being assisted by artiﬁcial autonomous\n",
      "systems can be found throughout human history. Ancient\n",
      "mythology describes Cadmus (ca. 2000 BCE), who sowed\n",
      "dragon teeth that turned into soldiers. Aristotle speculated\n",
      "that automata could replace human slavery: “There is only\n",
      "one condition i...\n",
      "Source PDF: https://arxiv.org/pdf/2502.02649v2\n",
      "\n",
      "--- Result 2 ---\n",
      "Score: 0.1243\n",
      "Chunk:\n",
      "8 to multi-step customer support sys-\n",
      "tems.9\n",
      "T o better understand what an AI agent is, we therefore re-\n",
      "view currently available AI agents and AI agent platforms\n",
      "(examples provided in footnotes throughout this document)\n",
      "as well as historical literature on the promise of AI agents\n",
      "(references throug...\n",
      "Source PDF: https://arxiv.org/pdf/2502.02649v2\n",
      "\n",
      "--- Result 3 ---\n",
      "Score: 0.1020\n",
      "Chunk:\n",
      "oriented” toward a certain state of the world.\n",
      "W ooldridge & Jennings (1995): “Perhaps the most general way in which the term agent is use d is to denote a hardware\n",
      "or (more usually) software-based computer system that enjo ys the following properties: • autonomy: agents operate\n",
      "without the direct i...\n",
      "Source PDF: https://arxiv.org/pdf/2502.02649v2\n",
      "\n",
      "--- Result 4 ---\n",
      "Score: 0.0986\n",
      "Chunk:\n",
      "4 \n",
      "gestures take various forms. Illustration is acted out speech, Emblems are actions that \n",
      "replace words and Deixis is a combination of gestures and voice \n",
      "•  Provide consequential communication of an individua l’s embodiment \n",
      "Bodily actions unintentionally give off information about who is in the ...\n",
      "Source PDF: https://arxiv.org/pdf/0707.3638v1\n",
      "\n",
      "--- Result 5 ---\n",
      "Score: 0.0921\n",
      "Chunk:\n",
      "actions on behalf of a user–across one or more domains–in lin e with the user’s expectations. ”\n",
      "Park et al. (2024a): can accurately simulate behavior across many contexts\n",
      "Sierra AI (2024) : The magic of AI agents—from both the technological and bus iness perspectives—comes through when\n",
      "they demonstr...\n",
      "Source PDF: https://arxiv.org/pdf/2502.02649v2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrieve_docs(query)\n",
    "#What are common techniques for handling missing values in datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f1eb8aa-38cd-412b-9dd9-0d449aea5b50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: How does a router differ from a switch?\n",
      "\n",
      "--- Result 1 ---\n",
      "Score: 0.0258\n",
      "Chunk:\n",
      "DHCP on a Linux workstation? iv) Explain what is subnetting and show an example of a network with two subnets. Consider that the network has the block IP 10.14.15.0 and mask /24 assigned. Show the range of IP addresses of the subnets (all the network).  v) Describe the function of the traceroute pro...\n",
      "Source PDF: https://arxiv.org/pdf/2308.01713v2_Experiments_on_Computer_Networks:_Quickly_Knowing_\n",
      "\n",
      "--- Result 2 ---\n",
      "Score: 0.0075\n",
      "Chunk:\n",
      "paper. This paper deals with the topology with router node\n",
      "degree equal to 4 (Figure 3) and the topology with router\n",
      "node degree equal to 3 (Figure 4).\n",
      "The topology in Figure 3 offers multiple paths with different\n",
      "costs (in terms of delays) to peers (the bandwidth of the links\n",
      "is the same). The one ...\n",
      "Source PDF: https://arxiv.org/pdf/1807.05061v1_Routing_and_Forwarding_in_nTorrent_using_ndnSIM\n",
      "\n",
      "--- Result 3 ---\n",
      "Score: -0.0096\n",
      "Chunk:\n",
      "the ethernet switch. Furthermore, the HPCC FPGA version from the original\n",
      "benchmark without ACCL was used to retrieve data for a purely CPU-based\n",
      "baseline. The two FPGAs are located on different nodes, such that data transfers\n",
      "of the baseline version use the Infiniband network of the hosts via MPI....\n",
      "Source PDF: https://arxiv.org/pdf/2403.18374v2_Optimizing_Communication_for_Latency_Sensitive_HPC\n",
      "\n",
      "--- Result 4 ---\n",
      "Score: -0.0153\n",
      "Chunk:\n",
      "33 ECE429 Computer Communications, R. Rojas-Cessa  \n",
      "4.2\tRouting\tIn the following exercises, a router is used to interconnect subnets, and in turn, hosts in the different subnets. The following exercises start with a single router. The number of routers increases as the chapter progresses. First, you...\n",
      "Source PDF: https://arxiv.org/pdf/2308.01713v2_Experiments_on_Computer_Networks:_Quickly_Knowing_\n",
      "\n",
      "--- Result 5 ---\n",
      "Score: -0.0169\n",
      "Chunk:\n",
      "layer. \n",
      " Manages the state (waiting, carried, done etc.) and \n",
      "localization of -containers. \n",
      " Manages the scheduling of the -containers on these -\n",
      "means (to ensure that the maximum weight limit of a band \n",
      "conveyor is not exceeded, for example) or the mapping of \n",
      "the -containers (which ones shou...\n",
      "Source PDF: https://arxiv.org/pdf/1904.05069v1_A_Proposal_for_an_Open_Logistics_Interconnection_R\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5n/8m4t00412h97v592vh5y0pgc0000gn/T/ipykernel_5165/4055242714.py:12: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'creationdate': \"D:20230810105152Z00'00'\", 'creator': 'Word', 'moddate': \"D:20230810105152Z00'00'\", 'page': 27, 'page_label': '28', 'producer': 'macOS Version 12.3 (Build 21E230) Quartz PDFContext', 'source': 'https://arxiv.org/pdf/2308.01713v2_Experiments_on_Computer_Networks:_Quickly_Knowing_', 'subdomain': 'networking_and_internet_architecture', 'title': 'Microsoft Word - ECE429-Computer-Communications-Laboratory-v31-202300802.docx', 'total_pages': 70}, page_content='DHCP on a Linux workstation? iv) Explain what is subnetting and show an example of a network with two subnets. Consider that the network has the block IP 10.14.15.0 and mask /24 assigned. Show the range of IP addresses of the subnets (all the network).  v) Describe the function of the traceroute program and provide a simple example on how to use it. vi) Describe what is routing in the Internet. vii) Describe what the echo command\\tdoes\\tand\\tshow\\tand\\texample\\tof\\tits\\tusage.'), 0.025799198905850584), (Document(metadata={'author': '', 'creationdate': '2018-07-16T01:04:38+00:00', 'creator': 'LaTeX with hyperref package', 'keywords': '', 'moddate': '2018-07-16T01:04:38+00:00', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.17', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'source': 'https://arxiv.org/pdf/1807.05061v1_Routing_and_Forwarding_in_nTorrent_using_ndnSIM', 'subdomain': 'networking_and_internet_architecture', 'subject': '', 'title': '', 'total_pages': 6, 'trapped': '/False'}, page_content='paper. This paper deals with the topology with router node\\ndegree equal to 4 (Figure 3) and the topology with router\\nnode degree equal to 3 (Figure 4).\\nThe topology in Figure 3 offers multiple paths with different\\ncosts (in terms of delays) to peers (the bandwidth of the links\\nis the same). The one in Figure 4 has bottleneck links to the\\nmajority of the peers. We assume that the .torrent ﬁle is known\\nby all the peers, and the torrent to be downloaded contains\\n1KB data and consists of 2 ﬁles. The ﬁle sizes used in this\\npaper are much smaller than that used in the paper [13] to\\nspeed up simulation and aid in rapid design and test iterations.\\nAs a result, the bandwidth rates are modiﬁed proportionally.\\nIn the topology described in Figure 3, Peer 4 acts as\\nthe seeder and announces preﬁxes across the network. The\\nannouncements propagate among the routers due to the routing\\nprotocol. Peer 1 starts downloading data ﬁrst. After 5 seconds,\\npeer 3 starts downloading data. After 5 more seconds, peer 2\\nstarts its downloading; both peers 1 and 3 still act as leechers\\nallowing us to study the effect of NDN forwarding. In the\\ntopology described in Figure 4, peer 1 is the seeder while\\npeers 2, 3, 4 and 5 act as leechers.\\nFig. 3. Topology with router node degree equal to 4.\\nFig. 4. Topology with router node degree equal to 3.\\nThis section presents a few simulation screenshots using\\nthe visualizer tool of ns-3. For the sake of simplicity, we ﬁrst\\nassume a toy example topology consisting of a node that acts\\nas a producer and a node that acts as a consumer (Figure 5).\\nThe consumer node ﬁrst requests the torrent-ﬁle from the\\nproducer. For now, the implemented fetching strategy1 requires\\nthat a consumer fetches all the segments of the torrent-ﬁle\\nbefore it starts downloading the ﬁle manifests.\\nAfter all the ﬁle manifests have been fetched, the consumer\\nstarts downloading the individual data packets from the pro-\\nducer. Speciﬁcally, the consumer expresses Interests iteratively\\nfor all the data packets included in each ﬁle manifest. The\\noverall process is illustrated in Figure 6 and screenshots of'), 0.0074502312318448505), (Document(metadata={'author': \"Marius Meyer; Tobias Kenter; Lucian Petrica; Kenneth O'Brien; Michaela Blott; Christian Plessl\", 'creationdate': '2024-04-09T01:59:24+00:00', 'creator': 'LaTeX via pandoc', 'keywords': 'FPGA, HLS, HPC, inter-FPGA Communication', 'moddate': '2024-04-09T01:59:24+00:00', 'page': 5, 'page_label': '6', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'https://arxiv.org/pdf/2403.18374v2_Optimizing_Communication_for_Latency_Sensitive_HPC', 'subdomain': 'performance', 'subject': '', 'title': 'Optimizing Communication for Latency Sensitive HPC Applications on up to 48 FPGAs Using ACCL', 'total_pages': 15, 'trapped': '/False'}, page_content='the ethernet switch. Furthermore, the HPCC FPGA version from the original\\nbenchmark without ACCL was used to retrieve data for a purely CPU-based\\nbaseline. The two FPGAs are located on different nodes, such that data transfers\\nof the baseline version use the Infiniband network of the hosts via MPI.'), -0.009578444265708752), (Document(metadata={'creationdate': \"D:20230810105152Z00'00'\", 'creator': 'Word', 'moddate': \"D:20230810105152Z00'00'\", 'page': 32, 'page_label': '33', 'producer': 'macOS Version 12.3 (Build 21E230) Quartz PDFContext', 'source': 'https://arxiv.org/pdf/2308.01713v2_Experiments_on_Computer_Networks:_Quickly_Knowing_', 'subdomain': 'networking_and_internet_architecture', 'title': 'Microsoft Word - ECE429-Computer-Communications-Laboratory-v31-202300802.docx', 'total_pages': 70}, page_content='33 ECE429 Computer Communications, R. Rojas-Cessa  \\n4.2\\tRouting\\tIn the following exercises, a router is used to interconnect subnets, and in turn, hosts in the different subnets. The following exercises start with a single router. The number of routers increases as the chapter progresses. First, you will make a software router, which is a workstation configured to work as a router. Workstations used as routers must have at least two network cards. Setting a Linux workstation to function as an IP Router Configuring a Linux system as an IP router involves two steps: (1) modifying the configuration of Linux, so that IP forwarding is enabled and (2) configuring the routing table to provide information about the network. Exercise\\t4.4 Enabling a Linux host to perform forwarding. Step 1. Enable the Linux system to temporarily forward packets between interfaces.  To enable a Linux system to forward packets from one interface to another make the file /proc/sys/net/ipv4/ip_forward to contain a 1. To disable it, make it 0. To enable IP forwarding, by setting 1 ip-forwarding in the file, use the command: sudo su  echo “1” > /proc/sys/net/ipv4/ip_forward Check if IP forwarding is enabled by using either one of the following commands: sysctl net.ipv4.ip_forward or cat /proc/sys/net/ipv4/ip_forward Optionally, you can execute the following command to make a workstation able to forward packets (as a root user): sysctl net.ipv4.ip_forward=1 Remember to use a single method to enable forwarding in a workstation to avoid OS confusion. Note: Additional commands to set IP forwarding are described in the Appendix and the end of the chapter, in case troubleshooting is needed. Step 2. Configure the IP addresses of the hosts to have a different subnet for each interface of the router. Configure the IP addresses of the router’s interfaces to be able to communicate with the hosts. Setting IP routing table (static routing)'), -0.015263548367646207), (Document(metadata={'author': 'IEEE', 'creationdate': '2016-04-26T15:30:11+02:00', 'creator': 'Microsoft® Office Word 2007', 'moddate': '2016-04-26T15:30:11+02:00', 'page': 3, 'page_label': '4', 'producer': 'Microsoft® Office Word 2007', 'source': 'https://arxiv.org/pdf/1904.05069v1_A_Proposal_for_an_Open_Logistics_Interconnection_R', 'subdomain': 'networking_and_internet_architecture', 'title': 'Paper Title (use style: paper title)', 'total_pages': 7}, page_content='layer. \\n\\uf0b7 Manages the state (waiting, carried, done etc.) and \\nlocalization of \\uf070-containers. \\n\\uf0b7 Manages the scheduling of the \\uf070-containers on these \\uf070-\\nmeans (to ensure that the maximum weight limit of a band \\nconveyor is not exceeded, for example) or the mapping of \\nthe \\uf070-containers (which ones should be above on a \\ncontainer ship, etc. ) \\n\\uf0b7 Gives the orders to the \\uf070-means. \\n\\uf0b7 Signals \\uf070-means problems (breakdown s, delays) to the \\nLink layer. \\nThis layer does not define the \\uf070-containers and their \\ncontents. \\nB. The Link Layer \\nThe Link Layer manages the individual steps of movements \\nof \\uf070-containers on \\uf070-means. A \"step\" is one individual point -\\nto-point movement. The Link Layer receives blocks from the \\nNetwork Layer with the starting and the ending location of \\neach block.  \\nThe Link Layer divides and/or combine received blocks \\ninto several \"shipments\" and allocates a \\uf070-mean to each \\nshipment to handle it for this step. \\nAlthough this may not be a physical move in some cases, \\nthe Link Layer also manages the handling of a block by a \\ncompany/operator to another company/operator. \\nC. The Network Layer \\nThe Network Layer receives loads of \\uf070-containers from the \\nTransport Layer, w ith an initial starting and a final ending \\nlocation for each load. The Network Layer divides and/or \\ncombines the received loads into \"blocks\". \\nThe Network Layer computes and manages the routing of \\neach block from its initial starting location to its final ending \\nlocation. The Network Layer manages and maintains the data \\nstructures necessary to compute the best paths for the blocks. \\nD. The Transport Layer \\nThe Transport Layer receives orders made of \\uf070-containers \\nfrom the Order Layer, with an initial starting and a final ending \\nlocation for each order.  The Transport Layer divides and/or \\ncombines the received orders into \"loads\". \\nThe Transport Layer manages the end -to-end trip of each \\nload from its initial starting location to its final ending location. \\nIt checks t hat the final ending location can handle a load \\nshipped there. It signals to the Order Layer the initial departure, \\nthe current location and the final arrival of each \\uf070-container. \\nThe Transport Layer ensures that deadlines are respected. \\nE. The Order Layer \\nThe Order Layer receives sets of \\uf070-containers from the \\nContainer Layer, with an initial starting and a final ending \\nlocation for each set. The Order Layer establishes the \"dispatch \\nnote\" associated to each \\uf070-container of each set. It also records'), -0.016938885595954245)]\n",
      "  results = vector_db.similarity_search_with_relevance_scores(question, k=top_k)\n"
     ]
    }
   ],
   "source": [
    "retrieve_docs(query)\n",
    "#What is the difference between TCP and UDP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba72a15-45ca-41d2-a7f2-bd79455b0cf2",
   "metadata": {},
   "source": [
    "<font size=\"5\"> Query Refinement </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae384e1-1f4c-4f45-aecc-9885bf1b9cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "\n",
    "# Load dictionary\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2)\n",
    "sym_spell.load_dictionary(\"frequency_dictionary_en_82_765.txt\", term_index=0, count_index=1)\n",
    "\n",
    "def clean_query(query):\n",
    "    query = query.lower()\n",
    "    query = re.sub(r'[^\\w\\s]', '', query)  # remove punctuation\n",
    "    query = re.sub(r'\\s+', ' ', query).strip()\n",
    "\n",
    "    # Spelling correction\n",
    "    suggestions = sym_spell.lookup_compound(query, max_edit_distance=2)\n",
    "    if suggestions:\n",
    "        return suggestions[0].term\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cae75f2-b54e-498f-bd3c-0facab8b5e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Best paper on gans in cv!!\"\n",
    "print(clean_query(query))\n",
    "# Output: \"best paper on gans in cv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cb5577-bbd5-48ed-9e0a-255e4e6e9bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df1b2662-c86e-42ec-a56d-1cafd6bb75eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import re\n",
    "\n",
    "# Load embedding model once\n",
    "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        lines = re.findall(r\"^\\d+\\.\\s+(.*)\", text, re.MULTILINE)\n",
    "        return [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "# Confidence score calculation\n",
    "def compute_confidence(original: str, rewritten: str) -> float:\n",
    "    vec_orig = semantic_model.encode(original, convert_to_tensor=True)\n",
    "    vec_rewrite = semantic_model.encode(rewritten, convert_to_tensor=True)\n",
    "    return util.pytorch_cos_sim(vec_orig, vec_rewrite).item()\n",
    "\n",
    "def retrieve_multiquery_docs(question: str, confidence_threshold: float = 0.75):\n",
    "    print(f\"\\nMulti-Query Retrieval\\nQuery: {question}\\n\")\n",
    "\n",
    "    query_prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"\"\"You are an AI assistant. Rephrase the question in 5 different ways to improve retrieval from a document store. Separate each by a new line.\\nQuestion: {question}\"\"\"\n",
    "    )   \n",
    "    llm = OllamaLLM(model=\"llama3.2\")\n",
    "    llm_chain = query_prompt | llm | LineListOutputParser()\n",
    "\n",
    "    rephrased_queries = llm_chain.invoke({\"question\": question})\n",
    "\n",
    "    # Score & filter rephrased queries\n",
    "    filtered_queries = []\n",
    "    print(\"Rephrased Queries with Confidence Scores:\")\n",
    "    for i, q in enumerate(rephrased_queries, 1):\n",
    "        score = compute_confidence(question, q)\n",
    "        print(f\"{i}. {q} [Confidence: {score:.2f}]\")\n",
    "        if score >= confidence_threshold:\n",
    "            filtered_queries.append(q)\n",
    "\n",
    "    if not filtered_queries:\n",
    "        print(\"\\nNo high-confidence queries found. Using original query only.\\n\")\n",
    "        filtered_queries = [question]  # Fallback\n",
    "\n",
    "    # # Define a custom retriever using only high-confidence queries\n",
    "    # class StaticMultiQueryRetriever(MultiQueryRetriever):\n",
    "    #     def generate_queries(self, _):\n",
    "    #         return filtered_queries\n",
    "\n",
    "    # retriever = StaticMultiQueryRetriever(\n",
    "    #     retriever=vector_db.as_retriever(),\n",
    "    #     llm_chain=llm_chain,\n",
    "    #     parser_key=\"lines\"\n",
    "    # )\n",
    "\n",
    "    # docs = retriever.invoke(question)\n",
    "    \n",
    "    # for i, doc in enumerate(docs, 1):\n",
    "    #     print(f\"\\n--- Result {i} ---\")\n",
    "    #     print(f\"Chunk:\\n{doc.page_content[:300]}...\")\n",
    "    #     print(f\"Source PDF: {doc.metadata.get('source')}\")\n",
    "    \n",
    "    # return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88aa5abd-dce2-4d7e-8ddb-482d0b6b8581",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multi-Query Retrieval\n",
      "Query: what is agentic RAg\n",
      "\n",
      "Rephrased Queries with Confidence Scores:\n",
      "1. What does \"Agentic\" refer to in the context of Regulated Aggregates? [Confidence: 0.46]\n",
      "2. Can you provide information on \"Agential Regulatory Aggretates\"? [Confidence: 0.23]\n",
      "3. How is \"Agentia\" related to Regulatory Agenda Groups (RAGs)? [Confidence: 0.52]\n",
      "4. What is the meaning of \"Agence\" as it pertains to Regulatory Agencies in a policy context? [Confidence: 0.10]\n",
      "5. Is there any mention of \"Agent-based\" regulation or Agential concepts in the field of regulatory analysis? [Confidence: 0.28]\n",
      "\n",
      "Multi-Query Retrieval\n",
      "Query: what is agentic RAg\n",
      "\n",
      "Rephrased Queries with Confidence Scores:\n",
      "1. What is Agentic Relevant Aggregate (RaG) in natural language processing? [Confidence: 0.60]\n",
      "2. Can you explain the concept of Agentic RAg, its definition and applications? [Confidence: 0.84]\n",
      "3. How does Agentic RAg differ from other text similarity metrics, such as word Mover's Distance or Embedding Distance? [Confidence: 0.59]\n",
      "4. What are the key characteristics and advantages of using Agentic RAg for text classification tasks? [Confidence: 0.66]\n",
      "5. Can you provide an overview of how Agentic RAg is calculated and its relation to agent-based models in multi-agent systems? [Confidence: 0.74]\n",
      "\n",
      "Multi-Query Retrieval\n",
      "Query: what is agentic RAg\n",
      "\n",
      "Rephrased Queries with Confidence Scores:\n",
      "1. What is the term \"agent\" used in the context of RAG (Representative Agent-based) models? [Confidence: 0.66]\n",
      "2. Can you provide information on RAG, specifically related to agentic capabilities? [Confidence: 0.75]\n",
      "3. How does the concept of agency relate to Representative Agent-based systems (RAGs)? [Confidence: 0.50]\n",
      "4. What is the role of \"agentic\" characteristics in RAG models, and how are they defined? [Confidence: 0.77]\n",
      "5. Can you explain the significance of the term \"agent\" in Representative Agent-based reasoning (RAG), particularly in relation to its agentic properties? [Confidence: 0.61]\n"
     ]
    }
   ],
   "source": [
    "# question = input(\"enter question\")\n",
    "retrieve_multiquery_docs(question, 0.4)\n",
    "retrieve_multiquery_docs(question, 0.5)\n",
    "retrieve_multiquery_docs(question)\n",
    "# answer = generate_rag_answer(multiquery_retrieved_docs, question, llm)\n",
    "# print(\"\\nAnswer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3197210-4c70-45af-88d8-b1763d32e99b",
   "metadata": {},
   "source": [
    "No.\tRephrased Query\t                                               Cosine Similarity Score\t      Notes\n",
    "1\t\"In what ways does deep learning enhance medical image analysis?\"\t0.94\t\t\t\tExcellent — very semantically close\n",
    "2\t\"How is AI used to diagnose diseases from medical images?\"\t        0.88\t\t\t\tClose, generalizes DL to AI but still relevant\n",
    "3\t\"Can deep neural networks help in analyzing MRI scans?\"\t            0.85\t\t\t\tSlightly more specific but still on-topic\n",
    "4\t\"What tools are used in medical imaging?\"\t                        0.58\t\t\t\tLoosely related — tools not limited to DL\n",
    "5\t\"What are the recent advancements in hospital technologies?\"\t    0.43\t\t\t\tVery broad — not specific to DL or images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba84e5d9-d884-4889-aa6d-4c31802114cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b633a2c7-a77b-4066-82c0-03ac5fafdf88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffadb0bd-2f79-4f96-86a8-6d0e4e98c0a5",
   "metadata": {},
   "source": [
    "| **Stage** | **Tools/Tech/Methods** | **Brief Description** |\n",
    "|-----------|------------------------|-----------------------|\n",
    "| **p.1 – User Query Optimization** | - LLMs (GPT-4, Claude, LLaMA 3) <br> - Sentence Transformers (`all-MiniLM`, `bge-base-en`) <br> - Few-shot prompting for clarification <br> - Cosine similarity / intent match scoring | Refine vague or ambiguous user queries using LLMs. Generate multiple precise rephrasings and filter them using semantic similarity scores. Ask clarifying questions to resolve ambiguity before searching, enhancing precision and recall. |\n",
    "| **p.2 – Query Classification & Topic Extraction** | - LLM-based prompting <br> - SciBERT / TF-IDF + Logistic Regression <br> - `KeyBERT` for topic extraction <br> - Multi-label tagging with confidence scoring | Categorize the refined query into arXiv domains like cs.CL, cs.LG, stat.ML. Extract key topics to guide search and scoring. Store classification tags and their confidence scores for downstream filtering and re-ranking. |\n",
    "| **p.3 – Search Over arXiv or Knowledge Bases** | - Hybrid Search (BM25 + FAISS / Qdrant) <br> - Embeddings: `bge-small-en`, `MiniLM`, OpenAI <br> - Metadata-aware scoring (recency, citations) | Perform hybrid retrieval combining lexical (BM25) and semantic embeddings. Rank results using a weighted fusion of scores. Optionally boost documents by recency or citation metrics to surface relevant, high-impact papers. |\n",
    "| **p.4 – Storage & Query Logging** | - FAISS or Qdrant (vector DB) <br> - PostgreSQL / SQLite for metadata <br> - LangChain or LlamaIndex for hybrid access | Store user queries, refined variants, classification tags, embeddings, and search results in a structured + unstructured hybrid DB. Enables fast semantic lookup and detailed audit/logging for future training, debugging, and personalization. |\n",
    "| **p.5 – Re-ranking & Context Chunking** | - BERT-based re-rankers (`cross-encoder/ms-marco`) <br> - Smart chunking (abstracts, methods, conclusion) <br> - Feedback loop for active learning | Improve relevance by re-ranking retrieved documents using deep cross-encoders. Chunk long papers intelligently to optimize context windows. Capture user feedback (likes/dislikes) for active tuning of re-ranking logic. |\n",
    "| **p.6 – Answer Generation & Citation** | - RAG with LLMs (GPT-4, LLaMA 3, Mistral-Instruct) <br> - Chain-of-thought prompting <br> - Hallucination detection models <br> - Citation & confidence tagging | Generate grounded, research-grade answers from top-ranked paper chunks. Use reasoning chains for synthesis across multiple sources. Annotate output with citations, confidence levels, and optional hallucination flags to ensure trust and traceability. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee2b7b02-fc4e-4d44-b987-9e54e5a85ecb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OllamaLLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid choice. Please restart the program.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 73\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     llm \u001b[38;5;241m=\u001b[39m OllamaLLM(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3.2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWelcome to the Research Assistant!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChoose research type:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OllamaLLM' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    llm = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "    print(\"Welcome to the Research Assistant!\")\n",
    "    print(\"Choose research type:\")\n",
    "    print(\"1. Fundamental Research\")\n",
    "    print(\"2. Advanced Research\")\n",
    "    choice = input(\"Enter choice (1 or 2): \").strip()\n",
    "\n",
    "    if choice == \"1\":\n",
    "        # Step 1: Get user query\n",
    "        user_query = input(\"\\nEnter your research question: \").strip()\n",
    "\n",
    "        # Step 2: Rephrase queries\n",
    "        print(\"\\nGenerating rephrased queries...\\n\")\n",
    "        query_prompt = PromptTemplate(\n",
    "            input_variables=[\"question\"],\n",
    "            template=\"\"\"You are an AI assistant. Rephrase the question in 5 different ways to improve retrieval from a document store. Separate each by a new line.\\nQuestion: {question}\"\"\"\n",
    "        )\n",
    "        llm_chain = query_prompt | llm | LineListOutputParser()\n",
    "        rephrased_queries = llm_chain.invoke({\"question\": user_query})\n",
    "\n",
    "        # Step 3: Display rephrased queries with confidence\n",
    "        print(\"Rephrased Queries with Confidence Scores:\")\n",
    "        filtered_queries = []\n",
    "        for i, rq in enumerate(rephrased_queries, 1):\n",
    "            score = compute_confidence(user_query, rq)\n",
    "            print(f\"{i}. {rq} [Confidence: {score:.2f}]\")\n",
    "        print()\n",
    "\n",
    "        # Step 4: Ask user to select relevant queries\n",
    "        selected_indices = input(\n",
    "            \"Enter comma-separated numbers of the relevant queries (e.g., 1,3,5): \"\n",
    "        ).strip()\n",
    "        selected_indices = [int(i) - 1 for i in selected_indices.split(\",\") if i.strip().isdigit()]\n",
    "        selected_queries = [rephrased_queries[i] for i in selected_indices if i < len(rephrased_queries)]\n",
    "\n",
    "        if not selected_queries:\n",
    "            print(\"No valid selection made. Using original query.\")\n",
    "            selected_queries = [user_query]\n",
    "\n",
    "        # Step 5: Custom MultiQueryRetriever with selected queries\n",
    "        class StaticMultiQueryRetriever(MultiQueryRetriever):\n",
    "            def generate_queries(self, _):\n",
    "                return selected_queries\n",
    "\n",
    "        retriever = StaticMultiQueryRetriever(\n",
    "            retriever=vector_db.as_retriever(),\n",
    "            llm_chain=llm_chain,\n",
    "            parser_key=\"lines\"\n",
    "        )\n",
    "\n",
    "        print(\"\\nRetrieving documents...\\n\")\n",
    "        retrieved_docs = retriever.invoke(user_query)\n",
    "\n",
    "        print(\"\\nRetrieved Chunks:\")\n",
    "        for i, doc in enumerate(retrieved_docs, 1):\n",
    "            print(f\"\\n--- Result {i} ---\")\n",
    "            print(f\"Chunk:\\n{doc.page_content[:300]}...\")\n",
    "            print(f\"Source: {doc.metadata.get('source')}\")\n",
    "\n",
    "        # Step 6: Answer Generation using RAG\n",
    "        print(\"\\nGenerating answer...\\n\")\n",
    "        answer = generate_rag_answer(retrieved_docs, user_query, llm)\n",
    "        print(\"Answer:\\n\", answer)\n",
    "\n",
    "    elif choice == \"2\":\n",
    "        print(\"Advanced research flow is under construction. Stay tuned!\")\n",
    "    else:\n",
    "        print(\"Invalid choice. Please restart the program.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11b0ac3d-9459-4dcb-88c4-335c53eed1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24803\n"
     ]
    }
   ],
   "source": [
    "print(vector_db._collection.count())  # Should be > 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ae866f-98b3-4fb8-a5ff-5a875540d7a8",
   "metadata": {},
   "source": [
    "<font size=\"10\"> New </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b881fff3-041b-4cd7-adab-a6c25916eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import necessary packages\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import BaseOutputParser, StrOutputParser\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from typing import List, Literal\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4afe212-d413-4a5c-8d55-2d73d8c26223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Output Parser ----------\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        lines = re.findall(r\"^\\d+\\.\\s+(.*)\", text, re.MULTILINE)\n",
    "        return [line.strip() for line in lines if line.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa9ceab5-b14c-451d-ad09-58345d0874ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Classify Query\n",
    "def classify_query_quality(query: str) -> Literal[\"efficient\", \"okay\", \"vague\", \"unclear\"]:\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"\"\"Classify the quality of the following user query as one of:\n",
    "1. efficient\n",
    "2. okay\n",
    "3. vague\n",
    "4. unclear\n",
    "\n",
    "Only output the classification label (no explanation).\n",
    "User query: {question}\"\"\"\n",
    "    )\n",
    "    return (prompt | llm | StrOutputParser()).invoke({\"question\": query}).strip().lower()\n",
    "\n",
    "# Step 2: Rephrase\n",
    "def rephrase_query(query: str) -> List[str]:\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"\"\"You are an AI assistant. Rephrase the question in 5 different ways to improve retrieval from a document store. Number each version starting with 1.\\nQuestion: {question}\"\"\"\n",
    "    )\n",
    "    return (prompt | llm | LineListOutputParser()).invoke({\"question\": query})\n",
    "\n",
    "# Step 3: Confidence Score\n",
    "def compute_confidence(original: str, rewritten: str) -> float:\n",
    "    vec_orig = semantic_model.encode(original, convert_to_tensor=True)\n",
    "    vec_rewrite = semantic_model.encode(rewritten, convert_to_tensor=True)\n",
    "    return util.pytorch_cos_sim(vec_orig, vec_rewrite).item()\n",
    "\n",
    "# Step 4: Filter Top Queries\n",
    "def filter_queries_by_confidence(original: str, queries: List[str], threshold: float = 0.75, top_k: int = 5) -> List[str]:\n",
    "    scored = [(q, compute_confidence(original, q)) for q in queries]\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, (q, score) in enumerate(scored, 1):\n",
    "        print(f\"{i}. {q} [Confidence: {score:.2f}]\")\n",
    "    return [q for q, score in scored[:top_k] if score >= threshold] or [original]\n",
    "\n",
    "# Step 5: Retrieve Documents\n",
    "def retrieve_documents_from_queries(queries: List[str], retriever) -> List:\n",
    "    all_docs = []\n",
    "    for q in queries:\n",
    "        docs = retriever.get_relevant_documents(q)\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "    # Optional: Deduplicate\n",
    "    unique_docs = {doc.page_content: doc for doc in all_docs}.values()\n",
    "    return list(unique_docs)\n",
    "\n",
    "\n",
    "\n",
    "# Step 6: Generate Final Answer\n",
    "def generate_final_answer(question: str, docs: List, max_docs: int = 5) -> str:\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\", \"context\"],\n",
    "        template=\"\"\"You are a helpful assistant. Use the following retrieved context chunks to answer the user's question:\\n\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\"\"\n",
    "    )\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs[:max_docs]])\n",
    "    return (prompt | llm).invoke({\"question\": question, \"context\": context})\n",
    "\n",
    "# Final Pipeline\n",
    "def smart_document_query_pipeline(user_query: str, retriever) -> str:\n",
    "    print(f\"\\n[🔎] Original Query: {user_query}\")\n",
    "\n",
    "    quality = classify_query_quality(user_query)\n",
    "    print(f\"[📊] Query Quality: {quality}\")\n",
    "\n",
    "    rephrased = rephrase_query(user_query)\n",
    "    top_queries = filter_queries_by_confidence(user_query, rephrased)\n",
    "    \n",
    "    print(f\"[🔁] Using Top {len(top_queries)} Rephrased Queries for Retrieval:\")\n",
    "    retrieved_docs = retrieve_documents_from_queries(top_queries, retriever)\n",
    "    print(f\"[📚] Retrieved {len(retrieved_docs)} unique document chunks.\")\n",
    "\n",
    "    final_answer = generate_final_answer(user_query, retrieved_docs)\n",
    "    print(f\"\\n[✅] Final Answer:\\n{final_answer}\")\n",
    "    return final_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "501a8ebf-542b-4c7e-b802-b210aa691947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5n/8m4t00412h97v592vh5y0pgc0000gn/T/ipykernel_1849/3742961754.py:42: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(q)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'creationdate': '', 'creator': 'cairo 1.10.2 (http://cairographics.org)', 'page': 1, 'page_label': '2', 'producer': 'cairo 1.10.2 (http://cairographics.org)', 'source': 'https://arxiv.org/pdf/1202.0652v1', 'subdomain': 'software_engineering', 'total_pages': 13}, page_content='��������\\n� ������������ �\\n� ����� �������� ����������� �\\n��� ����� ����� ��� ����� � � � � � � � � � � � � � � � � � � � � � � � � � � � � � �\\n��� ���������� ��� ���������� ���� �� � � � � � � � � � � � � � � � � � � � � � � �\\n� ����� �������� �\\n��� ������� ������� � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � �\\n��� ���������� �� ����� � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � ��\\n���������� ��\\n�'),\n",
       " Document(metadata={'creationdate': '', 'creator': 'cairo 1.10.2 (http://cairographics.org)', 'page': 7, 'page_label': '8', 'producer': 'cairo 1.10.2 (http://cairographics.org)', 'source': 'https://arxiv.org/pdf/1202.0652v1', 'subdomain': 'software_engineering', 'total_pages': 13}, page_content='���� ��������������� ���������� ���� �� ������� ���� ��� ����������� ��\\n��� ������������\\n���������� ���������������� ��� ������ ���� �������� �� ��� ������ ��\\n��� �����\\n���������� ��������������������� ��� ����� ��� ������ ���� ����� � ����\\n����� ���� � ���� �� ����������\\n������� ��������� �� ���� ���� �� ����� � ���� �� � ����� ����� �����\\n�������� ��� ��� ������ ���� �� � ����\\n������� ���������������� � ����� ���� �������� �� ��� ����� ��������� �����\\n���� �� ������ ����������\\n������ ��������������������� ����� ��� ���� �� ���������� ���� ����� ���\\n��������� ������������� ���������� ��� �����\\n����� ��� ���� ���\\n��������� ��� ������� ��� ���� ��������� �� ���� ������ ��� �������� �� ��������\\n� �� ������ �� ������� ���� ��� �������\\n� �� ������� �� ��� ����� �� ����\\n� �� ��������� �� ����\\n� �� �������� �� ������� ���� ����\\n� �� ������ �� ��� ����� ��� ��� �������\\n���� ����� ���� �� ��� ���� ������ �������� ���� �� ���� ��������� ��� ���� ��� ������\\n������ ����� ���� ��� ����� ����� ���� ���� ����� ����� �� ��������������������\\n���� �� ���� �� ��� ������� �� ��� ��� ������� �� � �������� ��������\\n�'),\n",
       " Document(metadata={'creationdate': '', 'creator': 'cairo 1.10.2 (http://cairographics.org)', 'page': 5, 'page_label': '6', 'producer': 'cairo 1.10.2 (http://cairographics.org)', 'source': 'https://arxiv.org/pdf/1202.0652v1', 'subdomain': 'software_engineering', 'total_pages': 13}, page_content='����������� �������� ����� � �������� �������� ��� ���� ������������ ����� �� ��������\\n������������ ������ �� ��������� �������� ����� ������� �� � �������� �� ����� ���������\\n��� ������\\n���� ������� �������� ���� �������� � ��� �� ��������� ��� ��������� ������ ���\\n���� ����� �� ����� ��� ��� ����������� ��� ��������� ��� ��������� ��� �����\\n������� �� � ������� �������� ��� ������� ����� ��� ���������� ��� �������������\\n��� ��� ���� ����� �������� ��� �����������\\n������ ���� ������� � ����� ��� ������ ������� ������� �� ��� ������ ��� ����\\n������� �� ��������� �� ��������� ��������� �������� ��������� ��� ��� �� ��������\\n���� �� ���� ���� ������ ���� ���� ��� ������� �������� ����� �� � �����������\\n��� �� ���� ����� ������������ �� ���� �� �� ����� ����� ������� ����� �� ����\\n��� ������ �� ���������� ������ ��� ������ �������� �������� ������ ���� �������\\n��� ������� ����� ������� ��� ���� �� ��� ����� �� ��� ������� ������� ���� ��\\n����� ���������� ��� ���� ���� ���������� ��� ���� �� ���� ���� ��� ������\\n�� �������� ������ ��� ���� ������� ������ ��� ������� �� ��� �� ���� �� ������\\n��� ������ �������� ����� ����� ���� ��� ������������ ��� ������ ��� � �������\\n����� ��� ������� ��������������� �� ������� ��� �������� ��� ����� �������\\n����� ����� ���� ������ �������� ��� ������������ �� �������� ��������� ���\\n�� ������� ������� ���������� �� ���� �� �� ���� ���� �� ����� ��� �������� ������\\n����� �� ����� ������\\n������������������������������������������� �����������\\n����� ��� ��� ���� ��������� �� ������������ ��� ���� ����� ��� � ���� ����� ��������\\n������� ������ ������� ����� ���� ���� ������ ������\\n� ���� ���� ��� ����� ��� ���� �������\\n� ���� ���� ���� �� �� ����\\n� ��� �������� �������� ���� ���� ����\\n� ��� �������� �� ���� ������� ��� �������� ��� ��� ������� �� �� ���� �������������� ���\\n������ �� ������ ���������\\n�� ������ ��� � ������ ��� ����������� ������� ������� ��� ������� ��� ������� � ��� ��� ��\\n����� ��� ��� ���� ������� ������ ������� ��� ��� ����� �� ������������ ��� ��� ������� ���\\n����� �� ��� �������� ������ ��� ������� ��� ������ � ���� ���� � ������ ��� ����� ���\\n���� ���� �� ���� ��� ��� ��������� ���� ����� ���� ���������� ��� ����������� �� ��� �����\\n�� � ���� ���� ������� ������� ��� ����� �� ���� ����� ���� �������� ��� ������ ��� ��������\\n������� ������ �� ���� ���� �������� ������������\\n������������ ��� ����� ����\\n�'),\n",
       " Document(metadata={'creationdate': '', 'creator': 'cairo 1.10.2 (http://cairographics.org)', 'page': 8, 'page_label': '9', 'producer': 'cairo 1.10.2 (http://cairographics.org)', 'source': 'https://arxiv.org/pdf/1202.0652v1', 'subdomain': 'software_engineering', 'total_pages': 13}, page_content='������� �\\n����� ��������\\n���� �� ��������� ������ �������� �� ���\\n������� ����������� ���� ���� ������ �� ��� ���� �� ���� ��������� �� ���\\n���� ��������� �� ����� ������� ��������� �� ���� ����� ��� ��� ��������\\n�� �� ���� ��� ������������ ���� ��� ������ �� �������\\n�������� ��� ����������� ���� ��������� ��� � ������ �� ��������� ��� �����\\n�������� ��������� ��� ������ �� ��������� ��� ������� ��������\\n������������������� ��������� ������� ����� �� �������� ��� ������ ���� ����\\n���� � ���� ���� ���� ������ �� ���� ����� ������ �� � ����� ����������\\n����� ��� ������� ���������� ������� ��� ������ ���� ��� ����������� �� ���\\n�������������\\n����������� ��� ����������� ���� ������� ���� ��������� ��� ������� ��\\n��������� ��� ������� ��� � ����� ��� �� �������� ���� ��� �������������\\n��� ���� � ��������� ����� �������� ��� ������� ����� ������� ��� ����� ����\\n����� � ���������������\\n������������������� ���� ��� ���� �� ��� ��� �� ������ ��� ����� �� �������� ����\\n��������� ��� ��� �� �� ��� ������� ����� �� ����� �� �� �� ���������\\n����������� ���������� ���������� ��� �� ����� ��� �������� �������� ������\\n����������\\n����������� � ��������� ���\\n���������� ����� ��� ���������� ����� ������ ����� ��� ����������� ������ �� ������ ���\\n��������� ���������� �� ����������� ������������� �������� ���� ���� ��������� ���� ��������\\n���� ������ �� ���������� �������� ���� ������������� ���� �� ���������� ��� ���� ��������\\n�������� �� ����������� ���� ������� ��� ����������� ��������� ��� ����������� �� ��� ����\\n��������� ����������� ��� ������� ������� �� �������� ������� ��� �������� ����� �������\\n�� ����� �������� ����� ���� ��� ��������� �� � ������� ����� �� ��� ������� �������� ����\\n�'),\n",
       " Document(metadata={'aapl:keywords': \"['']\", 'author': 'Michael Bar-Sinai', 'creationdate': \"D:20150731210203Z00'00'\", 'creator': 'Word', 'keywords': '', 'moddate': \"D:20150731210203Z00'00'\", 'page': 0, 'page_label': '1', 'producer': 'Mac OS X 10.10.4 Quartz PDFContext', 'source': 'https://arxiv.org/pdf/1506.08978v2', 'subdomain': 'general_literature', 'subject': '', 'title': 'Big Data Technology Litrature Review', 'total_pages': 10}, page_content='1'),\n",
       " Document(metadata={'creationdate': '', 'creator': 'cairo 1.10.2 (http://cairographics.org)', 'page': 0, 'page_label': '1', 'producer': 'cairo 1.10.2 (http://cairographics.org)', 'source': 'https://arxiv.org/pdf/1202.0652v1', 'subdomain': 'software_engineering', 'total_pages': 13}, page_content='����� ��������\\n������ ����������\\n�������������������������\\n�������� ������ �����������\\n�� ��� ��� �� �� �����������\\n������� ���������� ������\\n����������������� ���\\n�������������� ����� �����\\n���������� �� �������� ��������\\n���������� �� ��������\\n������ ������ ��� ���������� �������\\n������� �� ���� ���\\n�������� �� ����'),\n",
       " Document(metadata={'creationdate': '', 'creator': 'cairo 1.10.2 (http://cairographics.org)', 'page': 9, 'page_label': '10', 'producer': 'cairo 1.10.2 (http://cairographics.org)', 'source': 'https://arxiv.org/pdf/1202.0652v1', 'subdomain': 'software_engineering', 'total_pages': 13}, page_content='�������� ������� ����� �� ����������� ������������ ��� ����� ������������ ����������� �� ���\\n����� ��� ��� �� ���� ����� ��������������� �� ������ ������� ��������� �� �������� ��\\n�������� ������ ���� ������ �� ��� ������ ���������� �������\\n���� �� ��� ���� ��� ����� ��� ���� � �������� ����������� ����������� �� ���� ���� ��\\n��������� ������� ���� ��� ���� �� ��������� ������� ����� ���� ���� ����� �� �� ���� ����\\n������� �� ������� ������������� ��� �� �����\\n��� ��������������� ��� �� ������� ������� �� � ������ �� ���������� ��� ����� �� �����\\n������� ��� ��� ����������� �� ����� ��������� ���������� ��� ���� �� ��� ������� ����������\\n���� ��������\\n��� ������� �������\\n��������� ��� ������ �� �� � ��� �� ���� �������� �� ���� ����� ��� �� ������ ���������\\n������� �������� ����� �� � ��� �� �������� ������� ��������� ��� �� ��������� ��������\\n������� ������� ������ ����������� ����� �������� ���� ���� �� ��������� �������� ����� ��\\n���� ����� ��� ���� ��� ���� ����� ��� �� ����� ����������� ���� ���� ���������� �������\\n��� �� ��������� �������������� ����������� ��� ��� �� ����� ��� ���������� ����� ��� ���\\n�������� ��� ��������� ��������\\n���� ������� ���������� �� ����� ������� ��� ��� ���� �������� ���� ���������������������\\n��� ���� �������� �� �� ��� ����� ���� �� ���� ������� ����\\n� ����������� ������������\\n� ������� ������ ������� ������\\n� ���� �������� ����� ��������� ��������� ������ ��������� ������� ���� �� ��� �� ����\\n�� ���� ���� �� ���������� ���� �������� ���� � ��������� ����� ���� ���� �������\\n����� �� ����� ������ ������ �� ������� ���������� ��� � ���� ��������� ����� ���� ����\\n������������� �� � �������� �������\\n� �������� ��� ������������� �������� ��� �������� ���� ���� ��� ���������� �� ���\\n������ ����� ������� ��� ���������� �� �������� ����������� ��� ��������� ������\\n�� ��� ������� �� ��� ���� �� ����������� ��������� ��� �������\\n� �������� ��� ���������� ����������� ����� ���� ����������� ������� �������\\n� ����������� ���� �� ��������������� ������\\n� ����������� ���� �� � ��������� �� ������� ������ �� ��� ���� ��� �� ����� �� ��\\n� ���� ����� � ����� �������\\n� ��� ���� �� ��������� ���������� ������������� ������������ ��� ��� ���� ���� �����\\n������� �� ��������� ��� ����������� ��� ������ �� � ��������� ����� ���������� � �����\\n���� �� ��� ������������� ������������ ��� �����\\n�'),\n",
       " Document(metadata={'creationdate': \"D:20241122005515Z00'00'\", 'creator': 'PyPDF', 'moddate': \"D:20241122005515Z00'00'\", 'page': 1, 'page_label': '2', 'producer': 'macOS バージョン14.5（ビルド23F79） Quartz PDFContext', 'source': 'https://arxiv.org/pdf/2411.14653v1', 'subdomain': 'other_computer_science', 'total_pages': 25}, page_content='2'),\n",
       " Document(metadata={'creationdate': '', 'creator': 'cairo 1.10.2 (http://cairographics.org)', 'page': 3, 'page_label': '4', 'producer': 'cairo 1.10.2 (http://cairographics.org)', 'source': 'https://arxiv.org/pdf/1202.0652v1', 'subdomain': 'software_engineering', 'total_pages': 13}, page_content='������� �\\n����� �������� �����������\\n�� ����� ���� ��� �������� �������� �������� �� ����� ��� �� ������ ���� ���� ���� ����\\n��������� ��������� ���� ���� �� ���������� ��� ����� � ��� �� ����������� �������� ����\\n���� ������ ����� ����� ������ � ������� �������� ������� ���� ��� ��� ��������� ������ �\\n����� ����� ��������� �� ���� ��� ������������� �������� ��� ���� �� ��� ������ ���� ������\\n������� �������� ����� ��� ������� �� �� ����� ��� �� �� �� �� ��� ������ ������� ��� ���\\n�������� ������������\\n�������� ������������ ���� �� ����� ��� ����� �� ����������� ����� �� ������ � ������� �����\\n���� ��� �� ������� � ����������������������� ���������� �� ��� ����� �������� �����������\\n�������� �� ��� ����� ������ ������� ���� ���������� ���� ��� ������� �������������������\\n��������� ��� ��������� �������� �� ������� ��� ������������� ������ ������ �� ��� ������\\n�� �� ����� ����� �������� ����� ����� ������������ ������ ��� ���� ���������� ���� ��� ���\\n�������� ��� ���������������������������� ����� ������ ����� ����� ��������� ����������\\n���� ���� ���� �� ������� �� ������������� � ������ �� ��������� ����� ��� ���� ������\\n���� �� �������������� ����� ����� ���� ��� ���� ����� ���� ������� ���� ��� ������������\\n�� ������������� ����� ����� ��� ������������ ���� ����� �� �� ������ �� ������� �� ������\\n�������� ������� �� ��� ������������ ���������� ���� ���� �� ��� ����� ������� ���� �����\\n���������������������������\\n��� �������� ����� ������� ���� ������ ����� ��� ������� ��� ������� ���� ��� ���������\\n����� �������� ��� �������� ����������� �� ���� �������� ��� ��� �������� ������ ����� �� ��\\n����������� ���� ������ ��� �� �� ������ ������ ������ ��� �������� ��� ���� ���� ��������\\n������� �� ������������ �������� ���� �� ��� �� � ������ �� �������� ����������\\n� ��� ��� �� �������� �������� ������� ��� ����� ��� ��� ����� �� ��� ������ ��� ���\\n����� �� ��� ��� ����� �� ���� �� ��������� ��� � ��� ������ ��� � ��� ����� ������ ������\\n�� �� ���� �� ���� ��� ����� ������ ����� ���� �� �����\\n� ��� ������� ������� ���� �������� �� ����� �� ������� �� ������� ��� ����� �� �������\\n���� �� ������ ������ ��������� � ������������� ������������� �� �����\\n�'),\n",
       " Document(metadata={'creationdate': '', 'creator': 'cairo 1.10.2 (http://cairographics.org)', 'page': 4, 'page_label': '5', 'producer': 'cairo 1.10.2 (http://cairographics.org)', 'source': 'https://arxiv.org/pdf/1202.0652v1', 'subdomain': 'software_engineering', 'total_pages': 13}, page_content='� �������� ������������ ���� �� ��� �������� ���� �� ������������ ��������� ���� ��������\\n��� ��� ������� ���� ��������� ����� ����� �� ���� ��������� ������ ��� �������� ���\\n������� ��� ������������ �� ����� ����� ��� ������� �� ���� � ������ ����������� ��\\n������� ��� ��������� ��������\\n� ����������� �� ��� ������� ������� �� �������� �� �� ��������� ������� ������� ��\\n����� �������������� ������� �� ��� �� ����� �������� �� ��� ���������� �� � ���� ��\\n����� ����� �� ������� ���������������� ����������� ��� ���������� ����� �� ��������� ����\\n������� �� �������� �� ���� ���� ������� �� ������� ���� �� ��� ������� ���� ��� ����\\n�� ���������� ��� ������� ��� ��� ����������� ���������\\n��������� �� ��� ������ � ������ ���� ��� ���� ��� �� ����� � ���������� ����� �� ��������\\n�� �� ���� ����� � ���� ������� ��� ������ ���������� �� ������ ��� ������ ��� ������ ������\\n������ ��� �� ����� ������� ��������� ���� �� ����� ����������� ��� ���������� �� �� �� ���\\n�������� ��� ��������� ������� ���� ������� ���� ����� �� ������ ������� ��������� ��� �����\\n�� ������ ��������� ��� ��� �� ����������� ������ ���� ��� �������� ��� ��� ���������������\\n�� ������ ��� ���� � ������������ ������� ����� ����� ��� ������ �� ��� ���������� ������\\n���� ���� �������\\n���� ��� ���� ������ �� �� � ��� ������ �� �������� ����������� ������� ���� ������ �����\\n�� ��� ����� �� ������� �������� ������ �������� �������� ����� �� ������� ���� �� ���\\n�������� ����� ���� ���� ���������� ��� ��� �������� ����� ��������� ���� � ����� ������� ��\\n��� �� ��� ���� ������� ������� �� ��� ����� ������� ����� ��� �� �������� �������������\\n��� ����� ����� ��� �����\\n�� ����� ��������� �������� ����������� ���� �������� �������� ��� ���� ������\\n��������� � ������ ��� ������ �������� � ������ ������� ��� ������� ���� ������� ���������\\n������� �� �������� ������� �������� ���������� ���� ��������� ��� ����� �� �������� ����\\n��������� ����������� �� �������� ����������� ��� ��� ����� �� ����������� ������ �������\\n�� ���� ��� ������ ��������� �������� ���� �������� ��� ��� ��� ������� ������� ��\\n���� ��� ������� ������� ��� ������� ����� ���� ����� �������������� ����������� ���� ��\\n���� ���� ������ ��� ����������� �������������� ���� ��������� ���� ��� ��� ������� ���\\n�����������������������������������������������������������������������������������������������\\n���� �� ��������� � ������� �������� �������� ������ �� ���� ��� ��� �� ����������� ������ �� �����\\n�����������\\n� ������������������������������������������������������������\\n� ��������������������������������������������������������\\n� ������������������������������������������������\\n� �������������������������������������\\n�������������������� ����������������������������������������������������������������\\n������������������������������������������������\\n�'),\n",
       " Document(metadata={'creationdate': '', 'creator': 'cairo 1.10.2 (http://cairographics.org)', 'page': 6, 'page_label': '7', 'producer': 'cairo 1.10.2 (http://cairographics.org)', 'source': 'https://arxiv.org/pdf/1202.0652v1', 'subdomain': 'software_engineering', 'total_pages': 13}, page_content='������ ���� ����� �� ��������\\n��� ���������� ��� ���������� ���� ��\\n�� ���� ���� ���� ����� � ���� ������������� ����������� ���������� �������\\n����������� ��� ���� ��������� ���������� ���� ���� ������ ��������� ��� ������������\\n� ������������������ �������� �� ������� ��� ���������� ��� �� ������ ��� �������� ���\\n���������� �� ��� �������� ������� ��� �� ���� � ������ ��� �� � �������� �� ��������� ��\\n���������� ��� ��� ������� �� ������ ������ �� ��� ������ ��� ���������� ���� �� ���� ����\\n������� ���� ������������� ����� ������������ �������� ��������� �� �������� ���� ������� ��\\n����������� ����������� ������������� �������� ����������� �������� ������������� �������\\n����������� ��� ������� ����������� ��� �������� ��� �� ����� ����� ��� ���� ���� ����\\n�� ��� ���� ������ �� ��������� ��������������\\n��� �������� ���� �� ��������� ����\\n����� ������������ � ������ ������ ���� ���������� �������� ���� ������� ���\\n�������� �� � ���� ����� ������\\n�������������� ��� ����������� ���� � ������ ������ ����� �� ��� ��� �����\\n������ ������\\n������ ���������� ������ ������ �� �������� �� ������ �� �������� �� ���\\n����� ������� ����� ���������� �� ������� �� ���� �� �� �� �����������\\n������������������� ����������� ����� ���� ������ ����� ���� ��� ���������\\n��� ����������� �� ��������� ��������� ����� ����� ���������� ���� ��������\\n��� ��������\\n����������������������� ����������� ��� ������ ������� �������� ��� ���\\n������� �� ������ ������������ ������� �������������� ��������� �� ���\\n�����������\\n�'),\n",
       " Document(metadata={'creationdate': '', 'creator': 'cairo 1.10.2 (http://cairographics.org)', 'page': 2, 'page_label': '3', 'producer': 'cairo 1.10.2 (http://cairographics.org)', 'source': 'https://arxiv.org/pdf/1202.0652v1', 'subdomain': 'software_engineering', 'total_pages': 13}, page_content='������� �\\n������������\\n���� ����� ��������� ��� ����������� �� ����� �������� ����������� ������� �� ��������������\\n�������� �������������\\n�'),\n",
       " Document(metadata={'creationdate': '', 'creator': 'cairo 1.10.2 (http://cairographics.org)', 'page': 12, 'page_label': '13', 'producer': 'cairo 1.10.2 (http://cairographics.org)', 'source': 'https://arxiv.org/pdf/1202.0652v1', 'subdomain': 'software_engineering', 'total_pages': 13}, page_content='����������\\n����� ���\\n�� ������������ ����������� ���������� ��������������� ����� ������ ������\\n��� ���� �����\\n������ ���\\n�� ��������������������� �������� ��� ������ ��� ����������������������\\n�����\\n������ ���\\n�� ��������� ��������� ������ �������� ������� �����\\n����������� � ��������� ���\\n�� ���������� ��� �� ���������� ������������� �������� �������� ���������\\n������������������ �� �������� ��� ������������ ��� �������� ����� �������� �����\\n���������� �� ������ ���\\n�� ��������� �� ���������� �������� ���������� ������ �� �������� ������������\\n�������������� ������� ���������� ���� ����� ������� ��� ���� �����\\n��������� � ������ ���\\n�� �������� ��� �� ������������ �������� ����������� ���� ������ �������� ����\\n���� ����� ������ ������ ��� ���� �����\\n�������� ���\\n�� �������������� ���������� ��������� �������� ����� ��� ����� �����\\n��'),\n",
       " Document(metadata={'creationdate': '', 'creator': 'cairo 1.10.2 (http://cairographics.org)', 'page': 10, 'page_label': '11', 'producer': 'cairo 1.10.2 (http://cairographics.org)', 'source': 'https://arxiv.org/pdf/1202.0652v1', 'subdomain': 'software_engineering', 'total_pages': 13}, page_content='� ��� ���� ����������� ����� ���� ��� ������� �������� ����� �� �� �����\\n� ����� ��� �� ��� ����� ��� ��� ��������� ���� ��� �������� ���������\\n� ���� ����� � �� � ������� ���� �� ������ ������� ������ ���� ���� ����� ���� ��������\\n���� �� ������� ��� ����� ���� ��� ���� � �������� ����� ��� ������ �������� ���\\n������ ���� ������ �� ��������\\n��� ������\\n� ������� � ����������� ���� �� �������������� ���������� �������� ��� ���� ����\\n� ������ � ������� ���� ��� � ���� �� � ���������� ��������� �� �������� ���������\\n� �� � ������� ���� �� ����������\\n� �������\\n� �������\\n������ � ������ ��� ���� ����� ������ �� �������� �� ��� ���������� ������ ���� ��� ��\\n�� ������� ������������ ������� ���� ����� ���� ��� ��������� ��� ����� �� ��� �������\\n��������� ��� ���� ��� ������ �� �������� ��� ������ �� ���� �� ��� ������ �� �� ����� ���\\n��� ����� �� ����� ���������� �� ���������\\n�� ��� ������������� ������� ��������� �� �� ������ ���� ��� ������������ ��� ���� ��������\\n�������\\n����������\\n� �������������\\n� �������� ���� ����� ��� ��� ��� ������� ������������� ���� ����� ������� �� ���� ��\\n������� �� ������� ��� ������������� ����� �� ��� ��� �� ��� ���� ������ �� ��������\\n������ ��� ����������� ������ �� �� ������������\\n� ���� ������ ��� ��� ������� ����� ���� ��� ���� �������� ������ �� ���������� ���� �\\n��� �� ������\\n� ���� ������ ���� ��� �� ������� ������ ����������� ��� ����� �� ���������� ���� ���\\n���� ������� ��� �� ������� �� �����������\\n� ��������� ������ ���� �� ������� ���������� ���������� �� � ��� �� ����� ���� ����� ����\\n��� ���� ��� ���� ����� �������������������\\n� ������������� �������� ��� �������� ��� ������� ������� ������ ������� ����� ������\\n�� ��������� �� ���� ��������������\\n��'),\n",
       " Document(metadata={'creationdate': '', 'creator': 'cairo 1.10.2 (http://cairographics.org)', 'page': 11, 'page_label': '12', 'producer': 'cairo 1.10.2 (http://cairographics.org)', 'source': 'https://arxiv.org/pdf/1202.0652v1', 'subdomain': 'software_engineering', 'total_pages': 13}, page_content='��� ���������� �� �����\\n��� �� ��� ������ �������� �� ��� ����� ������� �� ��� ��������� ���� ��� �� ��� ������ �������\\n�� �������� ������������ �� �� ���������� ��� � �������� ����� �� ���������� ���� �������\\n��� ������ ���� ������ ���� ���� ���� ������� ����� ��� ���� ���� ���� ��� ����� ������\\n����� ������� � ���� ����� ����� �� ���� � ���������� ����� �� ���� �� �������� �� ��������� �\\n�������� ��� ��� �������� �������� ���� ������ ���� ��������� �� ���� ��� �������� �����\\n��� ������ ������ ��� ��� ��� ������ ��� ���� ��������� ������������� ������� �� ���\\n��������� ���� ����� ���� ����� ��������� ���� ������ ��������� ������ ��� �������������\\n���� ��� ����� ���������� �� ��� ����� ������� ��� ���� ��������� ���������� �� ���� ��������\\n�������� ���������� �� ����� ������ ����� ��� ���� �� ���������� ���������� ��������� ��\\n��������\\n��')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "retriever = vector_db.as_retriever()\n",
    "\n",
    "retrieve_documents_from_queries(\"What are the key concepts behind Agentic RAG systems?\", retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6deec1c-4126-43e7-947b-b66ecd786be9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
