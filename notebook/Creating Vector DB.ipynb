{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9b9e787-5d8d-4d88-b892-164196f16010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading all PDFs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 59 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 79 0 (offset 0)\n",
      "Ignoring wrong pointing object 81 0 (offset 0)\n",
      "Ignoring wrong pointing object 87 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 104 0 (offset 0)\n",
      "Ignoring wrong pointing object 106 0 (offset 0)\n",
      "Ignoring wrong pointing object 129 0 (offset 0)\n",
      "Ignoring wrong pointing object 140 0 (offset 0)\n",
      "Ignoring wrong pointing object 152 0 (offset 0)\n",
      "Ignoring wrong pointing object 164 0 (offset 0)\n",
      "Ignoring wrong pointing object 194 0 (offset 0)\n",
      "Ignoring wrong pointing object 196 0 (offset 0)\n",
      "Ignoring wrong pointing object 198 0 (offset 0)\n",
      "Ignoring wrong pointing object 200 0 (offset 0)\n",
      "Ignoring wrong pointing object 320 0 (offset 0)\n",
      "Ignoring wrong pointing object 328 0 (offset 0)\n",
      "Ignoring wrong pointing object 330 0 (offset 0)\n",
      "Ignoring wrong pointing object 332 0 (offset 0)\n",
      "Ignoring wrong pointing object 349 0 (offset 0)\n",
      "Ignoring wrong pointing object 351 0 (offset 0)\n",
      "Ignoring wrong pointing object 355 0 (offset 0)\n",
      "Ignoring wrong pointing object 357 0 (offset 0)\n",
      "Ignoring wrong pointing object 363 0 (offset 0)\n",
      "Ignoring wrong pointing object 367 0 (offset 0)\n",
      "Ignoring wrong pointing object 373 0 (offset 0)\n",
      "Ignoring wrong pointing object 376 0 (offset 0)\n",
      "Ignoring wrong pointing object 406 0 (offset 0)\n",
      "Ignoring wrong pointing object 437 0 (offset 0)\n",
      "Ignoring wrong pointing object 442 0 (offset 0)\n",
      "Ignoring wrong pointing object 444 0 (offset 0)\n",
      "Ignoring wrong pointing object 448 0 (offset 0)\n",
      "Ignoring wrong pointing object 450 0 (offset 0)\n",
      "Ignoring wrong pointing object 478 0 (offset 0)\n",
      "Ignoring wrong pointing object 485 0 (offset 0)\n",
      "Ignoring wrong pointing object 495 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents...\n",
      "Embedding and creating Chroma DB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5n/8m4t00412h97v592vh5y0pgc0000gn/T/ipykernel_32161/1931151037.py:58: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " VectorDB created with 493 chunks!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5n/8m4t00412h97v592vh5y0pgc0000gn/T/ipykernel_32161/1931151037.py:63: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vector_db.persist()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from langchain_community.document_loaders import ArxivLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# -------------- STEP 3: LOAD ALL PDFs --------------\n",
    "print(\"\\nLoading all PDFs...\")\n",
    "all_docs = []\n",
    "downloaded_folder = \"../Database/arXiv_papers\"\n",
    "\n",
    "# Get full paths of all PDF files in the folder\n",
    "downloaded_files = [os.path.join(downloaded_folder, f) for f in os.listdir(downloaded_folder) if f.endswith(\".pdf\")]\n",
    "\n",
    "for path in downloaded_files:\n",
    "    try:\n",
    "        loader = PyPDFLoader(path)\n",
    "        docs = loader.load()\n",
    "        \n",
    "        filename = os.path.basename(path)\n",
    "        arxiv_id = filename.replace(\".pdf\", \"\")\n",
    "        arxiv_url = f\"https://arxiv.org/pdf/{arxiv_id}\"\n",
    "\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = arxiv_url\n",
    "        all_docs.extend(docs)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {path}: {e}\")\n",
    "\n",
    "# -------------- STEP 4: SPLIT TEXT --------------\n",
    "print(\"Splitting documents...\")\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer - choose one closest to your Ollama model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-hf\")  # or the model you used\n",
    "\n",
    "# Define a tokenizer function as expected by LangChain\n",
    "def token_length_function(text):\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# Now use LangChain's TokenTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    length_function=token_length_function,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(all_docs)\n",
    "\n",
    "\n",
    "# -------------- STEP 5: EMBEDDINGS + VECTOR DB --------------\n",
    "print(\"Embedding and creating Chroma DB...\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_db = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=\"../Database/Vector-DB\")\n",
    "\n",
    "# Optional: Persist the DB\n",
    "vector_db.persist()\n",
    "\n",
    "print(f\"\\n VectorDB created with {len(chunks)} chunks!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c59db2-42c7-4899-b187-51683da55f75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
