{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f991b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"ResearchPro2\" \n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = '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'\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b679fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install and import necessary packages\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import BaseOutputParser, StrOutputParser\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "from typing import List, Literal\n",
    "import re\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fed665",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "PDF_DIR = \"./arxiv_data\"\n",
    "DB_DIR = \"./arxiv_vector1_db\"\n",
    "\n",
    "# Embedding & Vector DB\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_db = Chroma(persist_directory=DB_DIR, embedding_function=embedding_model)\n",
    "\n",
    "# Load embedding model once for scoring\n",
    "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# LLM\n",
    "llm = OllamaLLM(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7cdea8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, BaseOutputParser\n",
    "from langchain.schema import Document, HumanMessage, AIMessage\n",
    "from langchain.vectorstores import VectorStore\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms.base import BaseLLM\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from pydantic import BaseModel, Field\n",
    "import logging\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"SageRAG\")\n",
    "\n",
    "# Rich console for prettier output\n",
    "console = Console()\n",
    "\n",
    "# Enhanced output parsers with better error handling\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Parse output that contains a numbered list and return as a list of strings.\"\"\"\n",
    "    \n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        \"\"\"Parse text into a list of strings.\"\"\"\n",
    "        # Updated regex to be more robust with various numbering styles\n",
    "        lines = re.findall(r\"^\\s*\\d+\\.?\\s+(.*?)$\", text, re.MULTILINE)\n",
    "        return [line.strip() for line in lines if line.strip()]\n",
    "    \n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"line_list\"\n",
    "\n",
    "class QueryRating(BaseModel):\n",
    "    \"\"\"Schema for query rating output.\"\"\"\n",
    "    rating: int = Field(description=\"Rating from 1-5\")\n",
    "    explanation: str = Field(description=\"Explanation for the rating\")\n",
    "\n",
    "@dataclass\n",
    "class RetrievedDocument:\n",
    "    \"\"\"Dataclass for tracking retrieved document info.\"\"\"\n",
    "    document: Document\n",
    "    score: float\n",
    "    query: str = \"\"\n",
    "    rank: int = 0\n",
    "\n",
    "class ResearchAssistant:\n",
    "    \"\"\"AI Research Assistant using RAG for scientific paper queries.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        llm: BaseLLM, \n",
    "        vector_db: VectorStore, \n",
    "        embeddings_model: Optional[SentenceTransformer] = None,\n",
    "        relevance_threshold: float = 0.6,\n",
    "        max_docs_per_query: int = 3,\n",
    "        always_use_fallback: bool = True  # New parameter to control fallback behavior\n",
    "    ):\n",
    "        \"\"\"Initialize the Research Assistant with LLM and vector database.\n",
    "        \n",
    "        Args:\n",
    "            llm: The language model to use for generation\n",
    "            vector_db: Vector database for document retrieval\n",
    "            embeddings_model: Optional SentenceTransformer model for semantic similarity\n",
    "            relevance_threshold: Minimum relevance score for documents\n",
    "            max_docs_per_query: Maximum documents to use per query\n",
    "            always_use_fallback: Whether to always use LLM fallback for general knowledge questions\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.vector_db = vector_db\n",
    "        self.relevance_threshold = relevance_threshold\n",
    "        self.max_docs_per_query = max_docs_per_query\n",
    "        self.always_use_fallback = always_use_fallback\n",
    "        \n",
    "        # Initialize sentence transformer model for semantic similarity if not provided\n",
    "        if embeddings_model is None:\n",
    "            logger.info(\"Initializing default embedding model\")\n",
    "            self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        else:\n",
    "            self.semantic_model = embeddings_model\n",
    "            \n",
    "        # Initialize memory\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        # Create a retriever for direct use\n",
    "        self.retriever = vector_db.as_retriever()\n",
    "        \n",
    "        # Define parsers\n",
    "        self.json_parser = JsonOutputParser(pydantic_object=QueryRating)\n",
    "        self.list_parser = LineListOutputParser()\n",
    "        \n",
    "        logger.info(\"Research Assistant initialized successfully\")\n",
    "    \n",
    "    def rate_query(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Rates the query and gives an explanation.\n",
    "        \n",
    "        Args:\n",
    "            query: The user's query to be evaluated\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing rating and explanation\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an intelligent assistant trained to evaluate search queries for a scientific research database.\n",
    "        Given the following query: \"{query}\"\n",
    "        \n",
    "        1. Rate the query on a scale of 1 (very poor) to 5 (excellent) based on:\n",
    "           - Clarity: Is the query clear and unambiguous?\n",
    "           - Specificity: Does it contain specific technical terms or concepts?\n",
    "           - Relevance: Is it focused on retrieving scientific content?\n",
    "           - Retrievability: Will it work well with vector search?\n",
    "        \n",
    "        2. Provide a short explanation for the rating (what makes it effective or ineffective).\n",
    "        \n",
    "        Respond in JSON format:\n",
    "        {{\n",
    "          \"rating\": <number between 1-5>,\n",
    "          \"explanation\": \"<your explanation>\"\n",
    "        }}\n",
    "        \"\"\")\n",
    "        \n",
    "        chain: Runnable = prompt | self.llm | self.json_parser\n",
    "        \n",
    "        try:\n",
    "            return chain.invoke({\"query\": query})\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error rating query: {e}\")\n",
    "            # Fallback response if parsing fails\n",
    "            return {\n",
    "                \"rating\": 3, \n",
    "                \"explanation\": \"Unable to rate query properly. Consider adding more specific terms.\"\n",
    "            }\n",
    "    \n",
    "    def is_general_knowledge_question(self, query: str) -> bool:\n",
    "        \"\"\"Determines if a question is likely a general knowledge question rather than research-specific.\n",
    "        \n",
    "        Args:\n",
    "            query: The user's query to evaluate\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating if this is likely a general knowledge question\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an intelligent assistant analyzing a question. Determine if this is a general knowledge question\n",
    "        that doesn't require specific scientific papers to answer properly.\n",
    "        \n",
    "        Question: \"{query}\"\n",
    "        \n",
    "        Examples of general knowledge questions:\n",
    "        - What is an operating system?\n",
    "        - Who invented the telephone?\n",
    "        - How does gravity work?\n",
    "        - What is the difference between RAM and ROM?\n",
    "        \n",
    "        Examples of research-specific questions:\n",
    "        - What are the latest developments in CRISPR gene editing?\n",
    "        - How does the transformer architecture improve machine translation accuracy?\n",
    "        - What methodologies are used to measure quantum entanglement?\n",
    "        - What were the findings of the 2023 paper on climate change impact on coral reefs?\n",
    "        \n",
    "        Is this a general knowledge question that could be answered without specific research papers?\n",
    "        Answer only YES or NO.\n",
    "        \"\"\")\n",
    "        \n",
    "        try:\n",
    "            result = self.llm.invoke(prompt.format(query=query))\n",
    "            return \"YES\" in result.upper()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error determining if general knowledge question: {e}\")\n",
    "            return False  # Default to assuming it's not general knowledge\n",
    "    \n",
    "    def suggest_rewrites(self, query: str, chat_history: Optional[List] = None) -> List[str]:\n",
    "        \"\"\"Returns rephrased versions of the query optimized for retrieval.\n",
    "        \n",
    "        Args:\n",
    "            query: Original query to rewrite\n",
    "            chat_history: Optional conversation history for context\n",
    "            \n",
    "        Returns:\n",
    "            List of rewritten queries\n",
    "        \"\"\"\n",
    "        history_context = \"\"\n",
    "        if chat_history:\n",
    "            history_context = \"Consider this conversation context when rewriting the query:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use only last 3 messages for brevity\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    history_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"history_context\"],\n",
    "            template=\"\"\"You are an AI assistant specializing in scientific research queries.\n",
    "            \n",
    "            {history_context}\n",
    "            \n",
    "            Rephrase the question in 5 different ways to improve retrieval from a scientific paper database. \n",
    "            Focus on:\n",
    "            1. Using technical terminology for better vector matching\n",
    "            2. Breaking down complex queries into clearer formulations\n",
    "            3. Adding relevant synonyms or related concepts\n",
    "            4. Varying syntax while preserving semantic meaning\n",
    "            5. Including key entities and relationships from the original query\n",
    "            \n",
    "            Number each version starting with 1.\n",
    "            \n",
    "            Question: {question}\"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            result = (prompt | self.llm | self.list_parser).invoke({\n",
    "                \"question\": query,\n",
    "                \"history_context\": history_context\n",
    "            })\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error suggesting rewrites: {e}\")\n",
    "            # Return a minimal set of rewrites if parsing fails\n",
    "            return [\n",
    "                query,  # Original query\n",
    "                f\"research about {query}\",\n",
    "                f\"papers discussing {query}\"\n",
    "            ]\n",
    "    \n",
    "    def compute_confidence(self, original: str, rewritten: str) -> float:\n",
    "        \"\"\"Computes semantic similarity between original and rewritten queries.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            rewritten: Rewritten version\n",
    "            \n",
    "        Returns:\n",
    "            Similarity score between 0-1\n",
    "        \"\"\"\n",
    "        try:\n",
    "            vec_orig = self.semantic_model.encode(original, convert_to_tensor=True)\n",
    "            vec_rewrite = self.semantic_model.encode(rewritten, convert_to_tensor=True)\n",
    "            return float(util.pytorch_cos_sim(vec_orig, vec_rewrite).item())\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error computing similarity: {e}\")\n",
    "            return 0.7  # Default reasonable value\n",
    "    \n",
    "    def score_queries(self, original: str, queries: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Scores and sorts rephrased queries by confidence.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            queries: List of rewritten queries\n",
    "            \n",
    "        Returns:\n",
    "            List of (query, score) tuples sorted by score\n",
    "        \"\"\"\n",
    "        scored = [(q, self.compute_confidence(original, q)) for q in queries]\n",
    "        return sorted(scored, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    def present_query_options(self, original: str, queries: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Presents the original and rewritten queries with confidence scores.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            queries: List of rewritten queries\n",
    "            \n",
    "        Returns:\n",
    "            List of (query, score) tuples including original query\n",
    "        \"\"\"\n",
    "        # Add original query at the top\n",
    "        scored_queries = [(\"Original: \" + original, 1.0)]\n",
    "        \n",
    "        # Add scored rewritten queries\n",
    "        rewritten_scores = self.score_queries(original, queries)\n",
    "        for i, (query, score) in enumerate(rewritten_scores, 1):\n",
    "            scored_queries.append((f\"Rewrite {i}: {query}\", score))\n",
    "            \n",
    "        # Display options in a nice format\n",
    "        console.print(\"\\n[bold cyan]Available search queries:[/bold cyan]\")\n",
    "        for i, (query, score) in enumerate(scored_queries):\n",
    "            confidence_color = \"green\" if score > 0.8 else \"yellow\" if score > 0.6 else \"red\"\n",
    "            console.print(f\"[bold]{i}.[/bold] {query}\")\n",
    "            console.print(f\"   [bold {confidence_color}]Confidence: {score:.2f}[/bold {confidence_color}]\")\n",
    "            \n",
    "        return scored_queries\n",
    "    \n",
    "    def retrieve_documents(\n",
    "        self, \n",
    "        queries: List[str], \n",
    "        k: int = 5\n",
    "    ) -> Tuple[List[RetrievedDocument], Dict[str, List[RetrievedDocument]]]:\n",
    "        \"\"\"Retrieves documents using multiple queries, preserving query information.\n",
    "        \n",
    "        Args:\n",
    "            queries: List of queries to retrieve documents for\n",
    "            k: Number of documents to retrieve per query\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (all unique documents, query->documents mapping)\n",
    "        \"\"\"\n",
    "        all_docs: List[RetrievedDocument] = []\n",
    "        unique_content = set()\n",
    "        query_docs_map: Dict[str, List[RetrievedDocument]] = {}\n",
    "        \n",
    "        # Retrieve docs for each query\n",
    "        for query in queries:\n",
    "            console.print(f\"\\n[bold blue]Query:[/bold blue] '{query}'\")\n",
    "            \n",
    "            try:\n",
    "                results = self.vector_db.similarity_search_with_relevance_scores(query, k=k)\n",
    "                docs_for_query: List[RetrievedDocument] = []\n",
    "                \n",
    "                for i, (doc, score) in enumerate(results, 1):\n",
    "                    # Create retrieved document object\n",
    "                    retrieved_doc = RetrievedDocument(\n",
    "                        document=doc,\n",
    "                        score=score,\n",
    "                        query=query,\n",
    "                        rank=i\n",
    "                    )\n",
    "                    \n",
    "                    # Display result info\n",
    "                    score_color = \"green\" if score > 0.8 else \"yellow\" if score > 0.6 else \"red\"\n",
    "                    console.print(f\"[bold]--- Result {i} ---[/bold]\")\n",
    "                    console.print(f\"[bold {score_color}]Score: {score:.4f}[/bold {score_color}]\")\n",
    "                    \n",
    "                    # Show document preview\n",
    "                    preview = doc.page_content[:150] + \"...\" if len(doc.page_content) > 150 else doc.page_content\n",
    "                    console.print(Panel(preview, title=\"Content Preview\", width=100))\n",
    "                    \n",
    "                    # Show metadata if available\n",
    "                    if hasattr(doc, 'metadata') and doc.metadata:\n",
    "                        source = doc.metadata.get('source', 'Unknown')\n",
    "                        console.print(f\"[dim]Source: {source}[/dim]\")\n",
    "                    \n",
    "                    # Add to results for this query\n",
    "                    docs_for_query.append(retrieved_doc)\n",
    "                    \n",
    "                    # Only add unique documents to overall collection\n",
    "                    if doc.page_content not in unique_content:\n",
    "                        unique_content.add(doc.page_content)\n",
    "                        all_docs.append(retrieved_doc)\n",
    "                \n",
    "                query_docs_map[query] = docs_for_query\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error retrieving documents for query '{query}': {e}\")\n",
    "                console.print(f\"[bold red]Error retrieving documents for query: {query}[/bold red]\")\n",
    "        \n",
    "        console.print(f\"\\n[bold green]Total unique documents:[/bold green] {len(all_docs)}\")\n",
    "        return all_docs, query_docs_map\n",
    "\n",
    "    def rerank_docs(self, query: str, docs: List[RetrievedDocument]) -> List[RetrievedDocument]:\n",
    "        \"\"\"Reranks documents based on semantic similarity to the query.\n",
    "        \n",
    "        Args:\n",
    "            query: Query to compare documents against\n",
    "            docs: List of retrieved documents\n",
    "            \n",
    "        Returns:\n",
    "            Reranked list of documents with updated scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            query_embedding = self.semantic_model.encode(query, convert_to_tensor=True)\n",
    "            reranked = []\n",
    "            \n",
    "            for doc in docs:\n",
    "                doc_embedding = self.semantic_model.encode(doc.document.page_content, convert_to_tensor=True)\n",
    "                new_score = float(util.pytorch_cos_sim(query_embedding, doc_embedding).item())\n",
    "                \n",
    "                # Create new RetrievedDocument with updated score\n",
    "                reranked_doc = RetrievedDocument(\n",
    "                    document=doc.document,\n",
    "                    score=new_score,\n",
    "                    query=doc.query,\n",
    "                    rank=0  # Will be updated after sorting\n",
    "                )\n",
    "                reranked.append(reranked_doc)\n",
    "            \n",
    "            # Sort by score and update ranks\n",
    "            reranked.sort(key=lambda x: x.score, reverse=True)\n",
    "            for i, doc in enumerate(reranked, 1):\n",
    "                doc.rank = i\n",
    "                \n",
    "            return reranked\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reranking documents: {e}\")\n",
    "            return docs  # Return original documents if reranking fails\n",
    "    \n",
    "    def can_answer_without_retrieval(self, question: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"Determines if a question can be answered directly from memory without retrieval.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (can_answer, answer_if_available)\n",
    "        \"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        if not chat_history:\n",
    "            return False, None\n",
    "            \n",
    "        # Create the prompt to check if we can answer directly from the conversation\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an AI assistant helping with a scientific research conversation.\n",
    "        Given the following conversation history and a new question, determine if the question:\n",
    "        1. Can be answered directly based ONLY on the conversation history (like \"what was my previous question?\")\n",
    "        2. Does NOT require retrieving new information from scientific papers\n",
    "        \n",
    "        If both conditions are true, provide the answer. Otherwise, respond with \"NEEDS_RETRIEVAL\".\n",
    "        \n",
    "        Conversation History:\n",
    "        {chat_history}\n",
    "        \n",
    "        New Question: {question}\n",
    "        \n",
    "        Your assessment (answer directly or respond with \"NEEDS_RETRIEVAL\"):\n",
    "        \"\"\")\n",
    "        \n",
    "        # Format the chat history for context\n",
    "        history_str = \"\"\n",
    "        for message in chat_history[-5:]:  # Use only last 5 messages for brevity\n",
    "            if hasattr(message, \"content\"):\n",
    "                role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                history_str += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        try:\n",
    "            # Ask the LLM if this can be answered without retrieval\n",
    "            response = self.llm.invoke(\n",
    "                prompt.format(chat_history=history_str, question=question)\n",
    "            )\n",
    "            \n",
    "            # If the response is \"NEEDS_RETRIEVAL\", we need to use retrieval\n",
    "            if \"NEEDS_RETRIEVAL\" in response:\n",
    "                return False, None\n",
    "            else:\n",
    "                logger.info(\"Question can be answered from conversation history\")\n",
    "                return True, response\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error determining if retrieval needed: {e}\")\n",
    "            return False, None  # Default to retrieval if there's an error\n",
    "    \n",
    "    def generate_final_answer(\n",
    "        self, \n",
    "        question: str, \n",
    "        docs: List[RetrievedDocument], \n",
    "        max_docs: int = 5,\n",
    "        fallback_to_llm: bool = True\n",
    "    ) -> str:\n",
    "        \"\"\"Generates the final answer from retrieved documents with fallback to LLM.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            docs: List of retrieved documents\n",
    "            max_docs: Maximum number of documents to include\n",
    "            fallback_to_llm: Whether to fall back to the LLM when docs are insufficient\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        # First, check if this might be a general knowledge question\n",
    "        is_general = self.is_general_knowledge_question(question)\n",
    "        \n",
    "        # First, check if we have any relevant documents\n",
    "        has_relevant_docs = any(doc.score >= self.relevance_threshold for doc in docs)\n",
    "        \n",
    "        # If no relevant docs were found and fallback is enabled, use LLM directly\n",
    "        if (not has_relevant_docs and fallback_to_llm) or (is_general and self.always_use_fallback):\n",
    "            if is_general:\n",
    "                console.print(\"[yellow]This appears to be a general knowledge question. Using LLM knowledge.[/yellow]\")\n",
    "            else:\n",
    "                console.print(\"[yellow]No relevant documents found. Falling back to LLM knowledge.[/yellow]\")\n",
    "            \n",
    "            # Create a direct LLM response prompt\n",
    "            fallback_prompt = PromptTemplate(\n",
    "                input_variables=[\"question\", \"chat_history\"],\n",
    "                template=\"\"\"You are a helpful scientific research assistant.\n",
    "                \n",
    "                The user has asked a question that either:\n",
    "                1. Could not be answered using our research paper database, or\n",
    "                2. Is a general knowledge question that doesn't require specific papers\n",
    "                \n",
    "                Please answer based on your general knowledge.\n",
    "                \n",
    "                {chat_history}\n",
    "                \n",
    "                Question: {question}\n",
    "                \n",
    "                Provide a helpful, informative answer. If this is a scientific or technical question,\n",
    "                explain concepts clearly and accurately. Don't apologize for not using specific papers \n",
    "                - just provide your best answer directly.\n",
    "                \n",
    "                Answer:\"\"\"\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                answer = (fallback_prompt | self.llm).invoke({\n",
    "                    \"question\": question,\n",
    "                    \"chat_history\": chat_context\n",
    "                })\n",
    "                \n",
    "                # Save the QA pair to memory\n",
    "                self.memory.chat_memory.add_messages([\n",
    "                    HumanMessage(content=question),\n",
    "                    AIMessage(content=answer)\n",
    "                ])\n",
    "                \n",
    "                return answer\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating fallback answer: {e}\")\n",
    "                return \"I'm sorry, I don't have enough information to answer your question accurately.\"\n",
    "        \n",
    "        # Format document content with metadata\n",
    "        context_pieces = []\n",
    "        for i, doc in enumerate(docs[:max_docs]):\n",
    "            chunk = f\"Document {i+1} [Relevance: {doc.score:.2f}]:\\n{doc.document.page_content}\"\n",
    "            \n",
    "            # Add metadata if available\n",
    "            if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                source = doc.document.metadata.get('source', 'Unknown')\n",
    "                chunk += f\"\\nSource: {source}\"\n",
    "                \n",
    "            context_pieces.append(chunk)\n",
    "            \n",
    "        context = \"\\n\\n---\\n\\n\".join(context_pieces)\n",
    "        \n",
    "        # If we have context but it might not be sufficient, use a prompt that can fall back to general knowledge\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful scientific research assistant analyzing scientific papers. \n",
    "            Use the following retrieved context chunks to answer the user's question thoroughly.\n",
    "            \n",
    "            Guidelines:\n",
    "            - Focus on providing accurate information from the papers\n",
    "            - Synthesize information across documents when appropriate\n",
    "            - Cite the sources of information in your answer (e.g., \"According to Document 1...\")\n",
    "            - If the information isn't sufficient in the context, supplement with your general knowledge,\n",
    "              clearly indicating which parts of your answer come from the papers and which parts are from your general knowledge\n",
    "            - If the context contains conflicting information, highlight the disagreement and possible reasons\n",
    "            - Maintain scientific accuracy above all else\n",
    "            - Don't apologize for using general knowledge - just be clear about what comes from papers vs. your knowledge\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Generate answer\n",
    "            answer = (prompt | self.llm).invoke({\n",
    "                \"question\": question, \n",
    "                \"context\": context,\n",
    "                \"chat_history\": chat_context\n",
    "            })\n",
    "            \n",
    "            # Save the QA pair to memory\n",
    "            self.memory.chat_memory.add_messages([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=answer)\n",
    "            ])\n",
    "            \n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating answer: {e}\")\n",
    "            return \"I'm sorry, I encountered an error while generating your answer. Please try rephrasing your question.\"\n",
    "    \n",
    "    def generate_combined_answer(\n",
    "        self, \n",
    "        question: str, \n",
    "        query_results: Dict[str, List[RetrievedDocument]],\n",
    "        fallback_to_llm: bool = True\n",
    "    ) -> str:\n",
    "        \"\"\"Generates a combined answer from multiple query results with fallback.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            query_results: Dictionary mapping queries to retrieved documents\n",
    "            fallback_to_llm: Whether to fall back to the LLM when docs are insufficient\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        # Check if this might be a general knowledge question\n",
    "        is_general = self.is_general_knowledge_question(question)\n",
    "        \n",
    "        # Check if we have any relevant documents at all\n",
    "        all_docs = []\n",
    "        for docs_list in query_results.values():\n",
    "            all_docs.extend(docs_list)\n",
    "            \n",
    "        has_relevant_docs = any(doc.score >= self.relevance_threshold for doc in all_docs)\n",
    "        \n",
    "        # If no relevant documents and fallback enabled, use LLM directly\n",
    "        if (not has_relevant_docs and fallback_to_llm) or (is_general and self.always_use_fallback):\n",
    "            if is_general:\n",
    "                console.print(\"[yellow]This appears to be a general knowledge question. Using LLM knowledge.[/yellow]\")\n",
    "            else:\n",
    "                console.print(\"[yellow]No relevant documents found across all queries. Falling back to LLM knowledge.[/yellow]\")\n",
    "            \n",
    "            # Create a direct LLM response prompt\n",
    "            fallback_prompt = PromptTemplate(\n",
    "                input_variables=[\"question\", \"chat_history\"],\n",
    "                template=\"\"\"You are a helpful scientific research assistant.\n",
    "                \n",
    "                The user has asked a question that either:\n",
    "                1. Could not be answered using our research paper database, or\n",
    "                2. Is a general knowledge question that doesn't require specific papers\n",
    "                \n",
    "                Please answer based on your general knowledge.\n",
    "                \n",
    "                {chat_history}\n",
    "                \n",
    "                Question: {question}\n",
    "                \n",
    "                Provide a helpful, informative answer. If this is a scientific or technical question,\n",
    "                explain concepts clearly and accurately. Don't apologize for not using specific papers \n",
    "                - just provide your best answer directly.\n",
    "                \n",
    "                Answer:\"\"\"\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                answer = (fallback_prompt | self.llm).invoke({\n",
    "                    \"question\": question,\n",
    "                    \"chat_history\": chat_context\n",
    "                })\n",
    "                \n",
    "                # Save the QA pair to memory\n",
    "                self.memory.chat_memory.add_messages([\n",
    "                    HumanMessage(content=question),\n",
    "                    AIMessage(content=answer)\n",
    "                ])\n",
    "                \n",
    "                return answer\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating fallback answer: {e}\")\n",
    "                return \"I'm sorry, I don't have enough information to answer your question accurately.\"\n",
    "        \n",
    "        # Combine all relevant documents from different queries\n",
    "        all_context_sections = []\n",
    "        \n",
    "        for query, docs in query_results.items():\n",
    "            # Use only the top N documents for each query\n",
    "            docs_for_query = docs[:self.max_docs_per_query]\n",
    "            if docs_for_query:\n",
    "                all_context_sections.append(f\"Results for query: '{query}'\")\n",
    "                for i, doc in enumerate(docs_for_query, 1):\n",
    "                    section = f\"Document {i} [Relevance: {doc.score:.2f}]:\\n{doc.document.page_content}\"\n",
    "                    \n",
    "                    # Add metadata if available\n",
    "                    if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                        source = doc.document.metadata.get('source', 'Unknown')\n",
    "                        section += f\"\\nSource: {source}\"\n",
    "                        \n",
    "                    all_context_sections.append(section)\n",
    "        \n",
    "        # Join all context sections\n",
    "        combined_context = \"\\n\\n---\\n\\n\".join(all_context_sections)\n",
    "        \n",
    "        # Create a prompt that emphasizes synthesizing information and can fall back to general knowledge\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful scientific research assistant analyzing scientific papers. \n",
    "            The user's question has been reformulated in several ways, and each formulation returned different documents.\n",
    "            \n",
    "            Guidelines:\n",
    "            - Synthesize information across ALL retrieved documents to provide a comprehensive answer\n",
    "            - Compare and contrast findings from different sources\n",
    "            - Highlight the most relevant information from each source\n",
    "            - Cite the specific documents you're referencing (e.g., \"According to the paper in Document 3...\")\n",
    "            - If information conflicts across sources, explain the different perspectives\n",
    "            - If the information is insufficient, supplement with your general knowledge,\n",
    "              clearly indicating which parts of your answer come from the papers and which are from your general knowledge\n",
    "            - Maintain scientific accuracy above all else\n",
    "            - Don't apologize for using general knowledge - just be clear about what information comes from papers vs. your knowledge\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Context from multiple query formulations:\n",
    "            {context}\n",
    "            \n",
    "            Original Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Generate answer\n",
    "            answer = (prompt | self.llm).invoke({\n",
    "                \"question\": question, \n",
    "                \"context\": combined_context,\n",
    "                \"chat_history\": chat_context\n",
    "            })\n",
    "            \n",
    "            # Save the QA pair to memory\n",
    "            self.memory.chat_memory.add_messages([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=answer)\n",
    "            ])\n",
    "            \n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating combined answer: {e}\")\n",
    "            return \"I'm sorry, I encountered an error while generating your answer. Please try rephrasing your question.\"\n",
    "    \n",
    "    def answer_query(self, query: str, use_retrieval: bool = True) -> str:\n",
    "        \"\"\"Direct answer method that either does retrieval or uses LLM fallback.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            use_retrieval: Whether to use retrieval process or direct LLM\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # First check if we can answer directly from conversation history\n",
    "        can_answer, direct_answer = self.can_answer_without_retrieval(query)\n",
    "        if can_answer:\n",
    "            console.print(\"[green]Answering from conversation history[/green]\")\n",
    "            return direct_answer\n",
    "        \n",
    "        # Check if this is a general knowledge question\n",
    "        is_general = self.is_general_knowledge_question(query)\n",
    "        \n",
    "        # If it's general knowledge and we're set to always use fallback for those\n",
    "        if is_general and self.always_use_fallback and not use_retrieval:\n",
    "            console.print(\"[yellow]General knowledge question detected. Using LLM directly.[/yellow]\")\n",
    "            # Create a direct LLM response prompt\n",
    "            chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "            chat_context = \"\"\n",
    "            if chat_history:\n",
    "                chat_context = \"Previous conversation:\\n\"\n",
    "                for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                    if hasattr(message, \"content\"):\n",
    "                        role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                        chat_context += f\"{role}: {message.content}\\n\"\n",
    "            \n",
    "            fallback_prompt = PromptTemplate(\n",
    "                input_variables=[\"question\", \"chat_history\"],\n",
    "                template=\"\"\"You are a helpful scientific research assistant.\n",
    "                \n",
    "                The user has asked a general knowledge question that doesn't require specific papers.\n",
    "                Please answer based on your general knowledge.\n",
    "                \n",
    "                {chat_history}\n",
    "                \n",
    "                Question: {question}\n",
    "                \n",
    "                Provide a helpful, informative answer. If this is a scientific or technical question,\n",
    "                explain concepts clearly and accurately.\n",
    "                \n",
    "                Answer:\"\"\"\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                answer = (fallback_prompt | self.llm).invoke({\n",
    "                    \"question\": query,\n",
    "                    \"chat_history\": chat_context\n",
    "                })\n",
    "                \n",
    "                # Save the QA pair to memory\n",
    "                self.memory.chat_memory.add_messages([\n",
    "                    HumanMessage(content=query),\n",
    "                    AIMessage(content=answer)\n",
    "                ])\n",
    "                \n",
    "                return answer\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating direct answer: {e}\")\n",
    "                return \"I'm sorry, I encountered an error while generating your answer.\"\n",
    "        \n",
    "        # Otherwise, proceed with full retrieval pipeline\n",
    "        return self.search_with_query_feedback(query)\n",
    "    \n",
    "    def search_with_query_feedback(self, query: str, num_results: int = 5) -> str:\n",
    "        \"\"\"Main pipeline that processes a query and returns an answer.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            num_results: Number of results to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Rate the original query\n",
    "            console.print(\"\\n[bold cyan]Evaluating your query...[/bold cyan]\")\n",
    "            rating_result = self.rate_query(query)\n",
    "            \n",
    "            rating_color = \"green\" if rating_result['rating'] >= 4 else \"yellow\" if rating_result['rating'] >= 3 else \"red\"\n",
    "            console.print(f\"[bold {rating_color}]Query Rating: {rating_result['rating']}/5[/bold {rating_color}]\")\n",
    "            console.print(f\"Explanation: {rating_result['explanation']}\")\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Check if this is a general knowledge question\n",
    "            is_general = self.is_general_knowledge_question(query)\n",
    "            if is_general and self.always_use_fallback:\n",
    "                console.print(\"[yellow]This appears to be a general knowledge question. Proceeding with simplified search.[/yellow]\")\n",
    "                # For general knowledge questions, we might still want to check the database briefly\n",
    "                # but we'll rely more on the LLM fallback\n",
    "            \n",
    "            # Step 2: Suggest rewrites if the rating is less than perfect\n",
    "            rewritten_queries = []\n",
    "            if rating_result[\"rating\"] < 5:\n",
    "                console.print(\"[bold cyan]Generating improved query variations...[/bold cyan]\")\n",
    "                # Get chat history for context-aware rewrites\n",
    "                chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "                rewritten_queries = self.suggest_rewrites(query, chat_history)\n",
    "                console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 3: Present options to the user\n",
    "            query_options = self.present_query_options(query, rewritten_queries)\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 4: Get user selection\n",
    "            console.print(\"[bold cyan]Select queries to use:[/bold cyan]\")\n",
    "            console.print(\"Enter the numbers of the queries you want to use (comma-separated, e.g., '0,2,3')\")\n",
    "            console.print(\"Or press Enter to use all queries\")\n",
    "            selected_indices_input = input(\"> \")\n",
    "            \n",
    "            if selected_indices_input.strip() == \"\":\n",
    "                # Use all queries if no selection\n",
    "                selected_indices = list(range(len(query_options)))\n",
    "            else:\n",
    "                try:\n",
    "                    selected_indices = [int(idx.strip()) for idx in selected_indices_input.split(\",\")]\n",
    "                except ValueError:\n",
    "                    console.print(\"[bold red]Invalid input. Using original query only.[/bold red]\")\n",
    "                    selected_indices = [0]  # Default to original query\n",
    "            \n",
    "            # Get the selected queries (without the prefix and score)\n",
    "            selected_queries = []\n",
    "            for idx in selected_indices:\n",
    "                if 0 <= idx < len(query_options):\n",
    "                    query_text = query_options[idx][0]\n",
    "                    # Remove the \"Original: \" or \"Rewrite N: \" prefix\n",
    "                    if \"Original: \" in query_text:\n",
    "                        query_text = query_text.replace(\"Original: \", \"\")\n",
    "                    elif \"Rewrite \" in query_text:\n",
    "                        query_text = query_text.split(\": \", 1)[1] if \": \" in query_text else query_text\n",
    "                    selected_queries.append(query_text)\n",
    "            \n",
    "            if not selected_queries:\n",
    "                console.print(\"[bold red]No valid queries selected. Using original query.[/bold red]\")\n",
    "                selected_queries = [query]\n",
    "                \n",
    "            console.print(f\"\\n[bold green]Selected {len(selected_queries)} queries for retrieval.[/bold green]\")\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 5: Retrieve documents\n",
    "            console.print(\"[bold cyan]Retrieving relevant documents...[/bold cyan]\")\n",
    "            docs, query_docs_map = self.retrieve_documents(selected_queries, k=num_results)\n",
    "            \n",
    "            # Step 6: Generate answer\n",
    "            if len(selected_queries) > 1:\n",
    "                console.print(\"\\n[bold cyan]Generating combined answer based on multiple query results...[/bold cyan]\")\n",
    "                final_answer = self.generate_combined_answer(\n",
    "                    query, \n",
    "                    query_docs_map,\n",
    "                    fallback_to_llm=True  # Always enable fallback\n",
    "                )\n",
    "            else:\n",
    "                # For single query, rerank and use the traditional approach\n",
    "                console.print(\"\\n[bold cyan]Reranking documents based on relevance to the original query...[/bold cyan]\")\n",
    "                reranked_docs = self.rerank_docs(query, docs)\n",
    "                \n",
    "                # Apply relevance threshold\n",
    "                filtered_docs = [\n",
    "                    doc for doc in reranked_docs if doc.score >= self.relevance_threshold\n",
    "                ]\n",
    "                \n",
    "                # Display relevance information\n",
    "                has_relevant_docs = len(filtered_docs) > 0\n",
    "                if not has_relevant_docs:\n",
    "                    console.print(\"[yellow]No documents met the relevance threshold. Using all retrieved documents but may fall back to LLM knowledge.[/yellow]\")\n",
    "                    filtered_docs = reranked_docs\n",
    "                \n",
    "                # Generate the final answer with fallback enabled\n",
    "                console.print(\"\\n[bold cyan]Generating answer...[/bold cyan]\")\n",
    "                final_answer = self.generate_final_answer(\n",
    "                    query, \n",
    "                    filtered_docs, \n",
    "                    fallback_to_llm=True  # Always enable fallback\n",
    "                )\n",
    "            \n",
    "            return final_answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in search pipeline: {e}\")\n",
    "            # Fall back to direct LLM answer if the pipeline fails\n",
    "            console.print(\"[bold red]Error in search pipeline. Falling back to LLM.[/bold red]\")\n",
    "            \n",
    "            chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "            chat_context = \"\"\n",
    "            if chat_history:\n",
    "                chat_context = \"Previous conversation:\\n\"\n",
    "                for message in chat_history[-3:]:\n",
    "                    if hasattr(message, \"content\"):\n",
    "                        role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                        chat_context += f\"{role}: {message.content}\\n\"\n",
    "            \n",
    "            # Create a direct LLM response as emergency fallback\n",
    "            emergency_prompt = PromptTemplate(\n",
    "                input_variables=[\"question\", \"chat_history\"],\n",
    "                template=\"\"\"You are a helpful scientific research assistant.\n",
    "                \n",
    "                There was an error retrieving information from our research database.\n",
    "                Please answer the question based on your general knowledge.\n",
    "                \n",
    "                {chat_history}\n",
    "                \n",
    "                Question: {question}\n",
    "                \n",
    "                Answer:\"\"\"\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                answer = (emergency_prompt | self.llm).invoke({\n",
    "                    \"question\": query,\n",
    "                    \"chat_history\": chat_context\n",
    "                })\n",
    "                return answer\n",
    "            except:\n",
    "                return \"I apologize, but I'm having technical difficulties. Please try again with a different question.\"\n",
    "\n",
    "# Example usage implementation \n",
    "def create_research_assistant(llm, vector_db, embeddings_model=None):\n",
    "    \"\"\"Factory function to create a research assistant with the specified components.\"\"\"\n",
    "    return ResearchAssistant(\n",
    "        llm=llm,\n",
    "        vector_db=vector_db,\n",
    "        embeddings_model=embeddings_model,\n",
    "        relevance_threshold=0.5,  # Lower threshold to be more lenient with document relevance\n",
    "        max_docs_per_query=3,\n",
    "        always_use_fallback=True  # Always use LLM fallback for general knowledge\n",
    "    )\n",
    "\n",
    "# Interactive command-line interface\n",
    "def run_cli(assistant):\n",
    "    \"\"\"Run an interactive CLI for the research assistant.\"\"\"\n",
    "    console.print(\"[bold green]Research Assistant CLI[/bold green]\")\n",
    "    console.print(\"Type 'exit' to quit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\n[bold blue]Ask a question:[/bold blue] \")\n",
    "        if question.lower() in ('exit', 'quit', 'q'):\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            answer = assistant.answer_query(question)\n",
    "            console.print(\"\\n[bold green]Answer:[/bold green]\")\n",
    "            console.print(Panel(Markdown(answer), width=100))\n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Error:[/bold red] {e}\")\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # You would need to define these before using:\n",
    "    # from langchain.llms import ChatOpenAI\n",
    "    # from langchain.vectorstores import Chroma\n",
    "    # \n",
    "    # llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    # vector_db = Chroma(embedding_function=...)\n",
    "    # \n",
    "    # assistant = create_research_assistant(llm, vector_db)\n",
    "    # run_cli(assistant)\n",
    "    print(\"Import this module to use the ResearchAssistant class\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
