{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f991b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"ResearchPro2\" \n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = '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'\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b679fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install and import necessary packages\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import BaseOutputParser, StrOutputParser\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "from typing import List, Literal\n",
    "import re\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fed665",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "PDF_DIR = \"./arxiv_data\"\n",
    "DB_DIR = \"./arxiv_vector1_db\"\n",
    "\n",
    "# Embedding & Vector DB\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_db = Chroma(persist_directory=DB_DIR, embedding_function=embedding_model)\n",
    "\n",
    "# Load embedding model once for scoring\n",
    "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# LLM\n",
    "llm = OllamaLLM(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7cdea8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, BaseOutputParser\n",
    "from langchain.schema import Document, HumanMessage, AIMessage\n",
    "from langchain.vectorstores import VectorStore\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms.base import BaseLLM\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Output Parser\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        lines = re.findall(r\"^\\d+\\.\\s+(.*)\", text, re.MULTILINE)\n",
    "        return [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "class ResearchAssistant:\n",
    "    def __init__(self, llm: BaseLLM, vector_db: VectorStore, embeddings_model: Optional[SentenceTransformer] = None):\n",
    "        \"\"\"Initialize the Research Assistant with LLM and vector database.\"\"\"\n",
    "        self.llm = llm\n",
    "        self.vector_db = vector_db\n",
    "        \n",
    "        # Initialize sentence transformer model for semantic similarity if not provided\n",
    "        if embeddings_model is None:\n",
    "            self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        else:\n",
    "            self.semantic_model = embeddings_model\n",
    "            \n",
    "        # Initialize memory - we'll manage it manually instead of using ConversationalRetrievalChain\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        # Create a retriever for direct use\n",
    "        self.retriever = vector_db.as_retriever()\n",
    "    \n",
    "    def rate_query(self, query: str) -> Dict[str, str | int]:\n",
    "        \"\"\"Rates the query and gives an explanation.\"\"\"\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an intelligent assistant trained to evaluate search queries.\n",
    "        Given the following query: \"{query}\"\n",
    "        1. Rate the query on a scale of 1 (very poor) to 5 (excellent) based on:\n",
    "           - Clarity\n",
    "           - Specificity\n",
    "           - Relevance\n",
    "           - Retrievability\n",
    "        2. Provide a short explanation for the rating (if it is vague, incomplete, or inefficient).\n",
    "        Respond in JSON format:\n",
    "        {{\n",
    "          \"rating\": 3,\n",
    "          \"explanation\": \"Too vague, lacks keywords.\"\n",
    "        }}\n",
    "        \"\"\")\n",
    "        parser = JsonOutputParser()\n",
    "        chain: Runnable = prompt | self.llm | parser\n",
    "        return chain.invoke({\"query\": query})\n",
    "    \n",
    "    def suggest_rewrites(self, query: str) -> List[str]:\n",
    "        \"\"\"Returns 5 rephrased versions of the query.\"\"\"\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\"],\n",
    "            template=\"\"\"You are an AI assistant. Rephrase the question in 5 different ways to improve retrieval from a document store. Number each version starting with 1.\\nQuestion: {question}\"\"\"\n",
    "        )\n",
    "        return (prompt | self.llm | LineListOutputParser()).invoke({\"question\": query})\n",
    "    \n",
    "    def compute_confidence(self, original: str, rewritten: str) -> float:\n",
    "        \"\"\"Computes semantic similarity between original and rewritten queries.\"\"\"\n",
    "        vec_orig = self.semantic_model.encode(original, convert_to_tensor=True)\n",
    "        vec_rewrite = self.semantic_model.encode(rewritten, convert_to_tensor=True)\n",
    "        return util.pytorch_cos_sim(vec_orig, vec_rewrite).item()\n",
    "    \n",
    "    def score_queries(self, original: str, queries: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Scores and sorts rephrased queries by confidence.\"\"\"\n",
    "        scored = [(q, self.compute_confidence(original, q)) for q in queries]\n",
    "        scored.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scored\n",
    "    \n",
    "    def present_query_options(self, original: str, queries: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Presents the original and rewritten queries with confidence scores.\"\"\"\n",
    "        # Add original query at the top\n",
    "        scored_queries = [(\"Original: \" + original, 1.0)]\n",
    "        \n",
    "        # Add scored rewritten queries\n",
    "        rewritten_scores = self.score_queries(original, queries)\n",
    "        for i, (query, score) in enumerate(rewritten_scores, 1):\n",
    "            scored_queries.append((f\"Rewrite {i}: {query}\", score))\n",
    "            \n",
    "        # Print options for user\n",
    "        print(\"Available search queries (with confidence scores):\")\n",
    "        for i, (query, score) in enumerate(scored_queries):\n",
    "            print(f\"{i}. {query} [Confidence: {score:.2f}]\")\n",
    "            \n",
    "        return scored_queries\n",
    "    \n",
    "    def retrieve_documents(self, queries: List[str], k: int = 5) -> Tuple[List[Document], Dict[str, List[Document]]]:\n",
    "        \"\"\"\n",
    "        Retrieves documents using multiple queries, preserving which query returned which documents.\n",
    "        Shows relevance scores for better transparency.\n",
    "        \"\"\"\n",
    "        all_docs = []\n",
    "        unique_docs = {}\n",
    "        query_docs_map = {}\n",
    "        \n",
    "        # Retrieve docs for each query\n",
    "        for query in queries:\n",
    "            print(f\"\\nQuery: '{query}'\")\n",
    "            results = self.vector_db.similarity_search_with_relevance_scores(query, k=k)\n",
    "            docs = []\n",
    "            \n",
    "            for i, (doc, score) in enumerate(results, 1):\n",
    "                print(f\"--- Result {i} ---\")\n",
    "                print(f\"Score: {score:.4f}\")\n",
    "                print(f\"Chunk:\\n{doc.page_content[:150]}...\")\n",
    "                if hasattr(doc, 'metadata') and doc.metadata:\n",
    "                    source = doc.metadata.get('source', 'Unknown')\n",
    "                    print(f\"Source: {source}\")\n",
    "                print()\n",
    "                \n",
    "                docs.append(doc)\n",
    "                \n",
    "                # Add to our overall collection with deduplication\n",
    "                if doc.page_content not in unique_docs:\n",
    "                    unique_docs[doc.page_content] = doc\n",
    "                    all_docs.append(doc)\n",
    "            \n",
    "            query_docs_map[query] = docs\n",
    "        \n",
    "        print(f\"\\nTotal unique documents: {len(all_docs)}\")\n",
    "        return all_docs, query_docs_map\n",
    "\n",
    "    def rerank_docs(self, query: str, docs: List[Document]) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"Reranks documents based on semantic similarity to the query.\"\"\"\n",
    "        query_embedding = self.semantic_model.encode(query, convert_to_tensor=True)\n",
    "        reranked = []\n",
    "        for doc in docs:\n",
    "            doc_embedding = self.semantic_model.encode(doc.page_content, convert_to_tensor=True)\n",
    "            score = util.pytorch_cos_sim(query_embedding, doc_embedding).item()\n",
    "            reranked.append((doc, score))\n",
    "        return sorted(reranked, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    def can_answer_without_retrieval(self, question: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Determines if a question can be answered directly from memory without retrieval.\n",
    "        Returns a tuple: (can_answer, answer_if_available)\n",
    "        \"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        if not chat_history:\n",
    "            return False, None\n",
    "            \n",
    "        # Create the prompt to check if we can answer directly from the conversation\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an AI assistant helping with a conversation.\n",
    "        Given the following conversation history and a new question, determine if the question:\n",
    "        1. Can be answered directly based ONLY on the conversation history (like \"what was my previous question?\")\n",
    "        2. Does NOT require retrieving new information from documents\n",
    "        \n",
    "        If both conditions are true, provide the answer. Otherwise, respond with \"NEEDS_RETRIEVAL\".\n",
    "        \n",
    "        Conversation History:\n",
    "        {chat_history}\n",
    "        \n",
    "        New Question: {question}\n",
    "        \n",
    "        Your assessment (answer directly or respond with \"NEEDS_RETRIEVAL\"):\n",
    "        \"\"\")\n",
    "        \n",
    "        # Format the chat history for context\n",
    "        history_str = \"\"\n",
    "        for message in chat_history:\n",
    "            if hasattr(message, \"content\"):\n",
    "                role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                history_str += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        # Ask the LLM if this can be answered without retrieval\n",
    "        response = self.llm.invoke(\n",
    "            prompt.format(chat_history=history_str, question=question)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # If the response is \"NEEDS_RETRIEVAL\", we need to use retrieval\n",
    "        if \"NEEDS_RETRIEVAL\" in response:\n",
    "            return False, None\n",
    "        else:\n",
    "            return True, response\n",
    "    \n",
    "    def generate_final_answer(self, question: str, docs: List[Document], max_docs: int = 5) -> str:\n",
    "        \"\"\"Generates the final answer from retrieved documents.\"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history:\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "            \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful research assistant. \n",
    "            Use the following retrieved context chunks to answer the user's question thoroughly.\n",
    "            If the information isn't in the context, indicate what's missing rather than making up information.\n",
    "            If the context contains conflicting or uncertain information, highlight the disagreement. \n",
    "            Do not fabricate any facts not grounded in the provided context.\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        context = \"\\n\\n---\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" \n",
    "                                     for i, doc in enumerate(docs[:max_docs])])\n",
    "        \n",
    "        answer = (prompt | self.llm).invoke({\n",
    "            \"question\": question, \n",
    "            \"context\": context,\n",
    "            \"chat_history\": chat_context\n",
    "        })\n",
    "        \n",
    "        # Save the QA pair to memory manually\n",
    "        self.memory.chat_memory.add_messages([\n",
    "            HumanMessage(content=question),\n",
    "            AIMessage(content=answer)\n",
    "        ])\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def generate_combined_answer(self, question: str, query_results: Dict[str, List[Document]], max_docs_per_query: int = 3) -> str:\n",
    "        \"\"\"Generates a combined answer from multiple query results.\"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history:\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        # Combine all relevant documents from different queries\n",
    "        all_context_sections = []\n",
    "        for query, docs in query_results.items():\n",
    "            # Use only the top N documents for each query\n",
    "            docs_for_query = docs[:max_docs_per_query]\n",
    "            if docs_for_query:\n",
    "                all_context_sections.append(f\"Results for query: '{query}'\")\n",
    "                for i, doc in enumerate(docs_for_query, 1):\n",
    "                    all_context_sections.append(f\"Document {i}:\\n{doc.page_content}\")\n",
    "        \n",
    "        # Join all context sections\n",
    "        combined_context = \"\\n\\n---\\n\\n\".join(all_context_sections)\n",
    "        \n",
    "        # Create a prompt that emphasizes synthesizing information across different query results\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful research assistant. \n",
    "            The user's question has been reformulated in several ways, and each formulation returned different documents.\n",
    "            Use the following retrieved context chunks from ALL query variations to comprehensively answer the user's question.\n",
    "            Synthesize information across all retrieved documents to provide the most complete answer possible.\n",
    "            If the information isn't in the context, indicate what's missing rather than making up information.\n",
    "            If the context contains conflicting information from different query results, highlight the disagreement.\n",
    "            Do not fabricate any facts not grounded in the provided context.\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Context from multiple query formulations:\n",
    "            {context}\n",
    "            \n",
    "            Original Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        answer = (prompt | self.llm).invoke({\n",
    "            \"question\": question, \n",
    "            \"context\": combined_context,\n",
    "            \"chat_history\": chat_context\n",
    "        })\n",
    "        \n",
    "        # Save the QA pair to memory manually\n",
    "        self.memory.chat_memory.add_messages([\n",
    "            HumanMessage(content=question),\n",
    "            AIMessage(content=answer)\n",
    "        ])\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def search_with_query_feedback(self, query: str, num_results: int = 5) -> str:\n",
    "        \"\"\"Main pipeline that processes a query and returns an answer.\"\"\"\n",
    "        # Step 1: Rate the original query\n",
    "        print(\"Evaluating your query...\")\n",
    "        rating_result = self.rate_query(query)\n",
    "        print(f\"Query Rating: {rating_result['rating']}/5\")\n",
    "        print(f\"Explanation: {rating_result['explanation']}\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 2: Suggest rewrites if the rating is less than perfect\n",
    "        rewritten_queries = []\n",
    "        if rating_result[\"rating\"] < 5:\n",
    "            print(\"Generating improved query variations...\")\n",
    "            rewritten_queries = self.suggest_rewrites(query)\n",
    "            print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 3: Present options to the user\n",
    "        query_options = self.present_query_options(query, rewritten_queries)\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 4: Get user selection\n",
    "        selected_indices = input(\"Enter the numbers of the queries you want to use (comma-separated, e.g., '0,2,3'): \")\n",
    "        try:\n",
    "            selected_indices = [int(idx.strip()) for idx in selected_indices.split(\",\")]\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter numbers separated by commas.\")\n",
    "            return \"Invalid query selection. Please try again.\"\n",
    "        \n",
    "        # Get the selected queries (without the prefix and score)\n",
    "        selected_queries = []\n",
    "        for idx in selected_indices:\n",
    "            if idx == 0:  # Original query\n",
    "                selected_queries.append(query)\n",
    "            else:  # Rewritten query\n",
    "                # Check if the index is valid\n",
    "                if idx <= len(rewritten_queries):\n",
    "                    query_text = rewritten_queries[idx-1]\n",
    "                    selected_queries.append(query_text)\n",
    "        \n",
    "        if not selected_queries:\n",
    "            print(\"No valid queries selected.\")\n",
    "            return \"No valid queries were selected. Please try again.\"\n",
    "            \n",
    "        print(f\"\\nSelected {len(selected_queries)} queries for retrieval.\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 5: Retrieve documents\n",
    "        print(\"Retrieving relevant documents...\")\n",
    "        docs, query_docs_map = self.retrieve_documents(selected_queries, k=num_results)\n",
    "        \n",
    "        # Step 6: Generate answer\n",
    "        # If using multiple queries, use the combined answer approach\n",
    "        if len(selected_queries) > 1:\n",
    "            print(\"\\nGenerating combined answer based on multiple query results...\")\n",
    "            final_answer = self.generate_combined_answer(query, query_docs_map, max_docs_per_query=3)\n",
    "        else:\n",
    "            # For single query, rerank and use the traditional approach\n",
    "            print(\"\\nReranking documents based on relevance to the original query...\")\n",
    "            reranked_docs_with_scores = self.rerank_docs(query, docs)\n",
    "    \n",
    "            # Apply relevance threshold\n",
    "            relevance_threshold = 0.6  # adjust as needed\n",
    "            filtered_docs_with_scores = [\n",
    "                (doc, score) for doc, score in reranked_docs_with_scores if score >= relevance_threshold\n",
    "            ]\n",
    "            \n",
    "            # If no documents meet the threshold, use all documents\n",
    "            if not filtered_docs_with_scores:\n",
    "                print(\"No documents met the relevance threshold. Using all retrieved documents.\")\n",
    "                filtered_docs_with_scores = reranked_docs_with_scores\n",
    "            \n",
    "            # Sort top N\n",
    "            filtered_docs_with_scores = sorted(filtered_docs_with_scores, key=lambda x: x[1], reverse=True)\n",
    "            filtered_docs = [doc for doc, _ in filtered_docs_with_scores]\n",
    "            \n",
    "            # Display top reranked documents\n",
    "            print(\"\\nTop reranked documents:\")\n",
    "            for i, (doc, score) in enumerate(filtered_docs_with_scores[:num_results], 1):\n",
    "                print(f\"{i}. Score: {score:.4f}\")\n",
    "                print(f\"   Preview: {doc.page_content[:150]}...\")\n",
    "                if hasattr(doc, 'metadata') and doc.metadata:\n",
    "                    print(f\"   Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "                    \n",
    "            # Generate the final answer\n",
    "            print(\"\\nGenerating your answer based on retrieved documents...\")\n",
    "            final_answer = self.generate_final_answer(query, filtered_docs[:num_results])\n",
    "        \n",
    "        return final_answer\n",
    "    \n",
    "    def ask_with_memory(self, question: str, num_results: int = 5) -> str:\n",
    "        \"\"\"\n",
    "        Uses conversation memory to process follow-up questions.\n",
    "        Now includes query improvement and document retrieval.\n",
    "        \"\"\"\n",
    "        # First, check if this is a question we can answer directly from memory\n",
    "        can_direct_answer, direct_answer = self.can_answer_without_retrieval(question)\n",
    "        if can_direct_answer:\n",
    "            print(\"Question can be answered directly from conversation history...\")\n",
    "            # Save the QA pair to memory manually\n",
    "            self.memory.chat_memory.add_messages([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=direct_answer)\n",
    "            ])\n",
    "            return direct_answer\n",
    "        \n",
    "        # Otherwise, use the full query improvement pipeline\n",
    "        # Step 1: Rate the query\n",
    "        print(\"Evaluating your query (with memory context)...\")\n",
    "        rating_result = self.rate_query(question)\n",
    "        print(f\"Query Rating: {rating_result['rating']}/5\")\n",
    "        print(f\"Explanation: {rating_result['explanation']}\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 2: Suggest rewrites if the rating is less than perfect\n",
    "        rewritten_queries = []\n",
    "        if rating_result[\"rating\"] < 5:\n",
    "            print(\"Generating improved query variations...\")\n",
    "            rewritten_queries = self.suggest_rewrites(question)\n",
    "            print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 3: Present options to the user\n",
    "        query_options = self.present_query_options(question, rewritten_queries)\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 4: Get user selection\n",
    "        selected_indices = input(\"Enter the numbers of the queries you want to use (comma-separated, e.g., '0,2,3'): \")\n",
    "        try:\n",
    "            selected_indices = [int(idx.strip()) for idx in selected_indices.split(\",\")]\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter numbers separated by commas.\")\n",
    "            return \"Invalid query selection. Please try again.\"\n",
    "        \n",
    "        # Get the selected queries\n",
    "        selected_queries = []\n",
    "        for idx in selected_indices:\n",
    "            if idx == 0:  # Original query\n",
    "                selected_queries.append(question)\n",
    "            else:  # Rewritten query\n",
    "                # Check if the index is valid\n",
    "                if idx <= len(rewritten_queries):\n",
    "                    query_text = rewritten_queries[idx-1]\n",
    "                    selected_queries.append(query_text)\n",
    "        \n",
    "        if not selected_queries:\n",
    "            print(\"No valid queries selected.\")\n",
    "            return \"No valid queries were selected. Please try again.\"\n",
    "            \n",
    "        print(f\"\\nSelected {len(selected_queries)} queries for retrieval.\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 5: Retrieve documents\n",
    "        print(\"Retrieving relevant documents...\")\n",
    "        if len(selected_queries) > 1:\n",
    "            # If multiple queries were selected, use the multi-query approach\n",
    "            docs, query_docs_map = self.retrieve_documents(selected_queries, k=num_results)\n",
    "            \n",
    "            # Generate a combined answer\n",
    "            print(\"\\nGenerating combined answer with conversation context...\")\n",
    "            answer = self.generate_combined_answer(question, query_docs_map)\n",
    "        else:\n",
    "            # If only one query was selected, use the standard approach\n",
    "            docs = self.retriever.get_relevant_documents(selected_queries[0])\n",
    "            \n",
    "            print(f\"Retrieved {len(docs)} documents\")\n",
    "            \n",
    "            # Show previews of the retrieved documents\n",
    "            print(\"\\nTop retrieved documents:\")\n",
    "            for i, doc in enumerate(docs[:5], 1):\n",
    "                print(f\"{i}. Preview: {doc.page_content[:150]}...\")\n",
    "                if hasattr(doc, 'metadata') and doc.metadata:\n",
    "                    print(f\"   Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "            \n",
    "            # Generate answer with memory context\n",
    "            print(\"\\nGenerating answer with conversation context...\")\n",
    "            answer = self.generate_final_answer(question, docs)\n",
    "        \n",
    "        # The answer is returned but not printed again\n",
    "        return answer\n",
    "    \n",
    "    def process_query(self, query: str, use_memory: bool = False, num_results: int = 5) -> str:\n",
    "        \"\"\"Main entry point that decides whether to use memory or the full pipeline.\"\"\"\n",
    "        # Check if this might be a follow-up question that should use memory\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        if use_memory or (chat_history and self._is_likely_followup(query)):\n",
    "            print(\"Using conversation memory to process this query...\")\n",
    "            return self.ask_with_memory(query, num_results)\n",
    "        else:\n",
    "            print(\"Using full query improvement pipeline...\")\n",
    "            return self.search_with_query_feedback(query, num_results)\n",
    "    \n",
    "    def _is_likely_followup(self, query: str) -> bool:\n",
    "        \"\"\"Heuristically determines if a query is likely a follow-up question.\"\"\"\n",
    "        # Look for pronouns, references, and questions that seem to refer to previous context\n",
    "        followup_indicators = [\n",
    "            \"it\", \"this\", \"that\", \"they\", \"them\", \"these\", \"those\",\n",
    "            \"previous\", \"earlier\", \"above\", \"mentioned\",\n",
    "            \"what about\", \"how about\", \"tell me more\", \"elaborate\",\n",
    "            \"why\", \"how does\", \"can you explain\"\n",
    "        ]\n",
    "        \n",
    "        query_lower = query.lower()\n",
    "        return any(indicator in query_lower for indicator in followup_indicators)\n",
    "\n",
    "# Example usage\n",
    "def main():    \n",
    "    # Initialize the research assistant\n",
    "    assistant = ResearchAssistant(llm=llm, vector_db=vector_db)\n",
    "    \n",
    "    # Main interaction loop\n",
    "    while True:\n",
    "        query = input(\"\\nEnter your query (or 'exit' to quit): \")\n",
    "        if query.lower() == 'exit':\n",
    "            break\n",
    "            \n",
    "        # Ask the user whether to use memory mode or full pipeline\n",
    "        use_memory = input(\"Use conversation memory? (y/n): \").lower() == 'y'\n",
    "        \n",
    "        # Process the query\n",
    "        answer = assistant.process_query(query, use_memory=use_memory)\n",
    "        # Only print the final answer once\n",
    "        print(\"\\nFinal Answer:\", answer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
