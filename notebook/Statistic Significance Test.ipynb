{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1ffd8c7-e88e-4eff-ae9a-cd0de8d776e5",
   "metadata": {},
   "source": [
    "## Statistic Significance for Rewrites Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad5fec3e-e0f4-4832-9b27-468e507c4428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friedman test:\n",
      "  statistic=955.5232, p-value=0.000000\n",
      "\n",
      "Pairwise Wilcoxon tests (Original vs Rewrites):\n",
      "              Comparison  Wilcoxon_Statistic         Raw_p   Corrected_p  \\\n",
      "0  Original vs Rewrite 1              9395.5  2.756789e-45  1.378394e-44   \n",
      "1  Original vs Rewrite 2              7340.0  2.518162e-50  1.259081e-49   \n",
      "2  Original vs Rewrite 3              1172.0  4.949241e-67  2.474620e-66   \n",
      "3  Original vs Rewrite 4              6638.0  4.497181e-52  2.248591e-51   \n",
      "4  Original vs Rewrite 5              2210.0  1.139091e-62  5.695456e-62   \n",
      "\n",
      "   Significant  \n",
      "0         True  \n",
      "1         True  \n",
      "2         True  \n",
      "3         True  \n",
      "4         True  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import friedmanchisquare, wilcoxon\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"Rating_rewrites.csv\")\n",
    "\n",
    "# Pivot so each row = Query ID, columns = Original/Rewrite1...Rewrite5\n",
    "pivot_df = df.pivot(index=\"Query ID\", columns=\"Query Type\", values=\"Query Score\")\n",
    "\n",
    "# Drop rows with missing values (if any queries are incomplete)\n",
    "pivot_df = pivot_df.dropna()\n",
    "\n",
    "# --- 1. Friedman Test (global test across all query types) ---\n",
    "scores = [pivot_df[col].values for col in pivot_df.columns]\n",
    "friedman_stat, friedman_p = friedmanchisquare(*scores)\n",
    "print(\"Friedman test:\")\n",
    "print(f\"  statistic={friedman_stat:.4f}, p-value={friedman_p:.6f}\")\n",
    "\n",
    "# --- 2. Pairwise Wilcoxon tests (Original vs each Rewrite) ---\n",
    "results = []\n",
    "for col in pivot_df.columns:\n",
    "    if col.lower() != \"original\":  # skip self-comparison\n",
    "        stat, p = wilcoxon(pivot_df[\"Original\"], pivot_df[col])\n",
    "        results.append((col, stat, p))\n",
    "\n",
    "# Correct for multiple testing (Bonferroni or FDR)\n",
    "cols, stats, pvals = zip(*results)\n",
    "reject, pvals_corrected, _, _ = multipletests(pvals, method=\"bonferroni\")\n",
    "\n",
    "# Save pairwise results\n",
    "pairwise_df = pd.DataFrame({\n",
    "    \"Comparison\": [f\"Original vs {c}\" for c in cols],\n",
    "    \"Wilcoxon_Statistic\": stats,\n",
    "    \"Raw_p\": pvals,\n",
    "    \"Corrected_p\": pvals_corrected,\n",
    "    \"Significant\": reject\n",
    "})\n",
    "pairwise_df.to_csv(\"query_score_significance.csv\", index=False)\n",
    "\n",
    "print(\"\\nPairwise Wilcoxon tests (Original vs Rewrites):\")\n",
    "print(pairwise_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac36e74d-53c1-46ea-a873-83257f799bba",
   "metadata": {},
   "source": [
    "# Intent Match and Answer Qualty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244b9b43-11f6-45ba-bf34-bf9db1c0ad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "Intent Match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc56f621-6471-4001-93b0-4632d041c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import friedmanchisquare, wilcoxon\n",
    "import itertools\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "\n",
    "OUTPUT_FILE = \"intent_match_significance.csv\"\n",
    "df = pd.read_csv(\"evaluated_queries.csv\")\n",
    "# Filter only rewrites\n",
    "rewrites = df[df[\"Query Type\"].str.contains(\"Rewrite\")]\n",
    "\n",
    "# Pivot so each row = query, each column = a rewrite’s intent score\n",
    "pivot = rewrites.pivot(index=\"Query ID\", columns=\"Query Type\", values=\"Intent Match\")\n",
    "\n",
    "# ---- Friedman Test ----\n",
    "stat, p = friedmanchisquare(*[pivot[col] for col in pivot.columns])\n",
    "print(f\"Friedman test: statistic={stat:.4f}, p-value={p:.6f}\")\n",
    "\n",
    "# ---- Pairwise Wilcoxon Tests ----\n",
    "comparisons = list(itertools.combinations(pivot.columns, 2))\n",
    "results = []\n",
    "for c1, c2 in comparisons:\n",
    "    stat, pval = wilcoxon(pivot[c1], pivot[c2])\n",
    "    results.append([f\"{c1} vs {c2}\", stat, pval])\n",
    "\n",
    "# Correct p-values (Holm)\n",
    "comparisons_df = pd.DataFrame(results, columns=[\"Comparison\",\"Wilcoxon_Statistic\",\"Raw_p\"])\n",
    "comparisons_df[\"Corrected_p\"] = multipletests(comparisons_df[\"Raw_p\"], method=\"holm\")[1]\n",
    "comparisons_df[\"Significant\"] = comparisons_df[\"Corrected_p\"] < 0.05\n",
    "\n",
    "print(comparisons_df)\n",
    "comparisons_df.to_csv(OUTPUT_FILE, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ef9cb-3804-4dad-a8a9-9c4415d73a0b",
   "metadata": {},
   "source": [
    "# Answer Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c2843f-6ab8-41f1-889c-514bcf91235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import friedmanchisquare, wilcoxon\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from itertools import combinations\n",
    "\n",
    "# --- Load data ---\n",
    "df = pd.read_csv(\"evaluated_queries.csv\")\n",
    "\n",
    "# Pivot so each row = Query ID, columns = Original/Rewrite1...Rewrite5\n",
    "pivot_df = df.pivot(index=\"Query ID\", columns=\"Query Type\", values=\"Answer Quality\")\n",
    "\n",
    "# Drop rows with missing values (if any queries are incomplete)\n",
    "pivot_df = pivot_df.dropna()\n",
    "\n",
    "# --- 1. Friedman Test (global test across all query types) ---\n",
    "scores = [pivot_df[col].values for col in pivot_df.columns]\n",
    "friedman_stat, friedman_p = friedmanchisquare(*scores)\n",
    "print(\"Friedman test:\")\n",
    "print(f\"  statistic={friedman_stat:.4f}, p-value={friedman_p:.6f}\")\n",
    "\n",
    "# --- 2. All Pairwise Wilcoxon tests ---\n",
    "results = []\n",
    "for c1, c2 in combinations(pivot_df.columns, 2):\n",
    "    try:\n",
    "        stat, p = wilcoxon(pivot_df[c1], pivot_df[c2])\n",
    "        results.append((f\"{c1} vs {c2}\", stat, p))\n",
    "    except ValueError:\n",
    "        # Happens if the two columns are identical (all differences = 0)\n",
    "        results.append((f\"{c1} vs {c2}\", None, 1.0))\n",
    "\n",
    "# --- Multiple testing correction (Holm is usually better than Bonferroni) ---\n",
    "comparisons, stats, pvals = zip(*results)\n",
    "reject, pvals_corrected, _, _ = multipletests(pvals, method=\"holm\")\n",
    "\n",
    "# --- Save pairwise results ---\n",
    "pairwise_df = pd.DataFrame({\n",
    "    \"Comparison\": comparisons,\n",
    "    \"Wilcoxon_Statistic\": stats,\n",
    "    \"Raw_p\": pvals,\n",
    "    \"Corrected_p\": pvals_corrected,\n",
    "    \"Significant\": reject\n",
    "})\n",
    "\n",
    "# Round for readability\n",
    "pairwise_df = pairwise_df.round(6)\n",
    "\n",
    "# Save to CSV\n",
    "pairwise_df.to_csv(\"answer_quality_significance.csv\", index=False)\n",
    "\n",
    "print(\"\\nPairwise Wilcoxon tests (all query type comparisons):\")\n",
    "print(pairwise_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f587247b-7edd-47a5-ab74-9edd7c60a473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a3eea9-78a5-42b8-8b0b-c2313f60776c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c5864c7-5b35-4147-8eb5-a2cc5376221d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting fresh evaluation.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m intent_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     49\u001b[0m answer_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m tqdm(rows_to_process\u001b[38;5;241m.\u001b[39mindex, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(rows_to_process)):\n\u001b[1;32m     52\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m build_prompt(row)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "import time\n",
    "import re\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "# Load CSV\n",
    "INPUT_FILE = 'Query_statistics.csv'\n",
    "OUTPUT_FILE = 'evaluated queries.csv'\n",
    "OLLAMA_MODEL = 'mistral'\n",
    "BATCH_SIZE = 300  # Number of rows to process at a time\n",
    "\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    df = pd.read_csv(OUTPUT_FILE)  # Resume from saved file\n",
    "    print(f\"[INFO] Resuming from {OUTPUT_FILE}\")\n",
    "else:\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    # Add new columns for final scores\n",
    "    df['Query Score'] = None\n",
    "    print(\"[INFO] Starting fresh evaluation.\")\n",
    "\n",
    "# Process only rows where Query Type != \"Original\" and not yet scored\n",
    "rows_to_process = df[(df['Query Score'].isna())].head(BATCH_SIZE)\n",
    "\n",
    "if rows_to_process.empty:\n",
    "    print(\"[INFO] No more rows to process.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Define evaluation prompt templates\n",
    "def build_prompt(row):\n",
    "    return f\"\"\"\n",
    "You are evaluating a refined query and its answer.\n",
    "\n",
    "Original User Query: {row['Original Query']}\n",
    "Refined Query: {row['Refined Query']}\n",
    "LLM Answer: {row['LLM Answer']}\n",
    "\n",
    "1. Rate how well the **refined query** matches the intent of the original query (from 1 to 5).\n",
    "2. Rate how well the **LLM Answer** answers the intent of the user (from 1 to 5).\n",
    "\n",
    "Return your result in JSON format like:\n",
    "{{\"intent_match\": x, \"answer_quality\": y}}\n",
    "    \"\"\".strip()\n",
    "\n",
    "# Store results\n",
    "intent_scores = []\n",
    "answer_scores = []\n",
    "\n",
    "for idx in tqdm(rows_to_process.index, total=len(rows_to_process)):\n",
    "    prompt = build_prompt(row)\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=OLLAMA_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an honest and concise evaluator. Return only JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        content = response[\"message\"][\"content\"].strip()\n",
    "        print(f\"\\nRow {idx} response:\\n{content}\\n\")\n",
    "\n",
    "        # Extract numbers using regex\n",
    "        intent_match = re.search(r'\"intent_match\"\\s*:\\s*(\\d)', content)\n",
    "        answer_quality = re.search(r'\"answer_quality\"\\s*:\\s*(\\d)', content)\n",
    "\n",
    "        # Save directly into dataframe\n",
    "        df.loc[idx, \"Intent Match (1-5)\"] = float(intent_match.group(1)) if intent_match else None\n",
    "        df.loc[idx, \"Answer Quality (1-5)\"] = float(answer_quality.group(1)) if answer_quality else None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Row {idx}: {e}\")\n",
    "        df.loc[idx, \"Intent Match (1-5)\"] = None\n",
    "        df.loc[idx, \"Answer Quality (1-5)\"] = None\n",
    "\n",
    "    time.sleep(DELAY)\n",
    "\n",
    "# ==========================\n",
    "# SAVE PROGRESS\n",
    "# ==========================\n",
    "df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"[INFO] Evaluation complete. Progress saved to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69a56a61-4be1-4604-a147-f4ff45c67098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d29091c-a59a-402d-9414-c0ec5b74a70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0 – 299\n",
      "Processing batch 300 – 599\n",
      "Processing batch 600 – 899\n",
      "Processing batch 900 – 1199\n",
      "Processing batch 1200 – 1499\n",
      "Processing batch 1500 – 1799\n",
      "Processing batch 1800 – 2099\n",
      "Processing batch 2100 – 2399\n",
      "Processing batch 2400 – 2699\n",
      "Processing batch 2700 – 2999\n",
      "Processing batch 3000 – 3299\n",
      "Processing batch 3300 – 3599\n",
      "Processing batch 3600 – 3899\n",
      "Processing batch 3900 – 4199\n",
      "Processing batch 4200 – 4499\n",
      "Processing batch 4500 – 4799\n",
      "Processing batch 4800 – 5099\n",
      "Processing batch 5100 – 5399\n",
      "Processing batch 5400 – 5699\n",
      "Processing batch 5700 – 5999\n",
      "Processing batch 6000 – 6035\n",
      "✅ Evaluation complete. Saved as evaluated_queries.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "import json\n",
    "import time\n",
    "\n",
    "# ========================\n",
    "# CONFIGURATION\n",
    "# ========================\n",
    "INPUT_FILE = \"Query_statistics.csv\"\n",
    "OUTPUT_FILE = \"evaluated_queries.csv\"\n",
    "EVALUATOR_MODEL = \"mistral\"   # <- use different model from generator\n",
    "BATCH_SIZE = 300\n",
    "DELAY = 2  # seconds between batches, to avoid overloading\n",
    "\n",
    "# ========================\n",
    "# PROMPT TEMPLATE\n",
    "# ========================\n",
    "def build_batch_prompt(batch):\n",
    "    \"\"\"Builds a single evaluation prompt for multiple queries in one batch\"\"\"\n",
    "    prompt = \"\"\"\n",
    "You are an impartial evaluator. \n",
    "Rate refined queries and answers according to the following scale:\n",
    "\n",
    "- 1 = very poor\n",
    "- 2 = poor\n",
    "- 3 = fair\n",
    "- 4 = good\n",
    "- 5 = excellent\n",
    "\n",
    "For each case, provide two ratings:\n",
    "1. intent_match → how well the refined query matches the intent of the original\n",
    "2. answer_quality → how well the LLM answer addresses the user’s intent\n",
    "\n",
    "⚠️ Return ONLY valid JSON (list of objects). \n",
    "Do not add explanations, comments, or text outside JSON.\n",
    "\"\"\".strip()\n",
    "\n",
    "    cases = []\n",
    "    for idx, row in batch.iterrows():\n",
    "        cases.append({\n",
    "            \"id\": int(idx),\n",
    "            \"original_query\": row[\"Original Query\"],\n",
    "            \"refined_query\": row[\"Refined Query\"],\n",
    "            \"llm_answer\": row[\"LLM Answer\"]\n",
    "        })\n",
    "\n",
    "    prompt += f\"\\n\\nEvaluate these cases:\\n{json.dumps(cases, indent=2)}\\n\\n\"\n",
    "    prompt += \"\"\"Return your answer as a JSON list, where each element corresponds to a case:\n",
    "[\n",
    "  {\"id\": <row_id>, \"intent_match\": <1-5>, \"answer_quality\": <1-5>},\n",
    "  ...\n",
    "]\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# ========================\n",
    "# MAIN LOOP\n",
    "# ========================\n",
    "def main():\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    results = []\n",
    "\n",
    "    for start in range(0, len(df), BATCH_SIZE):\n",
    "        batch = df.iloc[start:start + BATCH_SIZE]\n",
    "        print(f\"Processing batch {start} – {start + len(batch) - 1}\")\n",
    "\n",
    "        prompt = build_batch_prompt(batch)\n",
    "\n",
    "        try:\n",
    "            response = ollama.chat(\n",
    "                model=EVALUATOR_MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a strict JSON-only evaluator.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            content = response[\"message\"][\"content\"].strip()\n",
    "            # Try parsing JSON safely\n",
    "            parsed = None\n",
    "            try:\n",
    "                parsed = json.loads(content)\n",
    "            except json.JSONDecodeError:\n",
    "                # Sometimes models wrap output in ```json ... ```\n",
    "                if \"```\" in content:\n",
    "                    cleaned = content.split(\"```\")[1]\n",
    "                    cleaned = cleaned.replace(\"json\", \"\").strip()\n",
    "                    parsed = json.loads(cleaned)\n",
    "\n",
    "            if parsed is None or not isinstance(parsed, list):\n",
    "                raise ValueError(\"Evaluator did not return a JSON list\")\n",
    "\n",
    "            results.extend(parsed)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed batch {start}: {e}\")\n",
    "            # Fill missing rows with None\n",
    "            for idx in batch.index:\n",
    "                results.append({\"id\": int(idx), \"intent_match\": None, \"answer_quality\": None})\n",
    "\n",
    "        time.sleep(DELAY)\n",
    "\n",
    "    # Merge results back into dataframe\n",
    "    res_df = pd.DataFrame(results).set_index(\"id\")\n",
    "    df = df.join(res_df, how=\"left\")\n",
    "    df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "    print(f\"✅ Evaluation complete. Saved as {OUTPUT_FILE}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8917cc4e-b122-49a4-b3db-df817bb2e3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
