{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c10830-c347-4913-a24e-885f6c401f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "717e669d-fc8e-43b5-829b-1e3084dd4c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86f06ed714a455f9e1ef1494e331e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Golden Questions:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'assistant' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m golden_dataset \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m tqdm(chunk_sample, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating Golden Questions\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 35\u001b[0m     question \u001b[38;5;241m=\u001b[39m generate_question_from_chunk(assistant, chunk\u001b[38;5;241m.\u001b[39mpage_content)\n\u001b[1;32m     36\u001b[0m     golden_dataset\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_question\u001b[39m\u001b[38;5;124m\"\u001b[39m: question,\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_chunk_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk\u001b[38;5;241m.\u001b[39mpage_content,\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk\u001b[38;5;241m.\u001b[39mmetadata\n\u001b[1;32m     40\u001b[0m     })\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Save your new evaluation dataset\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'assistant' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from sagerag import ResearchAssistant\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def generate_question_from_chunk(assistant, chunk_text):\n",
    "    \"\"\"Uses an LLM to generate a question that the chunk_text can answer.\"\"\"\n",
    "    \n",
    "    question_generation_prompt = PromptTemplate.from_template(\n",
    "        \"You are an expert researcher. Given the following paragraph from a scientific paper, \"\n",
    "        \"generate one clear, specific question that this text could perfectly answer. \"\n",
    "        \"Do not ask a question that is too broad. Focus only on the information present in the text. \"\n",
    "        \"Provide only the question and nothing else.\\n\\n\"\n",
    "        \"Text: \\\"{context}\\\"\\n\\n\"\n",
    "        \"Question:\"\n",
    "    )\n",
    "    \n",
    "    chain = question_generation_prompt | assistant.llm | StrOutputParser()\n",
    "    question = chain.invoke({\"context\": chunk_text}).strip()\n",
    "    return question\n",
    "\n",
    "# --- Example Usage in your notebook ---\n",
    "\n",
    "# Load the chunks you created earlier\n",
    "with open(\"../Database/processed_chunks.pkl\", \"rb\") as f:\n",
    "    all_chunks = pickle.load(f)\n",
    "\n",
    "# Create a sample of 200 chunks\n",
    "import random\n",
    "chunk_sample = random.sample(all_chunks, 200)\n",
    "\n",
    "golden_dataset = []\n",
    "for chunk in tqdm(chunk_sample, desc=\"Generating Golden Questions\"):\n",
    "    question = generate_question_from_chunk(assistant, chunk.page_content)\n",
    "    golden_dataset.append({\n",
    "        \"generated_question\": question,\n",
    "        \"source_chunk_text\": chunk.page_content,\n",
    "        \"source_metadata\": chunk.metadata\n",
    "    })\n",
    "\n",
    "# Save your new evaluation dataset\n",
    "df_golden = pd.DataFrame(golden_dataset)\n",
    "df_golden.to_csv(\"golden_evaluation_questions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3facb8d9-62a2-4efa-aa73-1b4075429f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
