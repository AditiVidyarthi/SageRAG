{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73c5da5b-056f-49dc-8464-5cfbd1492c5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement langchain_text_splitter (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain_text_splitter\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langchain_text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dfd335c-afae-415b-8f32-076d37fd468a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement langchain_text_splitter (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain_text_splitter\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langchain_text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b354f4-0b65-4e2e-8c7e-ad6f57c0271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"false\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"ResearchPro2\" \n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = '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'\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8f6906f-0a67-4bde-ac97-8d6e3cfeede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import necessary packages\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import BaseOutputParser, StrOutputParser\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "from typing import List, Literal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67465538-9a43-4952-9d20-79b8be8800d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5n/8m4t00412h97v592vh5y0pgc0000gn/T/ipykernel_58881/3112741827.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "/var/folders/5n/8m4t00412h97v592vh5y0pgc0000gn/T/ipykernel_58881/3112741827.py:6: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_db = Chroma(persist_directory=DB_DIR, embedding_function=embedding_model)\n"
     ]
    }
   ],
   "source": [
    "PDF_DIR = \"../../arxiv_data\"  # Go up two levels, then into arxiv_data\n",
    "DB_DIR = \"../../arxiv_vector1_db\"  # Go up two levels, then into arxiv_vector1_db\n",
    "\n",
    "# Embedding & Vector DB\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_db = Chroma(persist_directory=DB_DIR, embedding_function=embedding_model)\n",
    "\n",
    "# Load embedding model once for scoring\n",
    "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# LLM\n",
    "llm = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "# print(os.getcwd())\n",
    "\n",
    "# import os\n",
    "\n",
    "# # Get current working directory\n",
    "# cwd = os.getcwd()\n",
    "\n",
    "# # Move up two levels\n",
    "# target_path = os.path.abspath(os.path.join(cwd, \"../../arxiv_vector1_db\"))\n",
    "\n",
    "# print(\"Current Directory:\", cwd)\n",
    "# print(\"Resolved ../../arxiv_vector1_db :\", target_path)\n",
    "# # List files/directories at that location\n",
    "# files = os.listdir(target_path)\n",
    "\n",
    "# print(f\"Contents of {target_path}:\")\n",
    "# for f in files:\n",
    "#     print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9309a983-5c6d-4777-8d85-5b325502e401",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your query (or 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, BaseOutputParser\n",
    "from langchain.schema import Document, HumanMessage, AIMessage\n",
    "from langchain.vectorstores import VectorStore\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms.base import BaseLLM\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Output Parser\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        lines = re.findall(r\"^\\d+\\.\\s+(.*)\", text, re.MULTILINE)\n",
    "        return [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "class ResearchAssistant:\n",
    "    def __init__(self, llm: BaseLLM, vector_db: VectorStore, embeddings_model: Optional[SentenceTransformer] = None):\n",
    "        \"\"\"Initialize the Research Assistant with LLM and vector database.\"\"\"\n",
    "        self.llm = llm\n",
    "        self.vector_db = vector_db\n",
    "        \n",
    "        # Initialize sentence transformer model for semantic similarity if not provided\n",
    "        if embeddings_model is None:\n",
    "            self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        else:\n",
    "            self.semantic_model = embeddings_model\n",
    "            \n",
    "        # Initialize memory - we'll manage it manually instead of using ConversationalRetrievalChain\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        # Create a retriever for direct use\n",
    "        self.retriever = vector_db.as_retriever()\n",
    "    \n",
    "    def rate_query(self, query: str) -> Dict[str, str | int]:\n",
    "        \"\"\"Rates the query and gives an explanation.\"\"\"\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an intelligent assistant trained to evaluate search queries.\n",
    "        Given the following query: \"{query}\"\n",
    "        1. Rate the query on a scale of 1 (very poor) to 5 (excellent) based on:\n",
    "           - Clarity\n",
    "           - Specificity\n",
    "           - Relevance\n",
    "           - Retrievability\n",
    "        2. Provide a short explanation for the rating (if it is vague, incomplete, or inefficient).\n",
    "        Respond in JSON format:\n",
    "        {{\n",
    "          \"rating\": 3,\n",
    "          \"explanation\": \"Too vague, lacks keywords.\"\n",
    "        }}\n",
    "        \"\"\")\n",
    "        parser = JsonOutputParser()\n",
    "        chain: Runnable = prompt | self.llm | parser\n",
    "        return chain.invoke({\"query\": query})\n",
    "    \n",
    "    def suggest_rewrites(self, query: str) -> List[str]:\n",
    "        \"\"\"Returns 5 rephrased versions of the query.\"\"\"\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\"],\n",
    "            template=\"\"\"You are an AI assistant. Rephrase the question in 5 different ways to improve retrieval from a document store. Number each version starting with 1.\\nQuestion: {question}\"\"\"\n",
    "        )\n",
    "        return (prompt | self.llm | LineListOutputParser()).invoke({\"question\": query})\n",
    "    \n",
    "    def compute_confidence(self, original: str, rewritten: str) -> float:\n",
    "        \"\"\"Computes semantic similarity between original and rewritten queries.\"\"\"\n",
    "        vec_orig = self.semantic_model.encode(original, convert_to_tensor=True)\n",
    "        vec_rewrite = self.semantic_model.encode(rewritten, convert_to_tensor=True)\n",
    "        return util.pytorch_cos_sim(vec_orig, vec_rewrite).item()\n",
    "    \n",
    "    def score_queries(self, original: str, queries: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Scores and sorts rephrased queries by confidence.\"\"\"\n",
    "        scored = [(q, self.compute_confidence(original, q)) for q in queries]\n",
    "        scored.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scored\n",
    "    \n",
    "    def present_query_options(self, original: str, queries: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Presents the original and rewritten queries with confidence scores.\"\"\"\n",
    "        # Add original query at the top\n",
    "        scored_queries = [(\"Original: \" + original, 1.0)]\n",
    "        \n",
    "        # Add scored rewritten queries\n",
    "        rewritten_scores = self.score_queries(original, queries)\n",
    "        for i, (query, score) in enumerate(rewritten_scores, 1):\n",
    "            scored_queries.append((f\"Rewrite {i}: {query}\", score))\n",
    "            \n",
    "        # Print options for user\n",
    "        print(\"Available search queries (with confidence scores):\")\n",
    "        for i, (query, score) in enumerate(scored_queries):\n",
    "            print(f\"{i}. {query} [Confidence: {score:.2f}]\")\n",
    "            \n",
    "        return scored_queries\n",
    "    \n",
    "    def retrieve_documents(self, queries: List[str], k: int = 5) -> Tuple[List[Document], Dict[str, List[Document]]]:\n",
    "        \"\"\"\n",
    "        Retrieves documents using multiple queries, preserving which query returned which documents.\n",
    "        Shows relevance scores for better transparency.\n",
    "        \"\"\"\n",
    "        all_docs = []\n",
    "        unique_docs = {}\n",
    "        query_docs_map = {}\n",
    "        \n",
    "        # Retrieve docs for each query\n",
    "        for query in queries:\n",
    "            print(f\"\\nQuery: '{query}'\")\n",
    "            results = self.vector_db.similarity_search_with_relevance_scores(query, k=k)\n",
    "            docs = []\n",
    "            \n",
    "            for i, (doc, score) in enumerate(results, 1):\n",
    "                print(f\"--- Result {i} ---\")\n",
    "                print(f\"Score: {score:.4f}\")\n",
    "                print(f\"Chunk:\\n{doc.page_content[:150]}...\")\n",
    "                if hasattr(doc, 'metadata') and doc.metadata:\n",
    "                    source = doc.metadata.get('source', 'Unknown')\n",
    "                    print(f\"Source: {source}\")\n",
    "                print()\n",
    "                \n",
    "                docs.append(doc)\n",
    "                \n",
    "                # Add to our overall collection with deduplication\n",
    "                if doc.page_content not in unique_docs:\n",
    "                    unique_docs[doc.page_content] = doc\n",
    "                    all_docs.append(doc)\n",
    "            \n",
    "            query_docs_map[query] = docs\n",
    "        \n",
    "        print(f\"\\nTotal unique documents: {len(all_docs)}\")\n",
    "        return all_docs, query_docs_map\n",
    "\n",
    "    def rerank_docs(self, query: str, docs: List[Document]) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"Reranks documents based on semantic similarity to the query.\"\"\"\n",
    "        query_embedding = self.semantic_model.encode(query, convert_to_tensor=True)\n",
    "        reranked = []\n",
    "        for doc in docs:\n",
    "            doc_embedding = self.semantic_model.encode(doc.page_content, convert_to_tensor=True)\n",
    "            score = util.pytorch_cos_sim(query_embedding, doc_embedding).item()\n",
    "            reranked.append((doc, score))\n",
    "        return sorted(reranked, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    def can_answer_without_retrieval(self, question: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Determines if a question can be answered directly from memory without retrieval.\n",
    "        Returns a tuple: (can_answer, answer_if_available)\n",
    "        \"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        if not chat_history:\n",
    "            return False, None\n",
    "            \n",
    "        # Create the prompt to check if we can answer directly from the conversation\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an AI assistant helping with a conversation.\n",
    "        Given the following conversation history and a new question, determine if the question:\n",
    "        1. Can be answered directly based ONLY on the conversation history (like \"what was my previous question?\")\n",
    "        2. Does NOT require retrieving new information from documents\n",
    "        \n",
    "        If both conditions are true, provide the answer. Otherwise, respond with \"NEEDS_RETRIEVAL\".\n",
    "        \n",
    "        Conversation History:\n",
    "        {chat_history}\n",
    "        \n",
    "        New Question: {question}\n",
    "        \n",
    "        Your assessment (answer directly or respond with \"NEEDS_RETRIEVAL\"):\n",
    "        \"\"\")\n",
    "        \n",
    "        # Format the chat history for context\n",
    "        history_str = \"\"\n",
    "        for message in chat_history:\n",
    "            if hasattr(message, \"content\"):\n",
    "                role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                history_str += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        # Ask the LLM if this can be answered without retrieval\n",
    "        response = self.llm.invoke(\n",
    "            prompt.format(chat_history=history_str, question=question)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # If the response is \"NEEDS_RETRIEVAL\", we need to use retrieval\n",
    "        if \"NEEDS_RETRIEVAL\" in response:\n",
    "            return False, None\n",
    "        else:\n",
    "            return True, response\n",
    "    \n",
    "    def generate_final_answer(self, question: str, docs: List[Document], max_docs: int = 5) -> str:\n",
    "        \"\"\"Generates the final answer from retrieved documents.\"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history:\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "            \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful research assistant. \n",
    "            Use the following retrieved context chunks to answer the user's question thoroughly.\n",
    "            If the information isn't in the context, indicate what's missing rather than making up information.\n",
    "            If the context contains conflicting or uncertain information, highlight the disagreement. \n",
    "            Do not fabricate any facts not grounded in the provided context.\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        context = \"\\n\\n---\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" \n",
    "                                     for i, doc in enumerate(docs[:max_docs])])\n",
    "        \n",
    "        answer = (prompt | self.llm).invoke({\n",
    "            \"question\": question, \n",
    "            \"context\": context,\n",
    "            \"chat_history\": chat_context\n",
    "        })\n",
    "        \n",
    "        # Save the QA pair to memory manually\n",
    "        self.memory.chat_memory.add_messages([\n",
    "            HumanMessage(content=question),\n",
    "            AIMessage(content=answer)\n",
    "        ])\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def generate_combined_answer(self, question: str, query_results: Dict[str, List[Document]], max_docs_per_query: int = 3) -> str:\n",
    "        \"\"\"Generates a combined answer from multiple query results.\"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history:\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        # Combine all relevant documents from different queries\n",
    "        all_context_sections = []\n",
    "        for query, docs in query_results.items():\n",
    "            # Use only the top N documents for each query\n",
    "            docs_for_query = docs[:max_docs_per_query]\n",
    "            if docs_for_query:\n",
    "                all_context_sections.append(f\"Results for query: '{query}'\")\n",
    "                for i, doc in enumerate(docs_for_query, 1):\n",
    "                    all_context_sections.append(f\"Document {i}:\\n{doc.page_content}\")\n",
    "        \n",
    "        # Join all context sections\n",
    "        combined_context = \"\\n\\n---\\n\\n\".join(all_context_sections)\n",
    "        \n",
    "        # Create a prompt that emphasizes synthesizing information across different query results\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful research assistant. \n",
    "            The user's question has been reformulated in several ways, and each formulation returned different documents.\n",
    "            Use the following retrieved context chunks from ALL query variations to comprehensively answer the user's question.\n",
    "            Synthesize information across all retrieved documents to provide the most complete answer possible.\n",
    "            If the information isn't in the context, indicate what's missing rather than making up information.\n",
    "            If the context contains conflicting information from different query results, highlight the disagreement.\n",
    "            Do not fabricate any facts not grounded in the provided context.\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Context from multiple query formulations:\n",
    "            {context}\n",
    "            \n",
    "            Original Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        answer = (prompt | self.llm).invoke({\n",
    "            \"question\": question, \n",
    "            \"context\": combined_context,\n",
    "            \"chat_history\": chat_context\n",
    "        })\n",
    "        \n",
    "        # Save the QA pair to memory manually\n",
    "        self.memory.chat_memory.add_messages([\n",
    "            HumanMessage(content=question),\n",
    "            AIMessage(content=answer)\n",
    "        ])\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def search_with_query_feedback(self, query: str, num_results: int = 5) -> str:\n",
    "        \"\"\"Main pipeline that processes a query and returns an answer.\"\"\"\n",
    "        # Step 1: Rate the original query\n",
    "        print(\"Evaluating your query...\")\n",
    "        rating_result = self.rate_query(query)\n",
    "        print(f\"Query Rating: {rating_result['rating']}/5\")\n",
    "        print(f\"Explanation: {rating_result['explanation']}\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 2: Suggest rewrites if the rating is less than perfect\n",
    "        rewritten_queries = []\n",
    "        if rating_result[\"rating\"] < 5:\n",
    "            print(\"Generating improved query variations...\")\n",
    "            rewritten_queries = self.suggest_rewrites(query)\n",
    "            print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 3: Present options to the user\n",
    "        query_options = self.present_query_options(query, rewritten_queries)\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 4: Get user selection\n",
    "        selected_indices = input(\"Enter the numbers of the queries you want to use (comma-separated, e.g., '0,2,3'): \")\n",
    "        try:\n",
    "            selected_indices = [int(idx.strip()) for idx in selected_indices.split(\",\")]\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter numbers separated by commas.\")\n",
    "            return \"Invalid query selection. Please try again.\"\n",
    "        \n",
    "        # Get the selected queries (without the prefix and score)\n",
    "        selected_queries = []\n",
    "        for idx in selected_indices:\n",
    "            if idx == 0:  # Original query\n",
    "                selected_queries.append(query)\n",
    "            else:  # Rewritten query\n",
    "                # Check if the index is valid\n",
    "                if idx <= len(rewritten_queries):\n",
    "                    query_text = rewritten_queries[idx-1]\n",
    "                    selected_queries.append(query_text)\n",
    "        \n",
    "        if not selected_queries:\n",
    "            print(\"No valid queries selected.\")\n",
    "            return \"No valid queries were selected. Please try again.\"\n",
    "            \n",
    "        print(f\"\\nSelected {len(selected_queries)} queries for retrieval.\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 5: Retrieve documents\n",
    "        print(\"Retrieving relevant documents...\")\n",
    "        docs, query_docs_map = self.retrieve_documents(selected_queries, k=num_results)\n",
    "        \n",
    "        # Step 6: Generate answer\n",
    "        # If using multiple queries, use the combined answer approach\n",
    "        if len(selected_queries) > 1:\n",
    "            print(\"\\nGenerating combined answer based on multiple query results...\")\n",
    "            final_answer = self.generate_combined_answer(query, query_docs_map, max_docs_per_query=3)\n",
    "        else:\n",
    "            # For single query, rerank and use the traditional approach\n",
    "            print(\"\\nReranking documents based on relevance to the original query...\")\n",
    "            reranked_docs_with_scores = self.rerank_docs(query, docs)\n",
    "    \n",
    "            # Apply relevance threshold\n",
    "            relevance_threshold = 0.6  # adjust as needed\n",
    "            filtered_docs_with_scores = [\n",
    "                (doc, score) for doc, score in reranked_docs_with_scores if score >= relevance_threshold\n",
    "            ]\n",
    "            \n",
    "            # If no documents meet the threshold, use all documents\n",
    "            if not filtered_docs_with_scores:\n",
    "                print(\"No documents met the relevance threshold. Using all retrieved documents.\")\n",
    "                filtered_docs_with_scores = reranked_docs_with_scores\n",
    "            \n",
    "            # Sort top N\n",
    "            filtered_docs_with_scores = sorted(filtered_docs_with_scores, key=lambda x: x[1], reverse=True)\n",
    "            filtered_docs = [doc for doc, _ in filtered_docs_with_scores]\n",
    "            \n",
    "            # Display top reranked documents\n",
    "            print(\"\\nTop reranked documents:\")\n",
    "            for i, (doc, score) in enumerate(filtered_docs_with_scores[:num_results], 1):\n",
    "                print(f\"{i}. Score: {score:.4f}\")\n",
    "                print(f\"   Preview: {doc.page_content[:150]}...\")\n",
    "                if hasattr(doc, 'metadata') and doc.metadata:\n",
    "                    print(f\"   Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "                    \n",
    "            # Generate the final answer\n",
    "            print(\"\\nGenerating your answer based on retrieved documents...\")\n",
    "            final_answer = self.generate_final_answer(query, filtered_docs[:num_results])\n",
    "        \n",
    "        return final_answer\n",
    "    \n",
    "    def ask_with_memory(self, question: str, num_results: int = 5) -> str:\n",
    "        \"\"\"\n",
    "        Uses conversation memory to process follow-up questions.\n",
    "        Now includes query improvement and document retrieval.\n",
    "        \"\"\"\n",
    "        # First, check if this is a question we can answer directly from memory\n",
    "        can_direct_answer, direct_answer = self.can_answer_without_retrieval(question)\n",
    "        if can_direct_answer:\n",
    "            print(\"Question can be answered directly from conversation history...\")\n",
    "            # Save the QA pair to memory manually\n",
    "            self.memory.chat_memory.add_messages([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=direct_answer)\n",
    "            ])\n",
    "            return direct_answer\n",
    "        \n",
    "        # Otherwise, use the full query improvement pipeline\n",
    "        # Step 1: Rate the query\n",
    "        print(\"Evaluating your query (with memory context)...\")\n",
    "        rating_result = self.rate_query(question)\n",
    "        print(f\"Query Rating: {rating_result['rating']}/5\")\n",
    "        print(f\"Explanation: {rating_result['explanation']}\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 2: Suggest rewrites if the rating is less than perfect\n",
    "        rewritten_queries = []\n",
    "        if rating_result[\"rating\"] < 5:\n",
    "            print(\"Generating improved query variations...\")\n",
    "            rewritten_queries = self.suggest_rewrites(question)\n",
    "            print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 3: Present options to the user\n",
    "        query_options = self.present_query_options(question, rewritten_queries)\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 4: Get user selection\n",
    "        selected_indices = input(\"Enter the numbers of the queries you want to use (comma-separated, e.g., '0,2,3'): \")\n",
    "        try:\n",
    "            selected_indices = [int(idx.strip()) for idx in selected_indices.split(\",\")]\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter numbers separated by commas.\")\n",
    "            return \"Invalid query selection. Please try again.\"\n",
    "        \n",
    "        # Get the selected queries\n",
    "        selected_queries = []\n",
    "        for idx in selected_indices:\n",
    "            if idx == 0:  # Original query\n",
    "                selected_queries.append(question)\n",
    "            else:  # Rewritten query\n",
    "                # Check if the index is valid\n",
    "                if idx <= len(rewritten_queries):\n",
    "                    query_text = rewritten_queries[idx-1]\n",
    "                    selected_queries.append(query_text)\n",
    "        \n",
    "        if not selected_queries:\n",
    "            print(\"No valid queries selected.\")\n",
    "            return \"No valid queries were selected. Please try again.\"\n",
    "            \n",
    "        print(f\"\\nSelected {len(selected_queries)} queries for retrieval.\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 5: Retrieve documents\n",
    "        print(\"Retrieving relevant documents...\")\n",
    "        if len(selected_queries) > 1:\n",
    "            # If multiple queries were selected, use the multi-query approach\n",
    "            docs, query_docs_map = self.retrieve_documents(selected_queries, k=num_results)\n",
    "            \n",
    "            # Generate a combined answer\n",
    "            print(\"\\nGenerating combined answer with conversation context...\")\n",
    "            answer = self.generate_combined_answer(question, query_docs_map)\n",
    "        else:\n",
    "            # If only one query was selected, use the standard approach\n",
    "            docs = self.retriever.get_relevant_documents(selected_queries[0])\n",
    "            \n",
    "            print(f\"Retrieved {len(docs)} documents\")\n",
    "            \n",
    "            # Show previews of the retrieved documents\n",
    "            print(\"\\nTop retrieved documents:\")\n",
    "            for i, doc in enumerate(docs[:5], 1):\n",
    "                print(f\"{i}. Preview: {doc.page_content[:150]}...\")\n",
    "                if hasattr(doc, 'metadata') and doc.metadata:\n",
    "                    print(f\"   Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "            \n",
    "            # Generate answer with memory context\n",
    "            print(\"\\nGenerating answer with conversation context...\")\n",
    "            answer = self.generate_final_answer(question, docs)\n",
    "        \n",
    "        # The answer is returned but not printed again\n",
    "        return answer\n",
    "    \n",
    "    def process_query(self, query: str, use_memory: bool = False, num_results: int = 5) -> str:\n",
    "        \"\"\"Main entry point that decides whether to use memory or the full pipeline.\"\"\"\n",
    "        # Check if this might be a follow-up question that should use memory\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        if use_memory or (chat_history and self._is_likely_followup(query)):\n",
    "            print(\"Using conversation memory to process this query...\")\n",
    "            return self.ask_with_memory(query, num_results)\n",
    "        else:\n",
    "            print(\"Using full query improvement pipeline...\")\n",
    "            return self.search_with_query_feedback(query, num_results)\n",
    "    \n",
    "    def _is_likely_followup(self, query: str) -> bool:\n",
    "        \"\"\"Heuristically determines if a query is likely a follow-up question.\"\"\"\n",
    "        # Look for pronouns, references, and questions that seem to refer to previous context\n",
    "        followup_indicators = [\n",
    "            \"it\", \"this\", \"that\", \"they\", \"them\", \"these\", \"those\",\n",
    "            \"previous\", \"earlier\", \"above\", \"mentioned\",\n",
    "            \"what about\", \"how about\", \"tell me more\", \"elaborate\",\n",
    "            \"why\", \"how does\", \"can you explain\"\n",
    "        ]\n",
    "        \n",
    "        query_lower = query.lower()\n",
    "        return any(indicator in query_lower for indicator in followup_indicators)\n",
    "\n",
    "# Example usage\n",
    "def main():    \n",
    "    # Initialize the research assistant\n",
    "    assistant = ResearchAssistant(llm=llm, vector_db=vector_db)\n",
    "    \n",
    "    # Main interaction loop\n",
    "    while True:\n",
    "        query = input(\"\\nEnter your query (or 'exit' to quit): \")\n",
    "        if query.lower() == 'exit':\n",
    "            break\n",
    "            \n",
    "        # Ask the user whether to use memory mode or full pipeline\n",
    "        use_memory = input(\"Use conversation memory? (y/n): \").lower() == 'y'\n",
    "        \n",
    "        # Process the query\n",
    "        answer = assistant.process_query(query, use_memory=use_memory)\n",
    "        # Only print the final answer once\n",
    "        print(\"\\nFinal Answer:\", answer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d45fdf5-fe26-4291-9586-11f246b87e65",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your query (or 'exit' to quit):  What is operating system ?\n",
      "Use conversation memory? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using conversation memory to process this query...\n",
      "Evaluating your query (with memory context)...\n",
      "Query Rating: 4/5\n",
      "Explanation: The query is clear and concise. It directly asks about the definition of an operating system, which can be easily understood by most users. However, it could be more specific to improve retrievability. Adding relevant keywords like 'computer software' or 'system management' would increase the chances of getting precise results.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Generating improved query variations...\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Available search queries (with confidence scores):\n",
      "0. Original: What is operating system ? [Confidence: 1.00]\n",
      "1. Rewrite 1: What category of computer systems manages input/output operations, processes instructions, and interacts with users and other devices? [Confidence: 0.58]\n",
      "2. Rewrite 2: What category of program manages computer hardware components, allocates system resources, and facilitates communication between hardware devices and software applications? [Confidence: 0.50]\n",
      "3. Rewrite 3: What type of software controls hardware components and provides a platform for running applications? [Confidence: 0.46]\n",
      "4. Rewrite 4: Which fundamental component of computing systems determines how hardware resources such as memory, storage, and peripherals are allocated to various applications? [Confidence: 0.46]\n",
      "5. Rewrite 5: What classification of software dictates the behavior of a computer's central processing unit (CPU), provides an interface for user interaction, and coordinates system operations? [Confidence: 0.45]\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the numbers of the queries you want to use (comma-separated, e.g., '0,2,3'):  2,3,4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected 3 queries for retrieval.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieving relevant documents...\n",
      "\n",
      "Query: 'What category of computer systems manages input/output operations, processes instructions, and interacts with users and other devices?'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected where operator to be one of $gt, $gte, $lt, $lte, $ne, $eq, $in, $nin, got chunk_type in query.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/chromadb/api/models/CollectionCommon.py:90\u001b[0m, in \u001b[0;36mvalidation_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/chromadb/api/models/CollectionCommon.py:294\u001b[0m, in \u001b[0;36mCollectionCommon._validate_and_prepare_query_request\u001b[0;34m(self, query_embeddings, query_texts, query_images, query_uris, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    293\u001b[0m validate_base_record_set(record_set\u001b[38;5;241m=\u001b[39mquery_records)\n\u001b[0;32m--> 294\u001b[0m validate_filter_set(filter_set\u001b[38;5;241m=\u001b[39mfilters)\n\u001b[1;32m    295\u001b[0m validate_include(include\u001b[38;5;241m=\u001b[39minclude)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/chromadb/api/types.py:345\u001b[0m, in \u001b[0;36mvalidate_filter_set\u001b[0;34m(filter_set)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filter_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m     validate_where(filter_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filter_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere_document\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/chromadb/api/types.py:659\u001b[0m, in \u001b[0;36mvalidate_where\u001b[0;34m(where)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m operator \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$gt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$gte\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$nin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    658\u001b[0m ]:\n\u001b[0;32m--> 659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    660\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected where operator to be one of $gt, $gte, $lt, $lte, $ne, $eq, $in, $nin, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperator\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    662\u001b[0m     )\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(operand, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n",
      "\u001b[0;31mValueError\u001b[0m: Expected where operator to be one of $gt, $gte, $lt, $lte, $ne, $eq, $in, $nin, got chunk_type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m use_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse conversation memory? (y/n): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Process the query\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m answer \u001b[38;5;241m=\u001b[39m assistant\u001b[38;5;241m.\u001b[39mprocess_query(query, use_memory\u001b[38;5;241m=\u001b[39muse_memory)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal Answer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, answer)\n",
      "Cell \u001b[0;32mIn[18], line 507\u001b[0m, in \u001b[0;36mResearchAssistant.process_query\u001b[0;34m(self, query, use_memory, num_results)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_memory \u001b[38;5;129;01mor\u001b[39;00m (chat_history \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_likely_followup(query)):\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing conversation memory to process this query...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_with_memory(query, num_results)\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing full query improvement pipeline...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 475\u001b[0m, in \u001b[0;36mResearchAssistant.ask_with_memory\u001b[0;34m(self, question, num_results)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrieving relevant documents...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(selected_queries) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# If multiple queries were selected, use the multi-query approach\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     docs, query_docs_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve_documents(selected_queries, k\u001b[38;5;241m=\u001b[39mnum_results)\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# Generate a combined answer\u001b[39;00m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerating combined answer with conversation context...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 120\u001b[0m, in \u001b[0;36mResearchAssistant.retrieve_documents\u001b[0;34m(self, queries, k)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_foundational_query:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# First try to get foundational documents\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     filter_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfoundational\u001b[39m\u001b[38;5;124m\"\u001b[39m}}\n\u001b[0;32m--> 120\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_db\u001b[38;5;241m.\u001b[39msimilarity_search_with_relevance_scores(\n\u001b[1;32m    121\u001b[0m         query, k\u001b[38;5;241m=\u001b[39mk, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39mfilter_dict\n\u001b[1;32m    122\u001b[0m     )\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# If no foundational docs, fall back to regular search\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/vectorstores/base.py:555\u001b[0m, in \u001b[0;36mVectorStore.similarity_search_with_relevance_scores\u001b[0;34m(self, query, k, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return docs and relevance scores in the range [0, 1].\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \n\u001b[1;32m    541\u001b[0m \u001b[38;5;124;03m0 is dissimilar, 1 is most similar.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;124;03m    List of Tuples of (doc, similarity_score).\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    553\u001b[0m score_threshold \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 555\u001b[0m docs_and_similarities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_similarity_search_with_relevance_scores(\n\u001b[1;32m    556\u001b[0m     query, k\u001b[38;5;241m=\u001b[39mk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    557\u001b[0m )\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    559\u001b[0m     similarity \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m similarity \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, similarity \u001b[38;5;129;01min\u001b[39;00m docs_and_similarities\n\u001b[1;32m    561\u001b[0m ):\n\u001b[1;32m    562\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelevance scores must be between\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    564\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m 0 and 1, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocs_and_similarities\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    565\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    566\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/vectorstores/base.py:503\u001b[0m, in \u001b[0;36mVectorStore._similarity_search_with_relevance_scores\u001b[0;34m(self, query, k, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Default similarity search with relevance scores.\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \n\u001b[1;32m    487\u001b[0m \u001b[38;5;124;03mModify if necessary in subclass.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;124;03m    List of Tuples of (doc, similarity_score)\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    502\u001b[0m relevance_score_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_relevance_score_fn()\n\u001b[0;32m--> 503\u001b[0m docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score(query, k, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [(doc, relevance_score_fn(score)) \u001b[38;5;28;01mfor\u001b[39;00m doc, score \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:440\u001b[0m, in \u001b[0;36mChroma.similarity_search_with_score\u001b[0;34m(self, query, k, filter, where_document, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    439\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function\u001b[38;5;241m.\u001b[39membed_query(query)\n\u001b[0;32m--> 440\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__query_collection(\n\u001b[1;32m    441\u001b[0m         query_embeddings\u001b[38;5;241m=\u001b[39m[query_embedding],\n\u001b[1;32m    442\u001b[0m         n_results\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m    443\u001b[0m         where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m,\n\u001b[1;32m    444\u001b[0m         where_document\u001b[38;5;241m=\u001b[39mwhere_document,\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    446\u001b[0m     )\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _results_to_docs_and_scores(results)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/utils/utils.py:53\u001b[0m, in \u001b[0;36mxor_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExactly one argument in each of the following\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m groups must be defined:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(invalid_group_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:157\u001b[0m, in \u001b[0;36mChroma.__query_collection\u001b[0;34m(self, query_texts, query_embeddings, n_results, where, where_document, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import chromadb python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install chromadb`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m     )\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection\u001b[38;5;241m.\u001b[39mquery(  \u001b[38;5;66;03m# type: ignore[return-value]\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     query_texts\u001b[38;5;241m=\u001b[39mquery_texts,\n\u001b[1;32m    159\u001b[0m     query_embeddings\u001b[38;5;241m=\u001b[39mquery_embeddings,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     n_results\u001b[38;5;241m=\u001b[39mn_results,\n\u001b[1;32m    161\u001b[0m     where\u001b[38;5;241m=\u001b[39mwhere,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     where_document\u001b[38;5;241m=\u001b[39mwhere_document,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    164\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/chromadb/api/models/Collection.py:211\u001b[0m, in \u001b[0;36mCollection.query\u001b[0;34m(self, query_embeddings, query_texts, query_images, query_uris, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mquery\u001b[39m(\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    170\u001b[0m     query_embeddings: Optional[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m     ],\n\u001b[1;32m    187\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m QueryResult:\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the n_results nearest neighbor embeddings for provided query_embeddings or query_texts.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m \n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     query_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_prepare_query_request(\n\u001b[1;32m    212\u001b[0m         query_embeddings\u001b[38;5;241m=\u001b[39mquery_embeddings,\n\u001b[1;32m    213\u001b[0m         query_texts\u001b[38;5;241m=\u001b[39mquery_texts,\n\u001b[1;32m    214\u001b[0m         query_images\u001b[38;5;241m=\u001b[39mquery_images,\n\u001b[1;32m    215\u001b[0m         query_uris\u001b[38;5;241m=\u001b[39mquery_uris,\n\u001b[1;32m    216\u001b[0m         n_results\u001b[38;5;241m=\u001b[39mn_results,\n\u001b[1;32m    217\u001b[0m         where\u001b[38;5;241m=\u001b[39mwhere,\n\u001b[1;32m    218\u001b[0m         where_document\u001b[38;5;241m=\u001b[39mwhere_document,\n\u001b[1;32m    219\u001b[0m         include\u001b[38;5;241m=\u001b[39minclude,\n\u001b[1;32m    220\u001b[0m     )\n\u001b[1;32m    222\u001b[0m     query_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_query(\n\u001b[1;32m    223\u001b[0m         collection_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m    224\u001b[0m         query_embeddings\u001b[38;5;241m=\u001b[39mquery_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m         database\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase,\n\u001b[1;32m    231\u001b[0m     )\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_query_response(\n\u001b[1;32m    234\u001b[0m         response\u001b[38;5;241m=\u001b[39mquery_results, include\u001b[38;5;241m=\u001b[39mquery_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    235\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/chromadb/api/models/CollectionCommon.py:93\u001b[0m, in \u001b[0;36mvalidation_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     92\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(msg)\u001b[38;5;241m.\u001b[39mwith_traceback(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/chromadb/api/models/CollectionCommon.py:90\u001b[0m, in \u001b[0;36mvalidation_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     92\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/chromadb/api/models/CollectionCommon.py:294\u001b[0m, in \u001b[0;36mCollectionCommon._validate_and_prepare_query_request\u001b[0;34m(self, query_embeddings, query_texts, query_images, query_uris, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[1;32m    293\u001b[0m validate_base_record_set(record_set\u001b[38;5;241m=\u001b[39mquery_records)\n\u001b[0;32m--> 294\u001b[0m validate_filter_set(filter_set\u001b[38;5;241m=\u001b[39mfilters)\n\u001b[1;32m    295\u001b[0m validate_include(include\u001b[38;5;241m=\u001b[39minclude)\n\u001b[1;32m    296\u001b[0m validate_n_results(n_results\u001b[38;5;241m=\u001b[39mn_results)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/chromadb/api/types.py:345\u001b[0m, in \u001b[0;36mvalidate_filter_set\u001b[0;34m(filter_set)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidate_filter_set\u001b[39m(filter_set: FilterSet) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filter_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m         validate_where(filter_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filter_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere_document\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    347\u001b[0m         validate_where_document(filter_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere_document\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/chromadb/api/types.py:659\u001b[0m, in \u001b[0;36mvalidate_where\u001b[0;34m(where)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    647\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected operand value to be an list for operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperator\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperand\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    648\u001b[0m         )\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m operator \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$gt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$gte\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$nin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    658\u001b[0m ]:\n\u001b[0;32m--> 659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    660\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected where operator to be one of $gt, $gte, $lt, $lte, $ne, $eq, $in, $nin, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperator\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    662\u001b[0m     )\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(operand, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    666\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected where operand value to be a str, int, float, or list of those type, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperand\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    667\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected where operator to be one of $gt, $gte, $lt, $lte, $ne, $eq, $in, $nin, got chunk_type in query."
     ]
    }
   ],
   "source": [
    "# Initialize the research assistant\n",
    "assistant = ResearchAssistant(llm=llm, vector_db=vector_db)\n",
    "while True:\n",
    "    \n",
    "    query = input(\"\\nEnter your query (or 'exit' to quit): \")\n",
    "    if query.lower() == 'exit':\n",
    "        break\n",
    "        \n",
    "    # Ask the user whether to use memory mode or full pipeline\n",
    "    use_memory = input(\"Use conversation memory? (y/n): \").lower() == 'y'\n",
    "    \n",
    "    # Process the query\n",
    "    answer = assistant.process_query(query, use_memory=use_memory)\n",
    "    print(\"\\nFinal Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c9f53c-5fc2-4eee-9000-3cfae300122e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Improving DB with Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f10b5bc4-24cb-4096-941a-2a84a67e8908",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class FoundationalKnowledgeManager:\n",
    "    def __init__(self, vector_db, embeddings_model=None):\n",
    "        self.vector_db = vector_db\n",
    "        self.embeddings_model = embeddings_model\n",
    "        self.loaded_sources = set()  # Track which sources have been loaded\n",
    "        self.url_to_source_map = {\n",
    "            \"cs_algorithms\": \"https://raw.githubusercontent.com/OpenGenus/cs-fundamentals/main/algorithms.md\",\n",
    "            \"cs_data_structures\": \"https://raw.githubusercontent.com/OpenGenus/cs-fundamentals/main/data_structures.md\",\n",
    "            # Add more mappings as needed\n",
    "        }\n",
    "        \n",
    "    def load_from_github(self, topic_key):\n",
    "        \"\"\"Load foundational content from pre-defined GitHub sources\"\"\"\n",
    "        if topic_key not in self.url_to_source_map:\n",
    "            return False\n",
    "            \n",
    "        if topic_key in self.loaded_sources:\n",
    "            print(f\"Topic {topic_key} already loaded.\")\n",
    "            return True\n",
    "            \n",
    "        url = self.url_to_source_map[topic_key]\n",
    "        # Use requests to download the content\n",
    "        import requests\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            content = response.text\n",
    "            # Process and chunk the content\n",
    "            chunks = self._chunk_content(content, topic_key)\n",
    "            # Add chunks to vector DB\n",
    "            self._add_to_vector_db(chunks)\n",
    "            self.loaded_sources.add(topic_key)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def load_from_file(self, file_path, topic_name):\n",
    "        \"\"\"Load foundational content from a local file\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            return False\n",
    "            \n",
    "        source_key = f\"file_{topic_name}\"\n",
    "        if source_key in self.loaded_sources:\n",
    "            print(f\"File {file_path} already loaded as {topic_name}.\")\n",
    "            return True\n",
    "            \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            \n",
    "        # Process and chunk the content\n",
    "        chunks = self._chunk_content(content, topic_name)\n",
    "        # Add chunks to vector DB\n",
    "        self._add_to_vector_db(chunks)\n",
    "        self.loaded_sources.add(source_key)\n",
    "        return True\n",
    "        \n",
    "    def _chunk_content(self, content, source_name, chunk_size=1000, overlap=200):\n",
    "        \"\"\"Split content into overlapping chunks\"\"\"\n",
    "        chunks = []\n",
    "        # Simple chunking by character count\n",
    "        start = 0\n",
    "        while start < len(content):\n",
    "            end = min(start + chunk_size, len(content))\n",
    "            if end < len(content) and end - start == chunk_size:\n",
    "                # Try to find a good breaking point\n",
    "                break_point = content.rfind('\\n', start, end)\n",
    "                if break_point > start + chunk_size // 2:\n",
    "                    end = break_point\n",
    "                    \n",
    "            chunk_content = content[start:end]\n",
    "            chunks.append({\n",
    "                \"content\": chunk_content,\n",
    "                \"metadata\": {\n",
    "                    \"source\": source_name,\n",
    "                    \"chunk_type\": \"foundational\",\n",
    "                    \"start_char\": start,\n",
    "                    \"end_char\": end\n",
    "                }\n",
    "            })\n",
    "            start = end - overlap\n",
    "        return chunks\n",
    "        \n",
    "    def _add_to_vector_db(self, chunks):\n",
    "        \"\"\"Add chunks to the vector database\"\"\"\n",
    "        # Convert chunks to Document objects\n",
    "        documents = [\n",
    "            Document(\n",
    "                page_content=chunk[\"content\"],\n",
    "                metadata=chunk[\"metadata\"]\n",
    "            ) for chunk in chunks\n",
    "        ]\n",
    "        \n",
    "        # Use vector_db to add documents\n",
    "        self.vector_db.add_documents(documents)\n",
    "        print(f\"Added {len(documents)} foundational knowledge chunks to vector DB\")\n",
    "        \n",
    "    def load_standard_cs_topics(self):\n",
    "        \"\"\"Load a standard set of CS fundamentals\"\"\"\n",
    "        for topic in self.url_to_source_map.keys():\n",
    "            self.load_from_github(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "445904a8-9cd7-48a0-ab9e-093cfb9bd1a5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class FoundationalKnowledgeManager:\n",
    "    def __init__(self, vector_db, embedding_model):\n",
    "        self.vector_db = vector_db\n",
    "        self.embedding_model = embedding_model\n",
    "        self.github_sources = {\n",
    "            \"algorithms\": \"TheAlgorithms/Python\",\n",
    "            \"os\": \"tuhdo/os01\",\n",
    "            \"system_design\": \"donnemartin/system-design-primer\",\n",
    "            \"python\": \"jakevdp/PythonDataScienceHandbook\",\n",
    "            \"ml\": \"microsoft/ML-For-Beginners\",\n",
    "            \"database\": \"pingcap/awesome-database-learning\",\n",
    "            \"networks\": \"leandromoreira/linux-network-performance-parameters\"\n",
    "        }\n",
    "        \n",
    "    def load_from_github(self, topic_key):\n",
    "        \"\"\"Load content from predefined GitHub repositories with foundational CS knowledge\"\"\"\n",
    "        if topic_key not in self.github_sources:\n",
    "            return f\"Topic {topic_key} not found in available sources\"\n",
    "            \n",
    "        repo = self.github_sources[topic_key]\n",
    "        # Implement GitHub content fetching logic here\n",
    "        # Use GitHub API to get markdown/text content from README files and other docs\n",
    "        \n",
    "        # Process content and add to vector DB with metadata\n",
    "        # Important: Tag these documents as foundational knowledge\n",
    "        # self.vector_db.add_texts(texts, metadatas=[{\"chunk_type\": \"foundational\", \"topic\": topic_key}])\n",
    "        \n",
    "        return f\"Loaded foundational knowledge for {topic_key} from {repo}\"\n",
    "        \n",
    "    def load_from_file(self, file_path, topic_name):\n",
    "        \"\"\"Load content from local file\"\"\"\n",
    "        # Implement file loading logic\n",
    "        # Process content and add to vector DB with metadata \n",
    "        # self.vector_db.add_texts(texts, metadatas=[{\"chunk_type\": \"foundational\", \"topic\": topic_name}])\n",
    "        \n",
    "    def load_standard_cs_topics(self):\n",
    "        \"\"\"Load all standard CS topics\"\"\"\n",
    "        results = []\n",
    "        for topic in self.github_sources:\n",
    "            results.append(self.load_from_github(topic))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a74e76-b2d4-4a1d-9cc7-327f74f0271d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e74a57e-428c-4b7b-8ae3-bb9aa605d6c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Initializing SageRAG Research Assistant...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mInitializing SageRAG Research Assistant\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading embedding model<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading embedding model\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading language model<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading language model\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Connecting to vector database<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Connecting to vector database\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Initialization complete!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mInitialization complete!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 10:36:52,818 - SageRAG - INFO - Initializing default embedding model\n",
      "2025-04-21 10:36:57,316 - SageRAG - INFO - Research Assistant initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24803\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\"> Welcome </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\"></span>  <span style=\"color: #008080; text-decoration-color: #008080\"></span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\"></span>                                          <span style=\"font-weight: bold\">SageRAG Research Assistant</span>                                           <span style=\"color: #008080; text-decoration-color: #008080\"></span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\"></span>  <span style=\"color: #008080; text-decoration-color: #008080\"></span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\"></span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\"></span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\"></span> Ask questions about scientific papers in the database.                                                          <span style=\"color: #008080; text-decoration-color: #008080\"></span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\"></span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m\u001b[0m\u001b[36m\u001b[0m\u001b[36m Welcome \u001b[0m\u001b[36m\u001b[0m\u001b[36m\u001b[0m\n",
       "\u001b[36m\u001b[0m  \u001b[36m\u001b[0m\n",
       "\u001b[36m\u001b[0m                                          \u001b[1mSageRAG Research Assistant\u001b[0m                                           \u001b[36m\u001b[0m\n",
       "\u001b[36m\u001b[0m  \u001b[36m\u001b[0m\n",
       "\u001b[36m\u001b[0m                                                                                                                 \u001b[36m\u001b[0m\n",
       "\u001b[36m\u001b[0m Ask questions about scientific papers in the database.                                                          \u001b[36m\u001b[0m\n",
       "\u001b[36m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Options:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mOptions:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Ask a question\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m. Ask a question\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. View conversation history\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m. View conversation history\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Clear conversation history\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m. Clear conversation history\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. Go for Advanced Research\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4\u001b[0m. Go for Advanced Research\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. Exit\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m5\u001b[0m. Exit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your choice (1-5):  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 10:37:03,873 - SageRAG - INFO - Initializing default embedding model\n",
      "2025-04-21 10:37:09,537 - SageRAG - INFO - Research Assistant initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Options:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mOptions:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Ask a question\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m. Ask a question\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. View conversation history\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m. View conversation history\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Clear conversation history\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m. Clear conversation history\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. Go for Advanced Research\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4\u001b[0m. Go for Advanced Research\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. Exit\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m5\u001b[0m. Exit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your choice (1-5):  5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Thank you for using SageRAG Research Assistant. Goodbye!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mThank you for using SageRAG Research Assistant. Goodbye!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import os\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, BaseOutputParser\n",
    "from langchain.schema import Document, HumanMessage, AIMessage\n",
    "from langchain.vectorstores import VectorStore\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms.base import BaseLLM\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from pydantic import BaseModel, Field\n",
    "import logging\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "# Suppress library-specific logging\n",
    "import logging\n",
    "logging.getLogger('sentence_transformers').setLevel(logging.WARNING)\n",
    "logging.getLogger('transformers').setLevel(logging.WARNING)\n",
    "logging.getLogger('chromadb').setLevel(logging.WARNING)\n",
    "\n",
    "# Then set up your own logging as before\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"SageRAG\")\n",
    "\n",
    "# Rich console for prettier output\n",
    "console = Console()\n",
    "\n",
    "# Enhanced output parsers with better error handling\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Parse output that contains a numbered list and return as a list of strings.\"\"\"\n",
    "    \n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        \"\"\"Parse text into a list of strings.\"\"\"\n",
    "        # Updated regex to be more robust with various numbering styles\n",
    "        lines = re.findall(r\"^\\s*\\d+\\.?\\s+(.*?)$\", text, re.MULTILINE)\n",
    "        return [line.strip() for line in lines if line.strip()]\n",
    "    \n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"line_list\"\n",
    "\n",
    "class QueryRating(BaseModel):\n",
    "    \"\"\"Schema for query rating output.\"\"\"\n",
    "    rating: int = Field(description=\"Rating from 1-5\")\n",
    "    explanation: str = Field(description=\"Explanation for the rating\")\n",
    "\n",
    "@dataclass\n",
    "class RetrievedDocument:\n",
    "    \"\"\"Dataclass for tracking retrieved document info.\"\"\"\n",
    "    document: Document\n",
    "    score: float\n",
    "    query: str = \"\"\n",
    "    rank: int = 0\n",
    "\n",
    "class ResearchAssistant:\n",
    "    \"\"\"AI Research Assistant using RAG for scientific paper queries.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        llm: BaseLLM, \n",
    "        vector_db: VectorStore, \n",
    "        embeddings_model: Optional[SentenceTransformer] = None,\n",
    "        relevance_threshold: float = 0.2,\n",
    "        max_docs_per_query: int = 3\n",
    "    ):\n",
    "        \"\"\"Initialize the Research Assistant with LLM and vector database.\n",
    "        \n",
    "        Args:\n",
    "            llm: The language model to use for generation\n",
    "            vector_db: Vector database for document retrieval\n",
    "            embeddings_model: Optional SentenceTransformer model for semantic similarity\n",
    "            relevance_threshold: Minimum relevance score for documents\n",
    "            max_docs_per_query: Maximum documents to use per query\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.vector_db = vector_db\n",
    "        self.relevance_threshold = relevance_threshold\n",
    "        self.max_docs_per_query = max_docs_per_query\n",
    "        \n",
    "        # Initialize sentence transformer model for semantic similarity if not provided\n",
    "        if embeddings_model is None:\n",
    "            logger.info(\"Initializing default embedding model\")\n",
    "            self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        else:\n",
    "            self.semantic_model = embeddings_model\n",
    "            \n",
    "        # Initialize memory\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        # Create a retriever for direct use\n",
    "        self.retriever = vector_db.as_retriever()\n",
    "        \n",
    "        # Define parsers\n",
    "        self.json_parser = JsonOutputParser(pydantic_object=QueryRating)\n",
    "        self.list_parser = LineListOutputParser()\n",
    "        \n",
    "        logger.info(\"Research Assistant initialized successfully\")\n",
    "    \n",
    "    def rate_query(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Rates the query and gives an explanation.\n",
    "        \n",
    "        Args:\n",
    "            query: The user's query to be evaluated\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing rating and explanation\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an intelligent assistant trained to evaluate search queries for a scientific research database.\n",
    "        Given the following query: \"{query}\"\n",
    "        \n",
    "        1. Rate the query on a scale of 1 (very poor) to 5 (excellent) based on:\n",
    "           - Clarity: Is the query clear and unambiguous?\n",
    "           - Specificity: Does it contain specific technical terms or concepts?\n",
    "           - Relevance: Is it focused on retrieving scientific content?\n",
    "           - Retrievability: Will it work well with vector search?\n",
    "        \n",
    "        2. Provide a short explanation for the rating (what makes it effective or ineffective).\n",
    "        \n",
    "        Respond in JSON format:\n",
    "        {{\n",
    "          \"rating\": <number between 1-5>,\n",
    "          \"explanation\": \"<your explanation>\"\n",
    "        }}\n",
    "        \"\"\")\n",
    "        \n",
    "        chain: Runnable = prompt | self.llm | self.json_parser\n",
    "        \n",
    "        try:\n",
    "            return chain.invoke({\"query\": query})\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error rating query: {e}\")\n",
    "            # Fallback response if parsing fails\n",
    "            return {\n",
    "                \"rating\": 3, \n",
    "                \"explanation\": \"Unable to rate query properly. Consider adding more specific terms.\"\n",
    "            }\n",
    "    \n",
    "    # def suggest_rewrites(self, query: str, chat_history: Optional[List] = None) -> List[str]:\n",
    "    #     \"\"\"Returns rephrased versions of the query optimized for retrieval.\n",
    "        \n",
    "    #     Args:\n",
    "    #         query: Original query to rewrite\n",
    "    #         chat_history: Optional conversation history for context\n",
    "            \n",
    "    #     Returns:\n",
    "    #         List of rewritten queries\n",
    "    #     \"\"\"\n",
    "    #     history_context = \"\"\n",
    "    #     if chat_history:\n",
    "    #         history_context = \"Consider this conversation context when rewriting the query:\\n\"\n",
    "    #         for message in chat_history[-3:]:  # Use only last 3 messages for brevity\n",
    "    #             if hasattr(message, \"content\"):\n",
    "    #                 role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "    #                 history_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        # prompt = PromptTemplate(\n",
    "        #     input_variables=[\"question\", \"history_context\"],\n",
    "        #     template=\"\"\"You are an AI assistant specializing in scientific research queries.\n",
    "            \n",
    "        #     {history_context}\n",
    "            \n",
    "        #     Rewrite the following question in 5 different ways to improve retrieval from a scientific paper database.\n",
    "        #     Each rewrite should be a complete, standalone query that could be used directly for search.\n",
    "            \n",
    "        #     For each rewrite, focus on a different strategy and provide a brief explanation:\n",
    "        #     1. Technical terminology for better vector matching\n",
    "        #     2. Breaking down the query into a clearer, more detailed formulation\n",
    "        #     3. Including relevant synonyms or related concepts that might appear in papers\n",
    "        #     4. Using different syntax while preserving the original meaning\n",
    "        #     5. Explicitly mentioning key entities and relationships from the original query\n",
    "            \n",
    "        #     Format your response as a numbered list with the rewritten query and its explanation.\n",
    "        \n",
    "        #     Original Question: {question}\n",
    "            \n",
    "        #     Rewritten Queries:\n",
    "        #     1. Rewrite 1: **Technical terminology for better vector matching**: [your rewrite]\n",
    "        #     2. Rewrite 2: **Breaking down complex queries into clearer formulations**: [your rewrite]\n",
    "        #     ...\"\"\"\n",
    "        # )\n",
    "        \n",
    "    #     try:\n",
    "    #         # Make sure we're handling the response type correctly\n",
    "    #         result = (prompt | self.llm | self.list_parser).invoke({\n",
    "    #             \"question\": query,\n",
    "    #             \"history_context\": history_context\n",
    "    #         })\n",
    "    #         return result\n",
    "            \n",
    "    #     except Exception as e:\n",
    "    #         logger.error(f\"Error suggesting rewrites with explanations: {e}\")\n",
    "    #         # Return minimal set with explanations if parsing fails\n",
    "    #         return [\n",
    "    #             {\"query\": query, \"explanation\": \"Original query\", \"confidence\": 1.0},\n",
    "    #             {\"query\": f\"Research about {query}\", \"explanation\": \"Technical terminology\", \"confidence\": 0.7},\n",
    "    #             {\"query\": f\"Papers discussing {query} methodology and applications\", \"explanation\": \"Breaking down query\", \"confidence\": 0.7},\n",
    "    #             {\"query\": f\"Literature review on {query} techniques and algorithms\", \"explanation\": \"Including synonyms\", \"confidence\": 0.7},\n",
    "    #             {\"query\": f\"Recent advances in {query} research\", \"explanation\": \"Different syntax\", \"confidence\": 0.7},\n",
    "    #             {\"query\": f\"Theoretical foundations of {query}\", \"explanation\": \"Key entities and relationships\", \"confidence\": 0.7}\n",
    "    #         ]\n",
    "    \n",
    "    def suggest_rewrites(self, query: str, chat_history: Optional[List] = None) -> List[str]:\n",
    "        \"\"\"Returns rephrased versions of the query optimized for retrieval.\n",
    "        \n",
    "        Args:\n",
    "            query: Original query to rewrite\n",
    "            chat_history: Optional conversation history for context\n",
    "            \n",
    "        Returns:\n",
    "            List of rewritten queries\n",
    "        \"\"\"\n",
    "        history_context = \"\"\n",
    "        if chat_history:\n",
    "            history_context = \"Consider this conversation context when rewriting the query:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use only last 3 messages for brevity\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    history_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"history_context\"],\n",
    "            template=\"\"\"You are an AI assistant specializing in scientific research queries.\n",
    "        \n",
    "        {history_context}\n",
    "        \n",
    "        TASK: Create 5 alternative versions of the user's question that will help improve retrieval from a scientific paper database.\n",
    "        \n",
    "        Original question: {question}\n",
    "        \n",
    "        For each alternative version:\n",
    "        - Write a COMPLETE, EXECUTABLE query (not just a description)\n",
    "        - Make each version meaningfully different while preserving the core information need\n",
    "        - Include the ACTUAL REWRITTEN QUESTION text\n",
    "        \n",
    "        FORMAT YOUR RESPONSE EXACTLY LIKE THIS EXAMPLE:\n",
    "        \n",
    "        0. Original: What are the effects of climate change on coral reefs?\n",
    "           Confidence: 1.00\n",
    "        1. Rewrite 1: **Technical terminology for better vector matching**: \"What are the impacts of increased ocean acidification and sea surface temperature anomalies on scleractinian coral communities and associated reef ecosystems?\"\n",
    "           Confidence: 0.70\n",
    "        2. Rewrite 2: **Breaking down complex queries into clearer formulations**: \"How do rising ocean temperatures affect coral health? What role does ocean acidification play in coral bleaching events? What are the ecosystem-level consequences of coral reef degradation due to climate change?\"\n",
    "           Confidence: 0.70\n",
    "        3. Rewrite 3: **Adding relevant synonyms or related concepts**: \"Examine the effects of global warming, greenhouse gas emissions, and anthropogenic climate forcing on coral reef ecosystems, coral bleaching, calcification rates, and reef biodiversity.\"\n",
    "           Confidence: 0.70\n",
    "        4. Rewrite 4: **Varying syntax while preserving semantic meaning**: \"In what ways are coral reef systems being modified by climate change variables? How are anthozoans responding to altered oceanic conditions resulting from global climate shifts?\"\n",
    "           Confidence: 0.70\n",
    "        5. Rewrite 5: **Including key entities and relationships from the original query**: \"Analyze the causal relationship between climate change parameters (CO2 levels, temperature increase, ocean pH) and coral reef ecosystem health indicators (species diversity, coral cover percentage, bleaching frequency).\"\n",
    "           Confidence: 0.70\n",
    "        \n",
    "        Now, for the question \"{question}\", provide 5 complete, executable rewritten queries following the exact format above. Each rewrite must include the full rewritten question text, not just a description of how to rewrite it.\n",
    "        \"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            result = (prompt | self.llm | self.list_parser).invoke({\n",
    "                \"question\": query,\n",
    "                \"history_context\": history_context\n",
    "            })\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error suggesting rewrites: {e}\")\n",
    "            # Return a minimal set of rewrites if parsing fails\n",
    "            return [\n",
    "                query,  # Original query\n",
    "                f\"research about {query}\",\n",
    "                f\"papers discussing {query}\"\n",
    "            ]\n",
    "    def compute_confidence(self, original: str, rewritten: str) -> float:\n",
    "        \"\"\"Computes semantic similarity between original and rewritten queries.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            rewritten: Rewritten version\n",
    "            \n",
    "        Returns:\n",
    "            Similarity score between 0-1\n",
    "        \"\"\"\n",
    "        try:\n",
    "            vec_orig = self.semantic_model.encode(original, convert_to_tensor=True)\n",
    "            vec_rewrite = self.semantic_model.encode(rewritten, convert_to_tensor=True)\n",
    "            return float(util.pytorch_cos_sim(vec_orig, vec_rewrite).item())\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error computing similarity: {e}\")\n",
    "            return 0.7  # Default reasonable value\n",
    "    \n",
    "    def score_queries(self, original: str, queries: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Scores and sorts rephrased queries by confidence.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            queries: List of rewritten queries\n",
    "            \n",
    "        Returns:\n",
    "            List of (query, score) tuples sorted by score\n",
    "        \"\"\"\n",
    "        scored = [(q, self.compute_confidence(original, q)) for q in queries]\n",
    "        return sorted(scored, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    def present_query_options(self, original: str, queries: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Presents the original and rewritten queries with confidence scores.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            queries: List of rewritten queries\n",
    "            \n",
    "        Returns:\n",
    "            List of (query, score) tuples including original query\n",
    "        \"\"\"\n",
    "        # Add original query at the top\n",
    "        scored_queries = [(\"Original: \" + original, 1.0)]\n",
    "        \n",
    "        # Add scored rewritten queries\n",
    "        rewritten_scores = self.score_queries(original, queries)\n",
    "        for i, (query, score) in enumerate(rewritten_scores, 1):\n",
    "            if \"Query:\" in query:\n",
    "                parts = query.split(\"Query:\")\n",
    "                # Format with Query on a new line and bold\n",
    "                formatted_query = f\"{parts[0]}\\n[bold]Query:[/bold]{parts[1]}\"\n",
    "                scored_queries.append((f\"Rewrite {i}: {formatted_query}\", score))\n",
    "            else:\n",
    "                scored_queries.append((f\"Rewrite {i}: {query}\", score))\n",
    "            \n",
    "        # Display options in a nice format\n",
    "        console.print(\"\\n[bold cyan]Available search queries:[/bold cyan]\")\n",
    "        for i, (query, score) in enumerate(scored_queries):\n",
    "            confidence_color = \"green\" if score > 0.8 else \"yellow\" if score > 0.6 else \"red\"\n",
    "            console.print(f\"[bold]{i}.[/bold] {query}\")\n",
    "            console.print(f\"   [bold {confidence_color}]Confidence: {score:.2f}[/bold {confidence_color}]\")\n",
    "            \n",
    "        return scored_queries\n",
    "    \n",
    "    \n",
    "    def retrieve_documents(\n",
    "        self, \n",
    "        queries: List[str], \n",
    "        k: int = 5\n",
    "    ) -> Tuple[List[RetrievedDocument], Dict[str, List[RetrievedDocument]]]:\n",
    "        \"\"\"Retrieves documents using multiple queries, preserving query information.\n",
    "        \n",
    "        Args:\n",
    "            queries: List of queries to retrieve documents for\n",
    "            k: Number of documents to retrieve per query\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (all unique documents, query->documents mapping)\n",
    "        \"\"\"\n",
    "        all_docs: List[RetrievedDocument] = []\n",
    "        unique_content = set()\n",
    "        query_docs_map: Dict[str, List[RetrievedDocument]] = {}\n",
    "        \n",
    "        # Retrieve docs for each query\n",
    "        for query in queries:\n",
    "            console.print(f\"\\n[bold blue]Query:[/bold blue] '{query}'\")\n",
    "            \n",
    "            try:\n",
    "                results = self.vector_db.similarity_search_with_relevance_scores(query, k=k)\n",
    "                print(\"LOG123: this is results length\",len(results))\n",
    "                docs_for_query: List[RetrievedDocument] = []\n",
    "                \n",
    "                for i, (doc, score) in enumerate(results, 1):\n",
    "                    # Create retrieved document object\n",
    "                    retrieved_doc = RetrievedDocument(\n",
    "                        document=doc,\n",
    "                        score=score,\n",
    "                        query=query,\n",
    "                        rank=i\n",
    "                    )\n",
    "                    \n",
    "                    # Display result info\n",
    "                    score_color = \"green\" if score > 0.8 else \"yellow\" if score > 0.6 else \"red\"\n",
    "                    console.print(f\"[bold]--- Result {i} ---[/bold]\")\n",
    "                    console.print(f\"[bold {score_color}]Score: {score:.4f}[/bold {score_color}]\")\n",
    "                    \n",
    "                    # Show document preview\n",
    "                    preview = doc.page_content[:150] + \"...\" if len(doc.page_content) > 150 else doc.page_content\n",
    "                    console.print(Panel(preview, title=\"Content Preview\", width=100))\n",
    "                    \n",
    "                    # Show metadata if available\n",
    "                    if hasattr(doc, 'metadata') and doc.metadata:\n",
    "                        source = doc.metadata.get('source', 'Unknown')\n",
    "                        console.print(f\"[dim]Source: {source}[/dim]\")\n",
    "                    \n",
    "                    # Add to results for this query\n",
    "                    docs_for_query.append(retrieved_doc)\n",
    "                    \n",
    "                    # Only add unique documents to overall collection\n",
    "                    if doc.page_content not in unique_content:\n",
    "                        unique_content.add(doc.page_content)\n",
    "                        all_docs.append(retrieved_doc)\n",
    "                \n",
    "                query_docs_map[query] = docs_for_query\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error retrieving documents for query '{query}': {e}\")\n",
    "                console.print(f\"[bold red]Error retrieving documents for query: {query}[/bold red]\")\n",
    "        \n",
    "        console.print(f\"\\n[bold green]Total unique documents:[/bold green] {len(all_docs)}\")\n",
    "        return all_docs, query_docs_map\n",
    "\n",
    "    def rerank_docs(self, query: str, docs: List[RetrievedDocument]) -> List[RetrievedDocument]:\n",
    "        \"\"\"Reranks documents based on semantic similarity to the query.\n",
    "        \n",
    "        Args:\n",
    "            query: Query to compare documents against\n",
    "            docs: List of retrieved documents\n",
    "            \n",
    "        Returns:\n",
    "            Reranked list of documents with updated scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            query_embedding = self.semantic_model.encode(query, convert_to_tensor=True)\n",
    "            reranked = []\n",
    "            \n",
    "            for doc in docs:\n",
    "                doc_embedding = self.semantic_model.encode(doc.document.page_content, convert_to_tensor=True)\n",
    "                new_score = float(util.pytorch_cos_sim(query_embedding, doc_embedding).item())\n",
    "                \n",
    "                # Create new RetrievedDocument with updated score\n",
    "                reranked_doc = RetrievedDocument(\n",
    "                    document=doc.document,\n",
    "                    score=new_score,\n",
    "                    query=doc.query,\n",
    "                    rank=0  # Will be updated after sorting\n",
    "                )\n",
    "                reranked.append(reranked_doc)\n",
    "            \n",
    "            # Sort by score and update ranks\n",
    "            reranked.sort(key=lambda x: x.score, reverse=True)\n",
    "            for i, doc in enumerate(reranked, 1):\n",
    "                doc.rank = i\n",
    "                \n",
    "            return reranked\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reranking documents: {e}\")\n",
    "            return docs  # Return original documents if reranking fails\n",
    "    \n",
    "    def can_answer_without_retrieval(self, question: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"Determines if a question can be answered directly from memory without retrieval.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (can_answer, answer_if_available)\n",
    "        \"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        if not chat_history:\n",
    "            return False, None\n",
    "            \n",
    "        # Create the prompt to check if we can answer directly from the conversation\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an AI assistant helping with a scientific research conversation.\n",
    "        Given the following conversation history and a new question, determine if the question:\n",
    "        1. Can be answered directly based ONLY on the conversation history (like \"what was my previous question?\")\n",
    "        2. Does NOT require retrieving new information from scientific papers\n",
    "        \n",
    "        If both conditions are true, provide the answer. Otherwise, respond with \"NEEDS_RETRIEVAL\".\n",
    "        \n",
    "        Conversation History:\n",
    "        {chat_history}\n",
    "        \n",
    "        New Question: {question}\n",
    "        \n",
    "        Your assessment (answer directly or respond with \"NEEDS_RETRIEVAL\"):\n",
    "        \"\"\")\n",
    "        \n",
    "        # Format the chat history for context\n",
    "        history_str = \"\"\n",
    "        for message in chat_history[-5:]:  # Use only last 5 messages for brevity\n",
    "            if hasattr(message, \"content\"):\n",
    "                role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                history_str += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        try:\n",
    "            # Ask the LLM if this can be answered without retrieval\n",
    "            response = self.llm.invoke(\n",
    "                prompt.format(chat_history=history_str, question=question)\n",
    "            )\n",
    "            \n",
    "            # If the response is \"NEEDS_RETRIEVAL\", we need to use retrieval\n",
    "            if \"NEEDS_RETRIEVAL\" in response:\n",
    "                return False, None\n",
    "            else:\n",
    "                logger.info(\"Question can be answered from conversation history\")\n",
    "                return True, response\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error determining if retrieval needed: {e}\")\n",
    "            return False, None  \n",
    "    \n",
    "    def generate_final_answer(\n",
    "        self, \n",
    "        question: str, \n",
    "        docs: List[RetrievedDocument], \n",
    "        max_docs: int = 5\n",
    "    ) -> str:\n",
    "        \"\"\"Generates the final answer from retrieved documents with LLM fallback.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            docs: List of retrieved documents\n",
    "            max_docs: Maximum number of documents to include\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # First, evaluate if the documents are sufficient\n",
    "        is_sufficient, confidence = self.evaluate_document_relevance(question, docs)\n",
    "        \n",
    "        # If documents are insufficient, use LLM fallback\n",
    "        if not is_sufficient or confidence < 0.4:  # Adjust threshold as needed\n",
    "            console.print(\"[yellow]Retrieved documents insufficient. Using LLM fallback...[/yellow]\")\n",
    "            return self.generate_llm_answer(question)\n",
    "        \n",
    "        # Otherwise, continue with RAG as before\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "                \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful scientific research assistant analyzing scientific papers. \n",
    "            Use the following retrieved context chunks to answer the user's question thoroughly.\n",
    "            \n",
    "            Guidelines:\n",
    "            - Focus on providing accurate information from the papers\n",
    "            - Synthesize information across documents when appropriate\n",
    "            - Cite the sources of information in your answer (e.g., \"According to Document 1...\")\n",
    "            - If the information isn't in the context, indicate what's missing rather than making up information\n",
    "            - If the context contains conflicting information, highlight the disagreement and possible reasons\n",
    "            - Maintain scientific accuracy above all else\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Format document content with metadata\n",
    "        context_pieces = []\n",
    "        for i, doc in enumerate(docs[:max_docs]):\n",
    "            chunk = f\"Document {i+1} [Relevance: {doc.score:.2f}]:\\n{doc.document.page_content}\"\n",
    "            \n",
    "            # Add metadata if available\n",
    "            if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                source = doc.document.metadata.get('source', 'Unknown')\n",
    "                chunk += f\"\\nSource: {source}\"\n",
    "                \n",
    "            context_pieces.append(chunk)\n",
    "            \n",
    "        context = \"\\n\\n---\\n\\n\".join(context_pieces)\n",
    "        \n",
    "        try:\n",
    "            # Generate answer\n",
    "            answer = (prompt | self.llm).invoke({\n",
    "                \"question\": question, \n",
    "                \"context\": context,\n",
    "                \"chat_history\": chat_context\n",
    "            })\n",
    "            \n",
    "            # Save the QA pair to memory\n",
    "            self.memory.chat_memory.add_messages([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=answer)\n",
    "            ])\n",
    "            \n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating answer: {e}\")\n",
    "            return \"I'm sorry, I encountered an error while generating your answer. Please try rephrasing your question.\"\n",
    "    \n",
    "    def generate_combined_answer(\n",
    "        self, \n",
    "        question: str, \n",
    "        query_results: Dict[str, List[RetrievedDocument]]\n",
    "    ) -> str:\n",
    "        \"\"\"Generates a combined answer from multiple query results with LLM fallback.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            query_results: Dictionary mapping queries to retrieved documents\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Combine all relevant documents from different queries\n",
    "        all_docs = []\n",
    "        for query, docs in query_results.items():\n",
    "            all_docs.extend(docs[:self.max_docs_per_query])\n",
    "        \n",
    "        # Evaluate if the documents are sufficient\n",
    "        is_sufficient, confidence = self.evaluate_document_relevance(question, all_docs)\n",
    "        \n",
    "        # If documents are insufficient, use LLM fallback\n",
    "        if not is_sufficient or confidence < 0.2:  # Adjust threshold as needed\n",
    "            console.print(\"[yellow]Retrieved documents insufficient. Using LLM fallback...[/yellow]\")\n",
    "            return self.generate_llm_answer(question)\n",
    "        \n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        # Combine all context sections\n",
    "        all_context_sections = []\n",
    "        \n",
    "        for query, docs in query_results.items():\n",
    "            # Use only the top N documents for each query\n",
    "            docs_for_query = docs[:self.max_docs_per_query]\n",
    "            if docs_for_query:\n",
    "                all_context_sections.append(f\"Results for query: '{query}'\")\n",
    "                for i, doc in enumerate(docs_for_query, 1):\n",
    "                    section = f\"Document {i} [Relevance: {doc.score:.2f}]:\\n{doc.document.page_content}\"\n",
    "                    \n",
    "                    # Add metadata if available\n",
    "                    if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                        source = doc.document.metadata.get('source', 'Unknown')\n",
    "                        section += f\"\\nSource: {source}\"\n",
    "                        \n",
    "                    all_context_sections.append(section)\n",
    "        \n",
    "        # Join all context sections\n",
    "        combined_context = \"\\n\\n---\\n\\n\".join(all_context_sections)\n",
    "        \n",
    "        # Create a prompt that emphasizes synthesizing information across different query results\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful scientific research assistant analyzing scientific papers. \n",
    "            The user's question has been reformulated in several ways, and each formulation returned different documents.\n",
    "            \n",
    "            Guidelines:\n",
    "            - Synthesize information across ALL retrieved documents to provide a comprehensive answer\n",
    "            - Compare and contrast findings from different sources\n",
    "            - Highlight the most relevant information from each source\n",
    "            - Cite the specific documents you're referencing (e.g., \"According to the paper in Document 3...\")\n",
    "            - If information conflicts across sources, explain the different perspectives\n",
    "            - If the information isn't in the context, indicate what's missing\n",
    "            - Maintain scientific accuracy above all else\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Context from multiple query formulations:\n",
    "            {context}\n",
    "            \n",
    "            Original Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Generate answer\n",
    "            answer = (prompt | self.llm).invoke({\n",
    "                \"question\": question, \n",
    "                \"context\": combined_context,\n",
    "                \"chat_history\": chat_context\n",
    "            })\n",
    "            \n",
    "            # Save the QA pair to memory\n",
    "            self.memory.chat_memory.add_messages([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=answer)\n",
    "            ])\n",
    "            \n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating combined answer: {e}\")\n",
    "            return \"I'm sorry, I encountered an error while generating your answer. Please try rephrasing your question.\"\n",
    "            \n",
    "    def evaluate_document_relevance(self, question: str, docs: List[RetrievedDocument]) -> Tuple[bool, float]:\n",
    "        \"\"\"Evaluates whether documents are sufficient to answer the question.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            docs: List of retrieved documents\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (is_sufficient, confidence_score)\n",
    "        \"\"\"\n",
    "        if not docs:\n",
    "            return False, 0.0\n",
    "            \n",
    "        # If all docs have very low scores, they're likely not relevant\n",
    "        avg_score = sum(doc.score for doc in docs) / len(docs)\n",
    "        \n",
    "        # Create a prompt to evaluate document relevance\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are evaluating whether a set of retrieved documents contains information \n",
    "        to answer a user's question.\n",
    "        \n",
    "        User Question: {question}\n",
    "        \n",
    "        Retrieved Information:\n",
    "        {doc_summaries}\n",
    "        \n",
    "        Please analyze whether the retrieved documents contain enough information to answer the question.\n",
    "        Respond with a JSON object:\n",
    "        {{\n",
    "          \"is_sufficient\": true/false,\n",
    "          \"confidence\": <number between 0-1>,\n",
    "          \"explanation\": \"<brief explanation>\"\n",
    "        }}\n",
    "        \n",
    "        Only respond with the JSON object.\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create short summaries of each document\n",
    "        doc_summaries = []\n",
    "        for i, doc in enumerate(docs[:3]):  # Use top 3 docs for evaluation\n",
    "            summary = doc.document.page_content[:200] + \"...\" if len(doc.document.page_content) > 200 else doc.document.page_content\n",
    "            doc_summaries.append(f\"Document {i+1} [Score: {doc.score:.2f}]: {summary}\")\n",
    "        \n",
    "        doc_summaries_text = \"\\n\\n\".join(doc_summaries)\n",
    "        \n",
    "        try:\n",
    "            # Parse the LLM response as JSON\n",
    "            response = self.llm.invoke(\n",
    "                prompt.format(question=question, doc_summaries=doc_summaries_text)\n",
    "            )\n",
    "            \n",
    "            # Extract JSON from the response\n",
    "            json_match = re.search(r'\\{.*\\}', response.replace('\\n', ' '), re.DOTALL)\n",
    "            if json_match:\n",
    "                result = json.loads(json_match.group(0))\n",
    "                is_sufficient = result.get(\"is_sufficient\", False)\n",
    "                confidence = result.get(\"confidence\", 0.0)\n",
    "                \n",
    "                logger.info(f\"Document relevance: Sufficient={is_sufficient}, Confidence={confidence}\")\n",
    "                logger.info(f\"Explanation: {result.get('explanation', 'No explanation provided')}\")\n",
    "                \n",
    "                return is_sufficient, confidence\n",
    "            else:\n",
    "                # If JSON parsing fails, use heuristics based on average score\n",
    "                return avg_score > self.relevance_threshold, avg_score\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating document relevance: {e}\")\n",
    "            # Fall back to using avg score\n",
    "            return avg_score > self.relevance_threshold, avg_score\n",
    "\n",
    "    def generate_llm_answer(self, question: str) -> str:\n",
    "        \"\"\"Generates an answer directly from the LLM when documents aren't sufficient.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful scientific research assistant. \n",
    "            \n",
    "            The user has asked a question, but we couldn't find relevant documents in our scientific database.\n",
    "            Since this appears to be a question you can answer from your general knowledge, please provide\n",
    "            a helpful response.\n",
    "            \n",
    "            Guidelines:\n",
    "            - Be clear that you're answering from general knowledge rather than specific papers\n",
    "            - Provide accurate, factual information\n",
    "            - If the question is about a very specialized scientific topic that requires references to papers,\n",
    "              indicate that you lack specific citations but can provide general information\n",
    "            - If it's a basic concept, provide a thorough explanation\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Generate answer\n",
    "            answer = (prompt | self.llm).invoke({\n",
    "                \"question\": question,\n",
    "                \"chat_history\": chat_context\n",
    "            })\n",
    "            \n",
    "            # Save the QA pair to memory\n",
    "            self.memory.chat_memory.add_messages([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=answer)\n",
    "            ])\n",
    "            \n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating LLM answer: {e}\")\n",
    "            return \"I'm sorry, I couldn't find relevant information in my scientific database and encountered an error trying to provide a general answer. Please try rephrasing your question.\"\n",
    "    \n",
    "    def ask_with_memory(self, question: str, num_results: int = 5) -> str:\n",
    "        \"\"\"Uses conversation memory to process follow-up questions.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            num_results: Number of results to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # First, check if this is a question we can answer directly from memory\n",
    "            can_direct_answer, direct_answer = self.can_answer_without_retrieval(question)\n",
    "            if can_direct_answer:\n",
    "                console.print(\"[bold green]Question can be answered directly from conversation history...[/bold green]\")\n",
    "                # Save the QA pair to memory manually\n",
    "                self.memory.chat_memory.add_messages([\n",
    "                    HumanMessage(content=question),\n",
    "                    AIMessage(content=direct_answer)\n",
    "                ])\n",
    "                return direct_answer\n",
    "# Otherwise, use the full query improvement pipeline\n",
    "            \n",
    "            # Step 1: Rate the query\n",
    "            console.print(\"\\n[bold cyan]Evaluating your query (with memory context)...[/bold cyan]\")\n",
    "            rating_result = self.rate_query(question)\n",
    "            \n",
    "            rating_color = \"green\" if rating_result['rating'] >= 4 else \"yellow\" if rating_result['rating'] >= 3 else \"red\"\n",
    "            console.print(f\"[bold {rating_color}]Query Rating: {rating_result['rating']}/5[/bold {rating_color}]\")\n",
    "            console.print(f\"Explanation: {rating_result['explanation']}\")\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 2: Suggest rewrites if the rating is less than perfect\n",
    "            rewritten_queries = []\n",
    "            if rating_result[\"rating\"] < 5:\n",
    "                console.print(\"[bold cyan]Generating improved query variations...[/bold cyan]\")\n",
    "                # Get chat history for context in rewrites\n",
    "                chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "                rewritten_queries = self.suggest_rewrites(question, chat_history)\n",
    "                console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 3: Present options to the user\n",
    "            query_options = self.present_query_options(question, rewritten_queries)\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 4: Get user selection\n",
    "            console.print(\"[bold cyan]Select queries to use:[/bold cyan]\")\n",
    "            console.print(\"Enter the numbers of the queries you want to use (comma-separated, e.g., '0,2,3')\")\n",
    "            selected_indices_input = input(\"> \")\n",
    "            \n",
    "            try:\n",
    "                selected_indices = [int(idx.strip()) for idx in selected_indices_input.split(\",\")]\n",
    "            except ValueError:\n",
    "                console.print(\"[bold red]Invalid input. Using original query only.[/bold red]\")\n",
    "                selected_indices = [0]  # Default to original query\n",
    "            \n",
    "            # Get the selected queries\n",
    "            selected_queries = []\n",
    "            for idx in selected_indices:\n",
    "                if idx == 0:  # Original query\n",
    "                    selected_queries.append(question)\n",
    "                elif 0 < idx <= len(rewritten_queries):  # Rewritten query\n",
    "                    query_text = rewritten_queries[idx-1]\n",
    "                    selected_queries.append(query_text)\n",
    "            \n",
    "            if not selected_queries:\n",
    "                console.print(\"[bold red]No valid queries selected. Using original query.[/bold red]\")\n",
    "                selected_queries = [question]\n",
    "                \n",
    "            console.print(f\"\\n[bold green]Selected {len(selected_queries)} queries for retrieval.[/bold green]\")\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 5: Retrieve documents\n",
    "            console.print(\"[bold cyan]Retrieving relevant documents...[/bold cyan]\")\n",
    "            \n",
    "            if len(selected_queries) > 1:\n",
    "                # If multiple queries were selected, use the multi-query approach\n",
    "                docs, query_docs_map = self.retrieve_documents(selected_queries, k=num_results)\n",
    "                \n",
    "                # Generate a combined answer\n",
    "                console.print(\"\\n[bold cyan]Generating combined answer with conversation+docs context...[/bold cyan]\")\n",
    "                answer = self.generate_combined_answer(question, query_docs_map)\n",
    "            else:\n",
    "                # If only one query was selected, use the standard approach\n",
    "                try:\n",
    "                    retrieved_docs = []\n",
    "                    results = self.vector_db.similarity_search_with_relevance_scores(selected_queries[0], k=num_results)\n",
    "                    \n",
    "                    for i, (doc, score) in enumerate(results, 1):\n",
    "                        retrieved_docs.append(RetrievedDocument(\n",
    "                            document=doc,\n",
    "                            score=score,\n",
    "                            query=selected_queries[0],\n",
    "                            rank=i\n",
    "                        ))\n",
    "                    \n",
    "                    console.print(f\"[bold green]Retrieved {len(retrieved_docs)} documents[/bold green]\")\n",
    "                    \n",
    "                    # Show previews of the retrieved documents\n",
    "                    console.print(\"\\n[bold cyan]Top retrieved documents:[/bold cyan]\")\n",
    "                    for i, doc in enumerate(retrieved_docs[:5], 1):\n",
    "                        score_color = \"green\" if doc.score > 0.8 else \"yellow\" if doc.score > 0.6 else \"red\"\n",
    "                        console.print(f\"{i}. [bold {score_color}]Score: {doc.score:.4f}[/bold {score_color}]\")\n",
    "                        \n",
    "                        preview = doc.document.page_content[:150] + \"...\" if len(doc.document.page_content) > 150 else doc.document.page_content\n",
    "                        console.print(f\"   Preview: {preview}\")\n",
    "                        \n",
    "                        if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                            console.print(f\"   Source: {doc.document.metadata.get('source', 'Unknown')}\")\n",
    "                    \n",
    "                    # Generate answer with memory context\n",
    "                    console.print(\"\\n[bold cyan]Generating answer with conversation + doc context...[/bold cyan]\")\n",
    "                    answer = self.generate_final_answer(question, retrieved_docs)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in retrieval: {e}\")\n",
    "                    return f\"I'm sorry, I encountered an error while retrieving documents: {str(e)}\"\n",
    "            \n",
    "            return answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in ask_with_memory: {e}\")\n",
    "            return f\"I'm sorry, I encountered an error while processing your question: {str(e)}\"\n",
    "\n",
    "    def search_with_query_feedback(self, query: str, num_results: int = 5) -> str:\n",
    "        \"\"\"Main pipeline that processes a query and returns an answer.\"\"\"\n",
    "        # Step 1: Rate the query\n",
    "        console.print(\"\\n[bold cyan]Evaluating your query...[/bold cyan]\")\n",
    "        rating_result = self.rate_query(query)\n",
    "            \n",
    "        rating_color = \"green\" if rating_result['rating'] >= 4 else \"yellow\" if rating_result['rating'] >= 3 else \"red\"\n",
    "        console.print(f\"[bold {rating_color}]Query Rating: {rating_result['rating']}/5[/bold {rating_color}]\")\n",
    "        console.print(f\"Explanation: {rating_result['explanation']}\")\n",
    "        console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "        # Step 2: Suggest rewrites if the rating is less than perfect\n",
    "        rewritten_queries = []\n",
    "        if rating_result[\"rating\"] < 5:\n",
    "            console.print(\"[bold cyan]Generating improved query variations...[/bold cyan]\")\n",
    "            rewritten_queries = self.suggest_rewrites(query)\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 3: Present options to the user\n",
    "        query_options = self.present_query_options(query, rewritten_queries)\n",
    "        console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 4: Get user selection\n",
    "        console.print(\"[bold cyan]Select queries to use:[/bold cyan]\")\n",
    "        console.print(\"Enter the numbers of the queries you want to use (comma-separated, e.g., '0,2,3')\")\n",
    "        selected_indices_input = input(\"> \")\n",
    "            \n",
    "        try:\n",
    "            selected_indices = [int(idx.strip()) for idx in selected_indices_input.split(\",\")]\n",
    "        except ValueError:\n",
    "            console.print(\"[bold red]Invalid input. Using original query only.[/bold red]\")\n",
    "            selected_indices = [0]  # Default to original query\n",
    "        \n",
    "        # Get the selected queries\n",
    "        selected_queries = []\n",
    "        for idx in selected_indices:\n",
    "            if idx == 0:  # Original query\n",
    "                selected_queries.append(query)\n",
    "            elif 0 < idx <= len(rewritten_queries):  # Rewritten query\n",
    "                query_text = rewritten_queries[idx-1]\n",
    "                selected_queries.append(query_text)\n",
    "                \n",
    "        if not selected_queries:\n",
    "            console.print(\"[bold red]No valid queries selected. Using original query.[/bold red]\")\n",
    "            selected_queries = [query]\n",
    "            \n",
    "        console.print(f\"\\n[bold green]Selected {len(selected_queries)} queries for retrieval.[/bold green]\")\n",
    "        console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 5: Retrieve documents\n",
    "        console.print(\"[bold cyan]Retrieving relevant documents...[/bold cyan]\")\n",
    "        \n",
    "        if len(selected_queries) > 1:\n",
    "            # If multiple queries were selected, use the multi-query approach\n",
    "            print(\"LOG123: multiple queries this is working\")\n",
    "            docs, query_docs_map = self.retrieve_documents(selected_queries, k=num_results)\n",
    "            \n",
    "            # Generate a combined answer\n",
    "            console.print(\"\\n[bold cyan]Generating combined answer with documents context...[/bold cyan]\")\n",
    "            answer = self.generate_combined_answer(query, query_docs_map)\n",
    "        else:\n",
    "            # If only one query was selected, use the standard approach\n",
    "            print(\"LOG123: only this is working\")\n",
    "            try:\n",
    "                retrieved_docs = []\n",
    "                results = self.vector_db.similarity_search_with_relevance_scores(selected_queries[0], k=num_results)\n",
    "                \n",
    "                for i, (doc, score) in enumerate(results, 1):\n",
    "                    retrieved_docs.append(RetrievedDocument(\n",
    "                        document=doc,\n",
    "                        score=score,\n",
    "                        query=selected_queries[0],\n",
    "                        rank=i\n",
    "                    ))\n",
    "                \n",
    "                console.print(f\"[bold green]Retrieved {len(retrieved_docs)} documents[/bold green]\")\n",
    "                \n",
    "                # Show previews of the retrieved documents\n",
    "                console.print(\"\\n[bold cyan]Top retrieved documents:[/bold cyan]\")\n",
    "                for i, doc in enumerate(retrieved_docs[:5], 1):\n",
    "                    score_color = \"green\" if doc.score > 0.8 else \"yellow\" if doc.score > 0.6 else \"red\"\n",
    "                    console.print(f\"{i}. [bold {score_color}]Score: {doc.score:.4f}[/bold {score_color}]\")\n",
    "                    \n",
    "                    preview = doc.document.page_content[:150] + \"...\" if len(doc.document.page_content) > 150 else doc.document.page_content\n",
    "                    console.print(f\"   Preview: {preview}\")\n",
    "                    \n",
    "                    if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                        console.print(f\"   Source: {doc.document.metadata.get('source', 'Unknown')}\")\n",
    "                \n",
    "                # Generate answer with memory context\n",
    "                console.print(\"\\n[bold cyan]Generating answer with document context...[/bold cyan]\")\n",
    "                answer = self.generate_final_answer(query, retrieved_docs)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in retrieval: {e}\")\n",
    "                return f\"I'm sorry, I encountered an error while retrieving documents: {str(e)}\"\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def process_query(self, query: str, use_memory: bool = False, num_results: int = 5) -> str:\n",
    "        \"\"\"Main entry point that decides whether to use memory or the full pipeline.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            use_memory: Whether to explicitly use memory mode\n",
    "            num_results: Number of results to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Check if this might be a follow-up question that should use memory\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        should_use_memory = use_memory or (chat_history and self._is_likely_followup(query))\n",
    "        print(\"should_use_memory\",should_use_memory)\n",
    "        if should_use_memory:\n",
    "            console.print(\"[bold cyan]Using conversation memory to process this query...[/bold cyan]\")\n",
    "            return self.ask_with_memory(query, num_results)\n",
    "        else:\n",
    "            console.print(\"[bold cyan]Using full query improvement pipeline...[/bold cyan]\")\n",
    "            return self.search_with_query_feedback(query, num_results)\n",
    "    \n",
    "    def _is_likely_followup(self, query: str) -> bool:\n",
    "        \"\"\"Heuristically determines if a query is likely a follow-up question.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating if this is likely a follow-up\n",
    "        \"\"\"\n",
    "        # Look for pronouns, references, and questions that seem to refer to previous context\n",
    "        followup_indicators = [\n",
    "            # Pronouns\n",
    "            \"it\", \"this\", \"that\", \"they\", \"them\", \"these\", \"those\",\n",
    "            # Reference terms\n",
    "            \"previous\", \"earlier\", \"above\", \"mentioned\", \"last\", \"former\",\n",
    "            # Follow-up phrases\n",
    "            \"what about\", \"how about\", \"tell me more\", \"elaborate\", \"clarify\", \"explain further\",\n",
    "            \"can you expand\", \"additionally\", \"furthermore\", \"also\", \"related to that\",\n",
    "            # Short questions that likely need context\n",
    "            \"why\", \"how does\", \"can you explain\", \"what does\", \"what is\", \"who is\"\n",
    "        ]\n",
    "        \n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Check for any of the indicators\n",
    "        if any(indicator in query_lower for indicator in followup_indicators):\n",
    "            return True\n",
    "            \n",
    "        # # Check for very short queries (likely follow-ups)\n",
    "        # if len(query.split()) < 4:\n",
    "        #     return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    def clear_memory(self) -> None:\n",
    "        \"\"\"Clears the conversation memory.\"\"\"\n",
    "        self.memory.clear()\n",
    "        console.print(\"[bold green]Conversation memory cleared.[/bold green]\")\n",
    "        \n",
    "    def get_chat_history(self) -> str:\n",
    "        \"\"\"Returns the formatted chat history.\n",
    "        \n",
    "        Returns:\n",
    "            String representation of chat history\n",
    "        \"\"\"\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        if not chat_history:\n",
    "            return \"No conversation history.\"\n",
    "            \n",
    "        history_str = \"\"\n",
    "        for i, message in enumerate(chat_history):\n",
    "            if hasattr(message, \"content\"):\n",
    "                role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                history_str += f\"{role}: {message.content}\\n\\n\"\n",
    "                \n",
    "        return history_str\n",
    "\n",
    "    def Load_query(self, query):\n",
    "        \"\"\"\n",
    "        Load research papers from arXiv based on the given query and store them in the temporary database.\n",
    "        Returns the top 3 papers information.\n",
    "        \"\"\"\n",
    "        console.print(f\"[bold cyan]Searching arXiv for papers related to:[/bold cyan] {query}\")\n",
    "        \n",
    "        try:\n",
    "            # Use the working ArxivLoader from langchain_community\n",
    "            loader = ArxivLoader(\n",
    "                query=query,\n",
    "                load_max_docs=3,  # Get top 3 papers\n",
    "                load_all_available_meta=True\n",
    "            )\n",
    "            \n",
    "            documents = loader.load()\n",
    "            console.print(f\"[green]Found {len(documents)} relevant papers[/green]\")\n",
    "            \n",
    "            # Download PDFs for more complete content\n",
    "            enhanced_docs = self.download_pdf_content(documents)\n",
    "            \n",
    "            # Store documents in temporary database\n",
    "            paper_ids = self.StoreInDB(enhanced_docs)\n",
    "            \n",
    "            # Display basic information about the loaded papers\n",
    "            self.display_paper_summaries(enhanced_docs)\n",
    "            \n",
    "            return paper_ids\n",
    "        \n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Error loading papers: {str(e)}[/bold red]\")\n",
    "            # # Fall back to simulated papers if needed\n",
    "            # if self.allow_fallback:\n",
    "            #     console.print(\"[yellow]Using fallback data for demonstration purposes.[/yellow]\")\n",
    "            #     return self.create_fallback_papers(query)\n",
    "            return []\n",
    "    \n",
    "    def download_pdf_content(self, documents):\n",
    "        \"\"\"\n",
    "        Enhance documents by downloading the full PDF content when available.\n",
    "        \"\"\"\n",
    "        enhanced_docs = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            try:\n",
    "                # Get metadata\n",
    "                metadata = doc.metadata\n",
    "                title = metadata.get(\"Title\", \"Untitled\")\n",
    "                console.print(f\"[cyan]Processing paper: {title}[/cyan]\")\n",
    "                \n",
    "                # Try to find arXiv ID from page content\n",
    "                arxiv_id = None\n",
    "                match = re.search(r'arXiv:(\\d{4}\\.\\d{5})', doc.page_content)\n",
    "                \n",
    "                if match:\n",
    "                    arxiv_id = match.group(1)\n",
    "                else:\n",
    "                    # Try to extract from Entry ID\n",
    "                    entry_id = metadata.get(\"Entry ID\", \"\")\n",
    "                    id_match = re.search(r'abs/(\\d{4}\\.\\d{5})', entry_id)\n",
    "                    if id_match:\n",
    "                        arxiv_id = id_match.group(1)\n",
    "                \n",
    "                if arxiv_id:\n",
    "                    # Generate PDF URL\n",
    "                    pdf_url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "                    console.print(f\"[cyan]Found PDF URL: {pdf_url}[/cyan]\")\n",
    "                    \n",
    "                    # Create temp directory if it doesn't exist\n",
    "                    temp_dir = \"./temp_pdfs\"\n",
    "                    os.makedirs(temp_dir, exist_ok=True)\n",
    "                    \n",
    "                    # Download PDF\n",
    "                    pdf_filename = f\"{temp_dir}/{arxiv_id}.pdf\"\n",
    "                    \n",
    "                    # Check if file already exists\n",
    "                    if not os.path.exists(pdf_filename):\n",
    "                        console.print(f\"[cyan]Downloading PDF...[/cyan]\")\n",
    "                        response = requests.get(pdf_url, timeout=30)\n",
    "                        with open(pdf_filename, 'wb') as f:\n",
    "                            f.write(response.content)\n",
    "                    \n",
    "                    # Load PDF content\n",
    "                    console.print(f\"[cyan]Extracting content from PDF...[/cyan]\")\n",
    "                    pdf_loader = PyPDFLoader(pdf_filename)\n",
    "                    pdf_docs = pdf_loader.load()\n",
    "                    \n",
    "                    # Create enhanced document with PDF content\n",
    "                    full_content = doc.page_content + \"\\n\\n\" + \"\\n\\n\".join([pdf_doc.page_content for pdf_doc in pdf_docs])\n",
    "                    \n",
    "                    # Update metadata with PDF info\n",
    "                    updated_metadata = metadata.copy()\n",
    "                    updated_metadata[\"pdf_url\"] = pdf_url\n",
    "                    updated_metadata[\"pdf_path\"] = pdf_filename\n",
    "                    updated_metadata[\"pdf_pages\"] = len(pdf_docs)\n",
    "                    \n",
    "                    enhanced_doc = Document(page_content=full_content, metadata=updated_metadata)\n",
    "                    enhanced_docs.append(enhanced_doc)\n",
    "                    \n",
    "                else:\n",
    "                    # If PDF can't be found, use original document\n",
    "                    console.print(f\"[yellow]Could not extract arXiv ID for {title}. Using abstract only.[/yellow]\")\n",
    "                    enhanced_docs.append(doc)\n",
    "            \n",
    "            except Exception as e:\n",
    "                console.print(f\"[yellow]Error enhancing document: {str(e)}. Using abstract only.[/yellow]\")\n",
    "                enhanced_docs.append(doc)\n",
    "        \n",
    "        return enhanced_docs\n",
    "    \n",
    "    def StoreInDB(self, documents):\n",
    "        \"\"\"\n",
    "        Store loaded documents in the temporary vector database.\n",
    "        Returns list of document IDs.\n",
    "        \"\"\"\n",
    "        paper_ids = []\n",
    "        \n",
    "        console.print(\"[bold cyan]Storing papers in temporary database...[/bold cyan]\")\n",
    "        \n",
    "        try:\n",
    "            # Split documents into chunks for better retrieval\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000, \n",
    "                chunk_overlap=200\n",
    "            )\n",
    "            \n",
    "            for doc in documents:\n",
    "                # Create document chunks\n",
    "                chunks = text_splitter.split_documents([doc])\n",
    "                \n",
    "                # Create unique ID for the paper\n",
    "                paper_id = f\"paper_{uuid.uuid4().hex[:8]}\"\n",
    "                paper_ids.append(paper_id)\n",
    "                \n",
    "                # Add metadata to each chunk and sanitize it\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    # Create clean metadata dictionary\n",
    "                    clean_metadata = {\n",
    "                        \"paper_id\": paper_id,\n",
    "                        \"chunk_id\": i,\n",
    "                        \"title\": doc.metadata.get(\"Title\", \"Untitled\"),\n",
    "                        \"authors\": doc.metadata.get(\"Authors\", \"Unknown\"),\n",
    "                        \"published\": doc.metadata.get(\"Published\", \"Unknown\")\n",
    "                    }\n",
    "                    \n",
    "                    # Ensure all values are valid types (str, int, float, bool)\n",
    "                    for key, value in clean_metadata.items():\n",
    "                        if value is None:\n",
    "                            clean_metadata[key] = \"None\"  # Convert None to string\n",
    "                        elif not isinstance(value, (str, int, float, bool)):\n",
    "                            clean_metadata[key] = str(value)  # Convert complex types to string\n",
    "                    \n",
    "                    # Replace existing metadata with clean version\n",
    "                    chunk.metadata = clean_metadata\n",
    "                \n",
    "                # Add to vector database\n",
    "                self.vector_db.add_documents(chunks)\n",
    "            \n",
    "            # Persist the temporary database\n",
    "            self.vector_db.persist()\n",
    "            console.print(f\"[green]Successfully stored {len(documents)} papers in database[/green]\")\n",
    "            # Fixed typo from 'onsole' to 'console' and removed debug line\n",
    "            return paper_ids\n",
    "        \n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Error storing documents: {str(e)}[/bold red]\")\n",
    "            return []\n",
    "    \n",
    "    def display_paper_summaries(self, documents):\n",
    "        \"\"\"Display a summary of each loaded paper.\"\"\"\n",
    "        console.print(\"\\n[bold cyan]Loaded Papers:[/bold cyan]\")\n",
    "        \n",
    "        for i, doc in enumerate(documents):\n",
    "            title = doc.metadata.get(\"Title\", \"Untitled\")\n",
    "            authors = doc.metadata.get(\"Authors\", \"Unknown\")\n",
    "            published = doc.metadata.get(\"Published\", \"Unknown\")\n",
    "            \n",
    "            console.print(f\"\\n[bold]{i+1}. {title}[/bold]\")\n",
    "            console.print(f\"   Authors: {authors}\")\n",
    "            console.print(f\"   Published: {published}\")\n",
    "            \n",
    "            # Generate a brief summary of the paper\n",
    "            summary = self.generate_paper_summary(doc)\n",
    "            console.print(f\"   Summary: {summary}\")\n",
    "    \n",
    "    def generate_paper_summary(self, document):\n",
    "        \"\"\"Generate a concise summary of a paper using the LLM.\"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            Generate a concise 5-sentence summary of the following research paper:\n",
    "            \n",
    "            Title: {document.metadata.get('Title', 'Untitled')}\n",
    "            Abstract: {document.page_content[:500]}...\n",
    "            \n",
    "            Summary:\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.llm.predict(prompt)\n",
    "            return response.strip()\n",
    "        except:\n",
    "            return \"Summary not available.\"\n",
    "    \n",
    "    def query_temp_database(self, query, paper_ids=None, k=5):\n",
    "        \"\"\"\n",
    "        Query the temporary database with the given query.\n",
    "        \n",
    "        Args:\n",
    "            query: The query string\n",
    "            paper_ids: Optional list of paper IDs to restrict the search to\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of relevant document chunks\n",
    "        \"\"\"\n",
    "        console.print(f\"[bold cyan]Searching temporary database for:[/bold cyan] {query}\")\n",
    "        \n",
    "        try:\n",
    "            # Create filter for specific papers if provided\n",
    "            filter_dict = None\n",
    "            if paper_ids:\n",
    "                filter_dict = {\"paper_id\": {\"$in\": paper_ids}}\n",
    "            \n",
    "            # Query the vector database\n",
    "            search_results = self.vector_db.similarity_search(\n",
    "                query=query,\n",
    "                k=k,\n",
    "                filter=filter_dict\n",
    "            )\n",
    "            \n",
    "            console.print(f\"[green]Found {len(search_results)} relevant sections[/green]\")\n",
    "            return search_results\n",
    "        \n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Error querying database: {str(e)}[/bold red]\")\n",
    "            return []\n",
    "    \n",
    "    def answer_research_question(self, query, context_docs=None):\n",
    "        \"\"\"\n",
    "        Answer a research question using context from the temporary database.\n",
    "        \n",
    "        Args:\n",
    "            query: The research question\n",
    "            context_docs: Optional pre-retrieved context documents\n",
    "            \n",
    "        Returns:\n",
    "            Detailed answer to the research question\n",
    "        \"\"\"\n",
    "        # If context not provided, retrieve it\n",
    "        if not context_docs:\n",
    "            context_docs = self.query_temp_database(query, k=5)\n",
    "        \n",
    "        if not context_docs:\n",
    "            return \"I couldn't find relevant information to answer your question.\"\n",
    "        \n",
    "        # Prepare context\n",
    "        context_text = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(context_docs)])\n",
    "        \n",
    "        # Prepare sources\n",
    "        sources = []\n",
    "        for doc in context_docs:\n",
    "            paper_title = doc.metadata.get(\"title\", \"Unknown Title\")\n",
    "            authors = doc.metadata.get(\"authors\", \"Unknown Authors\")\n",
    "            source = f\"- {paper_title} by {authors}\"\n",
    "            if source not in sources:\n",
    "                sources.append(source)\n",
    "        \n",
    "        # Generate the answer\n",
    "        prompt = f\"\"\"\n",
    "        You are a helpful research assistant. Answer the following question based on the provided research paper extracts.\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Here are relevant extracts from research papers:\n",
    "        {context_text}\n",
    "        \n",
    "        Provide a comprehensive answer, citing specific information from the papers.\n",
    "        Make sure to organize your response clearly and highlight key insights.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            answer = self.llm.predict(prompt)\n",
    "            \n",
    "            # Add sources at the end\n",
    "            final_answer = f\"{answer}\\n\\nSources:\\n\" + \"\\n\".join(sources)\n",
    "            return final_answer\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"Error generating answer: {str(e)}\"\n",
    "\n",
    "\n",
    "def main():    \n",
    "    \"\"\"Main function to run the research assistant.\"\"\"\n",
    "    try:\n",
    "        from langchain_community.llms import OpenAI\n",
    "        from langchain_community.vectorstores import Chroma\n",
    "        from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "        \n",
    "        # Check for API key\n",
    "        if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "            console.print(\"[bold red]ERROR: OPENAI_API_KEY environment variable not set.[/bold red]\")\n",
    "            console.print(f\"Please set your API key with: export OPENAI_API_KEY={os.environ['OPENAI_API_KEY']}\")\n",
    "            return\n",
    "            \n",
    "        # Load models and vector database\n",
    "        console.print(\"[bold cyan]Initializing SageRAG Research Assistant...[/bold cyan]\")\n",
    "        \n",
    "        console.print(\"Loading embedding model...\")\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        \n",
    "        console.print(\"Loading language model...\")\n",
    "        llm = OllamaLLM(model=\"llama3.2\")\n",
    "        \n",
    "        console.print(\"Connecting to vector database...\")\n",
    "\n",
    "        vector_db = Chroma(persist_directory= DB_DIR, embedding_function=embeddings)\n",
    "        \n",
    "        console.print(\"[bold green]Initialization complete![/bold green]\")\n",
    "        \n",
    "        # Initialize the research assistant\n",
    "        assistant = ResearchAssistant(llm=llm, vector_db=vector_db)\n",
    "        print(assistant.vector_db._collection.count())\n",
    "        # Display welcome message\n",
    "        console.print(Panel.fit(\n",
    "            Markdown(\"# SageRAG Research Assistant\\n\\nAsk questions about scientific papers in the database.\"),\n",
    "            title=\"Welcome\",\n",
    "            border_style=\"cyan\"\n",
    "        ))\n",
    "        \n",
    "        # Main interaction loop\n",
    "        while True:\n",
    "            console.print(\"\\n[bold cyan]Options:[/bold cyan]\")\n",
    "            console.print(\"1. Ask a question\")\n",
    "            console.print(\"2. View conversation history\")\n",
    "            console.print(\"3. Clear conversation history\")\n",
    "            console.print(\"4. Go for Advanced Research\")\n",
    "            console.print(\"5. Exit\")\n",
    "            \n",
    "            choice = input(\"\\nEnter your choice (1-5): \").strip()\n",
    "            \n",
    "            if choice == \"1\":\n",
    "                query = input(f\"\\n[bold]Enter your query:[/bold] \")\n",
    "                if not query.strip():\n",
    "                    console.print(f\"[yellow]Empty query. Please try again.[/yellow]\")\n",
    "                    continue\n",
    "                    \n",
    "                # Ask the user whether to use memory mode or full pipeline\n",
    "                use_memory_input = input(\"Use conversation memory? (y/n): \").lower()\n",
    "                use_memory = use_memory_input.startswith('y')\n",
    "                \n",
    "                # Process the query\n",
    "                console.print(f\"\\n[bold cyan]Processing your query...[/bold cyan]\")\n",
    "                answer = assistant.process_query(query, use_memory=use_memory)\n",
    "                \n",
    "                # Display the final answer nicely formatted\n",
    "                console.print(Panel(\n",
    "                    Markdown(answer),\n",
    "                    title=\"Answer\",\n",
    "                    border_style=\"green\",\n",
    "                    width=100\n",
    "                ))\n",
    "\n",
    "                console.print(\"\\n[bold cyan]Would you like to get an llm generated answer?[/bold cyan]\")\n",
    "                llm_answer = input(\"(y/n): \").lower()\n",
    "                llm_answer = llm_answer.startswith('y')\n",
    "                if(llm_answer):\n",
    "                    answer = assistant.generate_llm_answer(query)\n",
    "                \n",
    "                # Display the final answer nicely formatted\n",
    "                console.print(Panel(\n",
    "                    Markdown(answer),\n",
    "                    title=\"Answer\",\n",
    "                    border_style=\"green\",\n",
    "                    width=100\n",
    "                ))\n",
    "            \n",
    "                \n",
    "            elif choice == \"2\":\n",
    "                history = assistant.get_chat_history()\n",
    "                console.print(Panel(\n",
    "                    history if history else \"No conversation history.\",\n",
    "                    title=\"Conversation History\",\n",
    "                    border_style=\"blue\"\n",
    "                ))\n",
    "                \n",
    "            elif choice == \"3\":\n",
    "                assistant.clear_memory()\n",
    "\n",
    "            elif choice == \"4\":\n",
    "                query = input(f\"\\n[bold]Enter your query:[/bold] \")\n",
    "                if not query.strip():\n",
    "                    console.print(f\"[yellow]Empty query. Please try again.[/yellow]\")\n",
    "                    continue\n",
    "                \n",
    "                # Initialize the adv research assistant\n",
    "                vector_db_temp = Chroma(persist_directory= \"./tempDB\", embedding_function=embeddings)\n",
    "                advAssistant = ResearchAssistant(llm=llm, vector_db=vector_db_temp)\n",
    "                # Process the query\n",
    "                console.print(f\"\\n[bold cyan]Processing your query...[/bold cyan]\")\n",
    "                # Step 1: Rate the query\n",
    "                console.print(\"\\n[bold cyan]Evaluating your query (with memory context)...[/bold cyan]\")\n",
    "                rating_result = advAssistant.rate_query(query)\n",
    "                \n",
    "                rating_color = \"green\" if rating_result['rating'] >= 4 else \"yellow\" if rating_result['rating'] >= 3 else \"red\"\n",
    "                console.print(f\"[bold {rating_color}]Query Rating: {rating_result['rating']}/5[/bold {rating_color}]\")\n",
    "                console.print(f\"Explanation: {rating_result['explanation']}\")\n",
    "                console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "                \n",
    "                # Step 2: Suggest rewrites if the rating is less than perfect\n",
    "                rewritten_queries = []\n",
    "                if rating_result[\"rating\"] < 5:\n",
    "                    console.print(\"[bold cyan]Generating improved query variations...[/bold cyan]\")\n",
    "                    rewritten_queries = advAssistant.suggest_rewrites(query)\n",
    "                    console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "                \n",
    "                # Step 3: Present options to the user\n",
    "                query_options = advAssistant.present_query_options(query, rewritten_queries)\n",
    "                console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "                \n",
    "                # Step 4: Get user selection\n",
    "                console.print(\"[bold cyan]Select queries to use:[/bold cyan]\")\n",
    "                console.print(\"Enter the query you want to use (comma-separated, e.g., '0,2,3')\")\n",
    "                selected_index = input(\"> \")\n",
    "                try:\n",
    "                    idx = int(selected_index)\n",
    "                    if idx == 0:  # Original query\n",
    "                        selected_query = query\n",
    "                    elif 0 < idx <= len(rewritten_queries):  # Rewritten query\n",
    "                        selected_query = rewritten_queries[idx-1]\n",
    "                    else:\n",
    "                        console.print(\"[yellow]Invalid selection. Using original query.[/yellow]\")\n",
    "                        selected_query = query\n",
    "                except ValueError:\n",
    "                    console.print(\"[yellow]Invalid input. Using original query.[/yellow]\")\n",
    "                    selected_query = query\n",
    "                \n",
    "                # Step 5: Load papers based on selected query\n",
    "                paper_ids = advAssistant.Load_query(selected_query)\n",
    "                \n",
    "                if paper_ids:\n",
    "                    # Step 6: Interactive research loop\n",
    "                    console.print(advAssistant.vector_db)\n",
    "                    while True:\n",
    "                        research_question = input(\"\\n[bold]Ask a question about these papers (or type 'exit' to return):[/bold] \")\n",
    "                        if research_question.lower() in ['exit', 'quit', 'back']:\n",
    "                            break\n",
    "                        if not research_question.strip():\n",
    "                            console.print(f\"[yellow]Empty query. Please try again.[/yellow]\")\n",
    "                            continue\n",
    "                            \n",
    "                        # Ask the user whether to use memory mode or full pipeline\n",
    "                        use_memory_input = input(\"Use conversation memory? (y/n): \").lower()\n",
    "                        use_memory = use_memory_input.startswith('y')\n",
    "                        \n",
    "                        # Process the query\n",
    "                        console.print(f\"\\n[bold cyan]Processing your query...[/bold cyan]\")\n",
    "                        answer = advAssistant.process_query(research_question, use_memory=use_memory)\n",
    "                        \n",
    "                        # Display the final answer nicely formatted\n",
    "                        console.print(Panel(\n",
    "                            Markdown(answer),\n",
    "                            title=\"Answer\",\n",
    "                            border_style=\"green\",\n",
    "                            width=100\n",
    "                        ))\n",
    "                else:\n",
    "                    console.print(\"[yellow]No papers were loaded. Please try a different query.[/yellow]\")\n",
    "                            \n",
    "                print(advAssistant.vector_db._collection.count())\n",
    "            elif choice == \"5\":\n",
    "                console.print(\"[bold green]Thank you for using SageRAG Research Assistant. Goodbye![/bold green]\")\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                console.print(\"[yellow]Invalid choice. Please enter a number from 1-4.[/yellow]\")\n",
    "                \n",
    "    except ImportError as e:\n",
    "        console.print(f\"[bold red]Error: Missing required packages: {e}[/bold red]\")\n",
    "        console.print(\"Please install the required packages with pip:\")\n",
    "        console.print(\"pip install langchain langchain_community sentence-transformers rich openai chromadb\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]An unexpected error occurred: {e}[/bold red]\")\n",
    "        import traceback\n",
    "        console.print(traceback.format_exc())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d63484da-ba54-4e16-b709-3fe5a698a20d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'assistant' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(assistant\u001b[38;5;241m.\u001b[39mvector_db\u001b[38;5;241m.\u001b[39m_collection\u001b[38;5;241m.\u001b[39mcount())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'assistant' is not defined"
     ]
    }
   ],
   "source": [
    "print(assistant.vector_db._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c210793-86df-4964-83f0-6bfdcd142b4e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 20:47:27,133 - SageRAG - INFO - Initializing default embedding model\n",
      "2025-04-19 20:47:31,195 - SageRAG - INFO - Research Assistant initialized successfully\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter What is LLM in NLP?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Evaluating your query...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mEvaluating your query\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 20:47:41,148 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Query Rating: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">/</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mQuery Rating: \u001b[0m\u001b[1;32m4\u001b[0m\u001b[1;32m/\u001b[0m\u001b[1;32m5\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Explanation: The query is clear and concise, making it easy to understand the question. It contains specific \n",
       "technical terms <span style=\"font-weight: bold\">(</span>LLM<span style=\"font-weight: bold\">)</span> that are relevant to NLP, which should help retrieve accurate scientific content. However, it\n",
       "lacks specificity in terms of the context or application of LLM in NLP, which might lead to a broad search result \n",
       "set. Additionally, while it's not too vague, the query could be more effective with additional keywords to narrow \n",
       "down the search results.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Explanation: The query is clear and concise, making it easy to understand the question. It contains specific \n",
       "technical terms \u001b[1m(\u001b[0mLLM\u001b[1m)\u001b[0m that are relevant to NLP, which should help retrieve accurate scientific content. However, it\n",
       "lacks specificity in terms of the context or application of LLM in NLP, which might lead to a broad search result \n",
       "set. Additionally, while it's not too vague, the query could be more effective with additional keywords to narrow \n",
       "down the search results.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Generating improved query variations...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mGenerating improved query variations\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 20:47:43,686 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Available search queries:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mAvailable search queries:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">.</span> Original: What is LLM in NLP?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m0\u001b[0m\u001b[1m.\u001b[0m Original: What is LLM in NLP?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Confidence: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;32mConfidence: \u001b[0m\u001b[1;32m1.00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: **Using technical terminology for better vector matching**: Rephrase as <span style=\"color: #008000; text-decoration-color: #008000\">\"What is a Large Language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Model (LLM) in Natural Language Processing?\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m1\u001b[0m: **Using technical terminology for better vector matching**: Rephrase as \u001b[32m\"What is a Large Language \u001b[0m\n",
       "\u001b[32mModel \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in Natural Language Processing?\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.64</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.64\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: **Including key entities and relationships from the original query**: Rephrase as <span style=\"color: #008000; text-decoration-color: #008000\">\"What is the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">relationship between Large Language Models (LLMs), transformer architectures, and their applications in natural </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language generation and text classification?\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m2\u001b[0m: **Including key entities and relationships from the original query**: Rephrase as \u001b[32m\"What is the \u001b[0m\n",
       "\u001b[32mrelationship between Large Language Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, transformer architectures, and their applications in natural \u001b[0m\n",
       "\u001b[32mlanguage generation and text classification?\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.57</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.57\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: **Adding relevant synonyms or related concepts**: Rephrase as <span style=\"color: #008000; text-decoration-color: #008000\">\"What is a transformer-based neural </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">network architecture used for natural language generation and text classification, known colloquially as a Large </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Language Model (LLM)?\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m3\u001b[0m: **Adding relevant synonyms or related concepts**: Rephrase as \u001b[32m\"What is a transformer-based neural \u001b[0m\n",
       "\u001b[32mnetwork architecture used for natural language generation and text classification, known colloquially as a Large \u001b[0m\n",
       "\u001b[32mLanguage Model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m?\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.56</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.56\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>: **Varying syntax while preserving semantic meaning**: Rephrase as <span style=\"color: #008000; text-decoration-color: #008000\">\"Can you explain the theoretical </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">foundations behind Large Language Models (LLMs) in natural language processing?\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m4\u001b[0m: **Varying syntax while preserving semantic meaning**: Rephrase as \u001b[32m\"Can you explain the theoretical \u001b[0m\n",
       "\u001b[32mfoundations behind Large Language Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in natural language processing?\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.56</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.56\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>: **Breaking down complex queries into clearer formulations**: Rephrase as <span style=\"color: #008000; text-decoration-color: #008000\">\"Describe the application of</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Large Language Models (LLMs) in Natural Language Understanding\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m5\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m5\u001b[0m: **Breaking down complex queries into clearer formulations**: Rephrase as \u001b[32m\"Describe the application of\u001b[0m\n",
       "\u001b[32mLarge Language Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in Natural Language Understanding\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.50</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.50\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant = ResearchAssistant(llm=llm, vector_db=vector_db)\n",
    "query=input(\"enter\")\n",
    "# Step 1: Rate the query\n",
    "console.print(\"\\n[bold cyan]Evaluating your query...[/bold cyan]\")\n",
    "rating_result = assistant.rate_query(query)\n",
    "    \n",
    "rating_color = \"green\" if rating_result['rating'] >= 4 else \"yellow\" if rating_result['rating'] >= 3 else \"red\"\n",
    "console.print(f\"[bold {rating_color}]Query Rating: {rating_result['rating']}/5[/bold {rating_color}]\")\n",
    "console.print(f\"Explanation: {rating_result['explanation']}\")\n",
    "console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "# Step 2: Suggest rewrites if the rating is less than perfect\n",
    "rewritten_queries = []\n",
    "if rating_result[\"rating\"] < 5:\n",
    "    console.print(\"[bold cyan]Generating improved query variations...[/bold cyan]\")\n",
    "    rewritten_queries = assistant.suggest_rewrites(query)\n",
    "    console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "# Step 3: Present options to the user\n",
    "    query_options = assistant.present_query_options(query, rewritten_queries)\n",
    "    console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a417a99-a5cc-49e1-95b0-bb92e086b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "0,1,4\n",
    "What are various tools used in Machine Transltion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc9d2221-b31a-41f0-90ed-90f51994a0d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import this module to use the ResearchAssistant class\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, BaseOutputParser\n",
    "from langchain.schema import Document, HumanMessage, AIMessage\n",
    "from langchain.vectorstores import VectorStore\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms.base import BaseLLM\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from pydantic import BaseModel, Field\n",
    "import logging\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"SageRAG\")\n",
    "\n",
    "# Rich console for prettier output\n",
    "console = Console()\n",
    "\n",
    "# Enhanced output parsers with better error handling\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Parse output that contains a numbered list and return as a list of strings.\"\"\"\n",
    "    \n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        \"\"\"Parse text into a list of strings.\"\"\"\n",
    "        # Updated regex to be more robust with various numbering styles\n",
    "        lines = re.findall(r\"^\\s*\\d+\\.?\\s+(.*?)$\", text, re.MULTILINE)\n",
    "        return [line.strip() for line in lines if line.strip()]\n",
    "    \n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"line_list\"\n",
    "\n",
    "class QueryRating(BaseModel):\n",
    "    \"\"\"Schema for query rating output.\"\"\"\n",
    "    rating: int = Field(description=\"Rating from 1-5\")\n",
    "    explanation: str = Field(description=\"Explanation for the rating\")\n",
    "\n",
    "@dataclass\n",
    "class RetrievedDocument:\n",
    "    \"\"\"Dataclass for tracking retrieved document info.\"\"\"\n",
    "    document: Document\n",
    "    score: float\n",
    "    query: str = \"\"\n",
    "    rank: int = 0\n",
    "\n",
    "class ResearchAssistant:\n",
    "    \"\"\"AI Research Assistant using RAG for scientific paper queries.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        llm: BaseLLM, \n",
    "        vector_db: VectorStore, \n",
    "        embeddings_model: Optional[SentenceTransformer] = None,\n",
    "        relevance_threshold: float = 0.6,\n",
    "        max_docs_per_query: int = 3,\n",
    "        always_use_fallback: bool = True  # New parameter to control fallback behavior\n",
    "    ):\n",
    "        \"\"\"Initialize the Research Assistant with LLM and vector database.\n",
    "        \n",
    "        Args:\n",
    "            llm: The language model to use for generation\n",
    "            vector_db: Vector database for document retrieval\n",
    "            embeddings_model: Optional SentenceTransformer model for semantic similarity\n",
    "            relevance_threshold: Minimum relevance score for documents\n",
    "            max_docs_per_query: Maximum documents to use per query\n",
    "            always_use_fallback: Whether to always use LLM fallback for general knowledge questions\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.vector_db = vector_db\n",
    "        self.relevance_threshold = relevance_threshold\n",
    "        self.max_docs_per_query = max_docs_per_query\n",
    "        self.always_use_fallback = always_use_fallback\n",
    "        \n",
    "        # Initialize sentence transformer model for semantic similarity if not provided\n",
    "        if embeddings_model is None:\n",
    "            logger.info(\"Initializing default embedding model\")\n",
    "            self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        else:\n",
    "            self.semantic_model = embeddings_model\n",
    "            \n",
    "        # Initialize memory\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        # Create a retriever for direct use\n",
    "        self.retriever = vector_db.as_retriever()\n",
    "        \n",
    "        # Define parsers\n",
    "        self.json_parser = JsonOutputParser(pydantic_object=QueryRating)\n",
    "        self.list_parser = LineListOutputParser()\n",
    "        \n",
    "        logger.info(\"Research Assistant initialized successfully\")\n",
    "    \n",
    "    def rate_query(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Rates the query and gives an explanation.\n",
    "        \n",
    "        Args:\n",
    "            query: The user's query to be evaluated\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing rating and explanation\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an intelligent assistant trained to evaluate search queries for a scientific research database.\n",
    "        Given the following query: \"{query}\"\n",
    "        \n",
    "        1. Rate the query on a scale of 1 (very poor) to 5 (excellent) based on:\n",
    "           - Clarity: Is the query clear and unambiguous?\n",
    "           - Specificity: Does it contain specific technical terms or concepts?\n",
    "           - Relevance: Is it focused on retrieving scientific content?\n",
    "           - Retrievability: Will it work well with vector search?\n",
    "        \n",
    "        2. Provide a short explanation for the rating (what makes it effective or ineffective).\n",
    "        \n",
    "        Respond in JSON format:\n",
    "        {{\n",
    "          \"rating\": <number between 1-5>,\n",
    "          \"explanation\": \"<your explanation>\"\n",
    "        }}\n",
    "        \"\"\")\n",
    "        \n",
    "        chain: Runnable = prompt | self.llm | self.json_parser\n",
    "        \n",
    "        try:\n",
    "            return chain.invoke({\"query\": query})\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error rating query: {e}\")\n",
    "            # Fallback response if parsing fails\n",
    "            return {\n",
    "                \"rating\": 3, \n",
    "                \"explanation\": \"Unable to rate query properly. Consider adding more specific terms.\"\n",
    "            }\n",
    "    \n",
    "    def is_general_knowledge_question(self, query: str) -> bool:\n",
    "        \"\"\"Determines if a question is likely a general knowledge question rather than research-specific.\n",
    "        \n",
    "        Args:\n",
    "            query: The user's query to evaluate\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating if this is likely a general knowledge question\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an intelligent assistant analyzing a question. Determine if this is a general knowledge question\n",
    "        that doesn't require specific scientific papers to answer properly.\n",
    "        \n",
    "        Question: \"{query}\"\n",
    "        \n",
    "        Examples of general knowledge questions:\n",
    "        - What is an operating system?\n",
    "        - Who invented the telephone?\n",
    "        - How does gravity work?\n",
    "        - What is the difference between RAM and ROM?\n",
    "        \n",
    "        Examples of research-specific questions:\n",
    "        - What are the latest developments in CRISPR gene editing?\n",
    "        - How does the transformer architecture improve machine translation accuracy?\n",
    "        - What methodologies are used to measure quantum entanglement?\n",
    "        - What were the findings of the 2023 paper on climate change impact on coral reefs?\n",
    "        \n",
    "        Is this a general knowledge question that could be answered without specific research papers?\n",
    "        Answer only YES or NO.\n",
    "        \"\"\")\n",
    "        \n",
    "        try:\n",
    "            result = self.llm.invoke(prompt.format(query=query))\n",
    "            return \"YES\" in result.upper()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error determining if general knowledge question: {e}\")\n",
    "            return False  # Default to assuming it's not general knowledge\n",
    "    \n",
    "    def suggest_rewrites(self, query: str, chat_history: Optional[List] = None) -> List[str]:\n",
    "        \"\"\"Returns rephrased versions of the query optimized for retrieval.\n",
    "        \n",
    "        Args:\n",
    "            query: Original query to rewrite\n",
    "            chat_history: Optional conversation history for context\n",
    "            \n",
    "        Returns:\n",
    "            List of rewritten queries\n",
    "        \"\"\"\n",
    "        history_context = \"\"\n",
    "        if chat_history:\n",
    "            history_context = \"Consider this conversation context when rewriting the query:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use only last 3 messages for brevity\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    history_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"history_context\"],\n",
    "            template=\"\"\"You are an AI assistant specializing in scientific research queries.\n",
    "            \n",
    "            {history_context}\n",
    "            \n",
    "            Rephrase the question in 5 different ways to improve retrieval from a scientific paper database. \n",
    "            Focus on:\n",
    "            1. Using technical terminology for better vector matching\n",
    "            2. Breaking down complex queries into clearer formulations\n",
    "            3. Adding relevant synonyms or related concepts\n",
    "            4. Varying syntax while preserving semantic meaning\n",
    "            5. Including key entities and relationships from the original query\n",
    "            \n",
    "            Number each version starting with 1.\n",
    "            \n",
    "            Question: {question}\"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            result = (prompt | self.llm | self.list_parser).invoke({\n",
    "                \"question\": query,\n",
    "                \"history_context\": history_context\n",
    "            })\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error suggesting rewrites: {e}\")\n",
    "            # Return a minimal set of rewrites if parsing fails\n",
    "            return [\n",
    "                query,  # Original query\n",
    "                f\"research about {query}\",\n",
    "                f\"papers discussing {query}\"\n",
    "            ]\n",
    "    \n",
    "    def compute_confidence(self, original: str, rewritten: str) -> float:\n",
    "        \"\"\"Computes semantic similarity between original and rewritten queries.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            rewritten: Rewritten version\n",
    "            \n",
    "        Returns:\n",
    "            Similarity score between 0-1\n",
    "        \"\"\"\n",
    "        try:\n",
    "            vec_orig = self.semantic_model.encode(original, convert_to_tensor=True)\n",
    "            vec_rewrite = self.semantic_model.encode(rewritten, convert_to_tensor=True)\n",
    "            return float(util.pytorch_cos_sim(vec_orig, vec_rewrite).item())\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error computing similarity: {e}\")\n",
    "            return 0.7  # Default reasonable value\n",
    "    \n",
    "    def score_queries(self, original: str, queries: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Scores and sorts rephrased queries by confidence.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            queries: List of rewritten queries\n",
    "            \n",
    "        Returns:\n",
    "            List of (query, score) tuples sorted by score\n",
    "        \"\"\"\n",
    "        scored = [(q, self.compute_confidence(original, q)) for q in queries]\n",
    "        return sorted(scored, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    def present_query_options(self, original: str, queries: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Presents the original and rewritten queries with confidence scores.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            queries: List of rewritten queries\n",
    "            \n",
    "        Returns:\n",
    "            List of (query, score) tuples including original query\n",
    "        \"\"\"\n",
    "        # Add original query at the top\n",
    "        scored_queries = [(\"Original: \" + original, 1.0)]\n",
    "        \n",
    "        # Add scored rewritten queries\n",
    "        rewritten_scores = self.score_queries(original, queries)\n",
    "        for i, (query, score) in enumerate(rewritten_scores, 1):\n",
    "            scored_queries.append((f\"Rewrite {i}: {query}\", score))\n",
    "            \n",
    "        # Display options in a nice format\n",
    "        console.print(\"\\n[bold cyan]Available search queries:[/bold cyan]\")\n",
    "        for i, (query, score) in enumerate(scored_queries):\n",
    "            confidence_color = \"green\" if score > 0.8 else \"yellow\" if score > 0.6 else \"red\"\n",
    "            console.print(f\"[bold]{i}.[/bold] {query}\")\n",
    "            console.print(f\"   [bold {confidence_color}]Confidence: {score:.2f}[/bold {confidence_color}]\")\n",
    "            \n",
    "        return scored_queries\n",
    "    \n",
    "    def retrieve_documents(\n",
    "        self, \n",
    "        queries: List[str], \n",
    "        k: int = 5\n",
    "    ) -> Tuple[List[RetrievedDocument], Dict[str, List[RetrievedDocument]]]:\n",
    "        \"\"\"Retrieves documents using multiple queries, preserving query information.\n",
    "        \n",
    "        Args:\n",
    "            queries: List of queries to retrieve documents for\n",
    "            k: Number of documents to retrieve per query\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (all unique documents, query->documents mapping)\n",
    "        \"\"\"\n",
    "        all_docs: List[RetrievedDocument] = []\n",
    "        unique_content = set()\n",
    "        query_docs_map: Dict[str, List[RetrievedDocument]] = {}\n",
    "        \n",
    "        # Retrieve docs for each query\n",
    "        for query in queries:\n",
    "            console.print(f\"\\n[bold blue]Query:[/bold blue] '{query}'\")\n",
    "            \n",
    "            try:\n",
    "                results = self.vector_db.similarity_search_with_relevance_scores(query, k=k)\n",
    "                docs_for_query: List[RetrievedDocument] = []\n",
    "                \n",
    "                for i, (doc, score) in enumerate(results, 1):\n",
    "                    # Create retrieved document object\n",
    "                    retrieved_doc = RetrievedDocument(\n",
    "                        document=doc,\n",
    "                        score=score,\n",
    "                        query=query,\n",
    "                        rank=i\n",
    "                    )\n",
    "                    \n",
    "                    # Display result info\n",
    "                    score_color = \"green\" if score > 0.8 else \"yellow\" if score > 0.6 else \"red\"\n",
    "                    console.print(f\"[bold]--- Result {i} ---[/bold]\")\n",
    "                    console.print(f\"[bold {score_color}]Score: {score:.4f}[/bold {score_color}]\")\n",
    "                    \n",
    "                    # Show document preview\n",
    "                    preview = doc.page_content[:150] + \"...\" if len(doc.page_content) > 150 else doc.page_content\n",
    "                    console.print(Panel(preview, title=\"Content Preview\", width=100))\n",
    "                    \n",
    "                    # Show metadata if available\n",
    "                    if hasattr(doc, 'metadata') and doc.metadata:\n",
    "                        source = doc.metadata.get('source', 'Unknown')\n",
    "                        console.print(f\"[dim]Source: {source}[/dim]\")\n",
    "                    \n",
    "                    # Add to results for this query\n",
    "                    docs_for_query.append(retrieved_doc)\n",
    "                    \n",
    "                    # Only add unique documents to overall collection\n",
    "                    if doc.page_content not in unique_content:\n",
    "                        unique_content.add(doc.page_content)\n",
    "                        all_docs.append(retrieved_doc)\n",
    "                \n",
    "                query_docs_map[query] = docs_for_query\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error retrieving documents for query '{query}': {e}\")\n",
    "                console.print(f\"[bold red]Error retrieving documents for query: {query}[/bold red]\")\n",
    "        \n",
    "        console.print(f\"\\n[bold green]Total unique documents:[/bold green] {len(all_docs)}\")\n",
    "        return all_docs, query_docs_map\n",
    "\n",
    "    def rerank_docs(self, query: str, docs: List[RetrievedDocument]) -> List[RetrievedDocument]:\n",
    "        \"\"\"Reranks documents based on semantic similarity to the query.\n",
    "        \n",
    "        Args:\n",
    "            query: Query to compare documents against\n",
    "            docs: List of retrieved documents\n",
    "            \n",
    "        Returns:\n",
    "            Reranked list of documents with updated scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            query_embedding = self.semantic_model.encode(query, convert_to_tensor=True)\n",
    "            reranked = []\n",
    "            \n",
    "            for doc in docs:\n",
    "                doc_embedding = self.semantic_model.encode(doc.document.page_content, convert_to_tensor=True)\n",
    "                new_score = float(util.pytorch_cos_sim(query_embedding, doc_embedding).item())\n",
    "                \n",
    "                # Create new RetrievedDocument with updated score\n",
    "                reranked_doc = RetrievedDocument(\n",
    "                    document=doc.document,\n",
    "                    score=new_score,\n",
    "                    query=doc.query,\n",
    "                    rank=0  # Will be updated after sorting\n",
    "                )\n",
    "                reranked.append(reranked_doc)\n",
    "            \n",
    "            # Sort by score and update ranks\n",
    "            reranked.sort(key=lambda x: x.score, reverse=True)\n",
    "            for i, doc in enumerate(reranked, 1):\n",
    "                doc.rank = i\n",
    "                \n",
    "            return reranked\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reranking documents: {e}\")\n",
    "            return docs  # Return original documents if reranking fails\n",
    "    \n",
    "    def can_answer_without_retrieval(self, question: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"Determines if a question can be answered directly from memory without retrieval.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (can_answer, answer_if_available)\n",
    "        \"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        if not chat_history:\n",
    "            return False, None\n",
    "            \n",
    "        # Create the prompt to check if we can answer directly from the conversation\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an AI assistant helping with a scientific research conversation.\n",
    "        Given the following conversation history and a new question, determine if the question:\n",
    "        1. Can be answered directly based ONLY on the conversation history (like \"what was my previous question?\")\n",
    "        2. Does NOT require retrieving new information from scientific papers\n",
    "        \n",
    "        If both conditions are true, provide the answer. Otherwise, respond with \"NEEDS_RETRIEVAL\".\n",
    "        \n",
    "        Conversation History:\n",
    "        {chat_history}\n",
    "        \n",
    "        New Question: {question}\n",
    "        \n",
    "        Your assessment (answer directly or respond with \"NEEDS_RETRIEVAL\"):\n",
    "        \"\"\")\n",
    "        \n",
    "        # Format the chat history for context\n",
    "        history_str = \"\"\n",
    "        for message in chat_history[-5:]:  # Use only last 5 messages for brevity\n",
    "            if hasattr(message, \"content\"):\n",
    "                role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                history_str += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        try:\n",
    "            # Ask the LLM if this can be answered without retrieval\n",
    "            response = self.llm.invoke(\n",
    "                prompt.format(chat_history=history_str, question=question)\n",
    "            )\n",
    "            \n",
    "            # If the response is \"NEEDS_RETRIEVAL\", we need to use retrieval\n",
    "            if \"NEEDS_RETRIEVAL\" in response:\n",
    "                return False, None\n",
    "            else:\n",
    "                logger.info(\"Question can be answered from conversation history\")\n",
    "                return True, response\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error determining if retrieval needed: {e}\")\n",
    "            return False, None  # Default to retrieval if there's an error\n",
    "    \n",
    "    def generate_final_answer(\n",
    "        self, \n",
    "        question: str, \n",
    "        docs: List[RetrievedDocument], \n",
    "        max_docs: int = 5,\n",
    "        fallback_to_llm: bool = True\n",
    "    ) -> str:\n",
    "        \"\"\"Generates the final answer from retrieved documents with fallback to LLM.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            docs: List of retrieved documents\n",
    "            max_docs: Maximum number of documents to include\n",
    "            fallback_to_llm: Whether to fall back to the LLM when docs are insufficient\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        # First, check if this might be a general knowledge question\n",
    "        is_general = self.is_general_knowledge_question(question)\n",
    "        \n",
    "        # First, check if we have any relevant documents\n",
    "        has_relevant_docs = any(doc.score >= self.relevance_threshold for doc in docs)\n",
    "        \n",
    "        # If no relevant docs were found and fallback is enabled, use LLM directly\n",
    "        if (not has_relevant_docs and fallback_to_llm) or (is_general and self.always_use_fallback):\n",
    "            if is_general:\n",
    "                console.print(\"[yellow]This appears to be a general knowledge question. Using LLM knowledge.[/yellow]\")\n",
    "            else:\n",
    "                console.print(\"[yellow]No relevant documents found. Falling back to LLM knowledge.[/yellow]\")\n",
    "            \n",
    "            # Create a direct LLM response prompt\n",
    "            fallback_prompt = PromptTemplate(\n",
    "                input_variables=[\"question\", \"chat_history\"],\n",
    "                template=\"\"\"You are a helpful scientific research assistant.\n",
    "                \n",
    "                The user has asked a question that either:\n",
    "                1. Could not be answered using our research paper database, or\n",
    "                2. Is a general knowledge question that doesn't require specific papers\n",
    "                \n",
    "                Please answer based on your general knowledge.\n",
    "                \n",
    "                {chat_history}\n",
    "                \n",
    "                Question: {question}\n",
    "                \n",
    "                Provide a helpful, informative answer. If this is a scientific or technical question,\n",
    "                explain concepts clearly and accurately. Don't apologize for not using specific papers \n",
    "                - just provide your best answer directly.\n",
    "                \n",
    "                Answer:\"\"\"\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                answer = (fallback_prompt | self.llm).invoke({\n",
    "                    \"question\": question,\n",
    "                    \"chat_history\": chat_context\n",
    "                })\n",
    "                \n",
    "                # Save the QA pair to memory\n",
    "                self.memory.chat_memory.add_messages([\n",
    "                    HumanMessage(content=question),\n",
    "                    AIMessage(content=answer)\n",
    "                ])\n",
    "                \n",
    "                return answer\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating fallback answer: {e}\")\n",
    "                return \"I'm sorry, I don't have enough information to answer your question accurately.\"\n",
    "        \n",
    "        # Format document content with metadata\n",
    "        context_pieces = []\n",
    "        for i, doc in enumerate(docs[:max_docs]):\n",
    "            chunk = f\"Document {i+1} [Relevance: {doc.score:.2f}]:\\n{doc.document.page_content}\"\n",
    "            \n",
    "            # Add metadata if available\n",
    "            if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                source = doc.document.metadata.get('source', 'Unknown')\n",
    "                chunk += f\"\\nSource: {source}\"\n",
    "                \n",
    "            context_pieces.append(chunk)\n",
    "            \n",
    "        context = \"\\n\\n---\\n\\n\".join(context_pieces)\n",
    "        \n",
    "        # If we have context but it might not be sufficient, use a prompt that can fall back to general knowledge\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful scientific research assistant analyzing scientific papers. \n",
    "            Use the following retrieved context chunks to answer the user's question thoroughly.\n",
    "            \n",
    "            Guidelines:\n",
    "            - Focus on providing accurate information from the papers\n",
    "            - Synthesize information across documents when appropriate\n",
    "            - Cite the sources of information in your answer (e.g., \"According to Document 1...\")\n",
    "            - If the information isn't sufficient in the context, supplement with your general knowledge,\n",
    "              clearly indicating which parts of your answer come from the papers and which parts are from your general knowledge\n",
    "            - If the context contains conflicting information, highlight the disagreement and possible reasons\n",
    "            - Maintain scientific accuracy above all else\n",
    "            - Don't apologize for using general knowledge - just be clear about what comes from papers vs. your knowledge\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Generate answer\n",
    "            answer = (prompt | self.llm).invoke({\n",
    "                \"question\": question, \n",
    "                \"context\": context,\n",
    "                \"chat_history\": chat_context\n",
    "            })\n",
    "            \n",
    "            # Save the QA pair to memory\n",
    "            self.memory.chat_memory.add_messages([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=answer)\n",
    "            ])\n",
    "            \n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating answer: {e}\")\n",
    "            return \"I'm sorry, I encountered an error while generating your answer. Please try rephrasing your question.\"\n",
    "    \n",
    "    def generate_combined_answer(\n",
    "        self, \n",
    "        question: str, \n",
    "        query_results: Dict[str, List[RetrievedDocument]],\n",
    "        fallback_to_llm: bool = True\n",
    "    ) -> str:\n",
    "        \"\"\"Generates a combined answer from multiple query results with fallback.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            query_results: Dictionary mapping queries to retrieved documents\n",
    "            fallback_to_llm: Whether to fall back to the LLM when docs are insufficient\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        # Check if this might be a general knowledge question\n",
    "        is_general = self.is_general_knowledge_question(question)\n",
    "        \n",
    "        # Check if we have any relevant documents at all\n",
    "        all_docs = []\n",
    "        for docs_list in query_results.values():\n",
    "            all_docs.extend(docs_list)\n",
    "            \n",
    "        has_relevant_docs = any(doc.score >= self.relevance_threshold for doc in all_docs)\n",
    "        \n",
    "        # If no relevant documents and fallback enabled, use LLM directly\n",
    "        if (not has_relevant_docs and fallback_to_llm) or (is_general and self.always_use_fallback):\n",
    "            if is_general:\n",
    "                console.print(\"[yellow]This appears to be a general knowledge question. Using LLM knowledge.[/yellow]\")\n",
    "            else:\n",
    "                console.print(\"[yellow]No relevant documents found across all queries. Falling back to LLM knowledge.[/yellow]\")\n",
    "            \n",
    "            # Create a direct LLM response prompt\n",
    "            fallback_prompt = PromptTemplate(\n",
    "                input_variables=[\"question\", \"chat_history\"],\n",
    "                template=\"\"\"You are a helpful scientific research assistant.\n",
    "                \n",
    "                The user has asked a question that either:\n",
    "                1. Could not be answered using our research paper database, or\n",
    "                2. Is a general knowledge question that doesn't require specific papers\n",
    "                \n",
    "                Please answer based on your general knowledge.\n",
    "                \n",
    "                {chat_history}\n",
    "                \n",
    "                Question: {question}\n",
    "                \n",
    "                Provide a helpful, informative answer. If this is a scientific or technical question,\n",
    "                explain concepts clearly and accurately. Don't apologize for not using specific papers \n",
    "                - just provide your best answer directly.\n",
    "                \n",
    "                Answer:\"\"\"\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                answer = (fallback_prompt | self.llm).invoke({\n",
    "                    \"question\": question,\n",
    "                    \"chat_history\": chat_context\n",
    "                })\n",
    "                \n",
    "                # Save the QA pair to memory\n",
    "                self.memory.chat_memory.add_messages([\n",
    "                    HumanMessage(content=question),\n",
    "                    AIMessage(content=answer)\n",
    "                ])\n",
    "                \n",
    "                return answer\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating fallback answer: {e}\")\n",
    "                return \"I'm sorry, I don't have enough information to answer your question accurately.\"\n",
    "        \n",
    "        # Combine all relevant documents from different queries\n",
    "        all_context_sections = []\n",
    "        \n",
    "        for query, docs in query_results.items():\n",
    "            # Use only the top N documents for each query\n",
    "            docs_for_query = docs[:self.max_docs_per_query]\n",
    "            if docs_for_query:\n",
    "                all_context_sections.append(f\"Results for query: '{query}'\")\n",
    "                for i, doc in enumerate(docs_for_query, 1):\n",
    "                    section = f\"Document {i} [Relevance: {doc.score:.2f}]:\\n{doc.document.page_content}\"\n",
    "                    \n",
    "                    # Add metadata if available\n",
    "                    if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                        source = doc.document.metadata.get('source', 'Unknown')\n",
    "                        section += f\"\\nSource: {source}\"\n",
    "                        \n",
    "                    all_context_sections.append(section)\n",
    "        \n",
    "        # Join all context sections\n",
    "        combined_context = \"\\n\\n---\\n\\n\".join(all_context_sections)\n",
    "        \n",
    "        # Create a prompt that emphasizes synthesizing information and can fall back to general knowledge\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful scientific research assistant analyzing scientific papers. \n",
    "            The user's question has been reformulated in several ways, and each formulation returned different documents.\n",
    "            \n",
    "            Guidelines:\n",
    "            - Synthesize information across ALL retrieved documents to provide a comprehensive answer\n",
    "            - Compare and contrast findings from different sources\n",
    "            - Highlight the most relevant information from each source\n",
    "            - Cite the specific documents you're referencing (e.g., \"According to the paper in Document 3...\")\n",
    "            - If information conflicts across sources, explain the different perspectives\n",
    "            - If the information is insufficient, supplement with your general knowledge,\n",
    "              clearly indicating which parts of your answer come from the papers and which are from your general knowledge\n",
    "            - Maintain scientific accuracy above all else\n",
    "            - Don't apologize for using general knowledge - just be clear about what information comes from papers vs. your knowledge\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Context from multiple query formulations:\n",
    "            {context}\n",
    "            \n",
    "            Original Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Generate answer\n",
    "            answer = (prompt | self.llm).invoke({\n",
    "                \"question\": question, \n",
    "                \"context\": combined_context,\n",
    "                \"chat_history\": chat_context\n",
    "            })\n",
    "            \n",
    "            # Save the QA pair to memory\n",
    "            self.memory.chat_memory.add_messages([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=answer)\n",
    "            ])\n",
    "            \n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating combined answer: {e}\")\n",
    "            return \"I'm sorry, I encountered an error while generating your answer. Please try rephrasing your question.\"\n",
    "    \n",
    "    def answer_query(self, query: str, use_retrieval: bool = True) -> str:\n",
    "        \"\"\"Direct answer method that either does retrieval or uses LLM fallback.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            use_retrieval: Whether to use retrieval process or direct LLM\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # First check if we can answer directly from conversation history\n",
    "        can_answer, direct_answer = self.can_answer_without_retrieval(query)\n",
    "        if can_answer:\n",
    "            console.print(\"[green]Answering from conversation history[/green]\")\n",
    "            return direct_answer\n",
    "        \n",
    "        # Check if this is a general knowledge question\n",
    "        is_general = self.is_general_knowledge_question(query)\n",
    "        \n",
    "        # If it's general knowledge and we're set to always use fallback for those\n",
    "        if is_general and self.always_use_fallback and not use_retrieval:\n",
    "            console.print(\"[yellow]General knowledge question detected. Using LLM directly.[/yellow]\")\n",
    "            # Create a direct LLM response prompt\n",
    "            chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "            chat_context = \"\"\n",
    "            if chat_history:\n",
    "                chat_context = \"Previous conversation:\\n\"\n",
    "                for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                    if hasattr(message, \"content\"):\n",
    "                        role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                        chat_context += f\"{role}: {message.content}\\n\"\n",
    "            \n",
    "            fallback_prompt = PromptTemplate(\n",
    "                input_variables=[\"question\", \"chat_history\"],\n",
    "                template=\"\"\"You are a helpful scientific research assistant.\n",
    "                \n",
    "                The user has asked a general knowledge question that doesn't require specific papers.\n",
    "                Please answer based on your general knowledge.\n",
    "                \n",
    "                {chat_history}\n",
    "                \n",
    "                Question: {question}\n",
    "                \n",
    "                Provide a helpful, informative answer. If this is a scientific or technical question,\n",
    "                explain concepts clearly and accurately.\n",
    "                \n",
    "                Answer:\"\"\"\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                answer = (fallback_prompt | self.llm).invoke({\n",
    "                    \"question\": query,\n",
    "                    \"chat_history\": chat_context\n",
    "                })\n",
    "                \n",
    "                # Save the QA pair to memory\n",
    "                self.memory.chat_memory.add_messages([\n",
    "                    HumanMessage(content=query),\n",
    "                    AIMessage(content=answer)\n",
    "                ])\n",
    "                \n",
    "                return answer\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating direct answer: {e}\")\n",
    "                return \"I'm sorry, I encountered an error while generating your answer.\"\n",
    "        \n",
    "        # Otherwise, proceed with full retrieval pipeline\n",
    "        return self.search_with_query_feedback(query)\n",
    "    \n",
    "    def search_with_query_feedback(self, query: str, num_results: int = 5) -> str:\n",
    "        \"\"\"Main pipeline that processes a query and returns an answer.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            num_results: Number of results to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Rate the original query\n",
    "            console.print(\"\\n[bold cyan]Evaluating your query...[/bold cyan]\")\n",
    "            rating_result = self.rate_query(query)\n",
    "            \n",
    "            rating_color = \"green\" if rating_result['rating'] >= 4 else \"yellow\" if rating_result['rating'] >= 3 else \"red\"\n",
    "            console.print(f\"[bold {rating_color}]Query Rating: {rating_result['rating']}/5[/bold {rating_color}]\")\n",
    "            console.print(f\"Explanation: {rating_result['explanation']}\")\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Check if this is a general knowledge question\n",
    "            is_general = self.is_general_knowledge_question(query)\n",
    "            if is_general and self.always_use_fallback:\n",
    "                console.print(\"[yellow]This appears to be a general knowledge question. Proceeding with simplified search.[/yellow]\")\n",
    "                # For general knowledge questions, we might still want to check the database briefly\n",
    "                # but we'll rely more on the LLM fallback\n",
    "            \n",
    "            # Step 2: Suggest rewrites if the rating is less than perfect\n",
    "            rewritten_queries = []\n",
    "            if rating_result[\"rating\"] < 5:\n",
    "                console.print(\"[bold cyan]Generating improved query variations...[/bold cyan]\")\n",
    "                # Get chat history for context-aware rewrites\n",
    "                chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "                rewritten_queries = self.suggest_rewrites(query, chat_history)\n",
    "                console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 3: Present options to the user\n",
    "            query_options = self.present_query_options(query, rewritten_queries)\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 4: Get user selection\n",
    "            console.print(\"[bold cyan]Select queries to use:[/bold cyan]\")\n",
    "            console.print(\"Enter the numbers of the queries you want to use (comma-separated, e.g., '0,2,3')\")\n",
    "            console.print(\"Or press Enter to use all queries\")\n",
    "            selected_indices_input = input(\"> \")\n",
    "            \n",
    "            if selected_indices_input.strip() == \"\":\n",
    "                # Use all queries if no selection\n",
    "                selected_indices = list(range(len(query_options)))\n",
    "            else:\n",
    "                try:\n",
    "                    selected_indices = [int(idx.strip()) for idx in selected_indices_input.split(\",\")]\n",
    "                except ValueError:\n",
    "                    console.print(\"[bold red]Invalid input. Using original query only.[/bold red]\")\n",
    "                    selected_indices = [0]  # Default to original query\n",
    "            \n",
    "            # Get the selected queries (without the prefix and score)\n",
    "            selected_queries = []\n",
    "            for idx in selected_indices:\n",
    "                if 0 <= idx < len(query_options):\n",
    "                    query_text = query_options[idx][0]\n",
    "                    # Remove the \"Original: \" or \"Rewrite N: \" prefix\n",
    "                    if \"Original: \" in query_text:\n",
    "                        query_text = query_text.replace(\"Original: \", \"\")\n",
    "                    elif \"Rewrite \" in query_text:\n",
    "                        query_text = query_text.split(\": \", 1)[1] if \": \" in query_text else query_text\n",
    "                    selected_queries.append(query_text)\n",
    "            \n",
    "            if not selected_queries:\n",
    "                console.print(\"[bold red]No valid queries selected. Using original query.[/bold red]\")\n",
    "                selected_queries = [query]\n",
    "                \n",
    "            console.print(f\"\\n[bold green]Selected {len(selected_queries)} queries for retrieval.[/bold green]\")\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 5: Retrieve documents\n",
    "            console.print(\"[bold cyan]Retrieving relevant documents...[/bold cyan]\")\n",
    "            docs, query_docs_map = self.retrieve_documents(selected_queries, k=num_results)\n",
    "            \n",
    "            # Step 6: Generate answer\n",
    "            if len(selected_queries) > 1:\n",
    "                console.print(\"\\n[bold cyan]Generating combined answer based on multiple query results...[/bold cyan]\")\n",
    "                final_answer = self.generate_combined_answer(\n",
    "                    query, \n",
    "                    query_docs_map,\n",
    "                    fallback_to_llm=True  # Always enable fallback\n",
    "                )\n",
    "            else:\n",
    "                # For single query, rerank and use the traditional approach\n",
    "                console.print(\"\\n[bold cyan]Reranking documents based on relevance to the original query...[/bold cyan]\")\n",
    "                reranked_docs = self.rerank_docs(query, docs)\n",
    "                \n",
    "                # Apply relevance threshold\n",
    "                filtered_docs = [\n",
    "                    doc for doc in reranked_docs if doc.score >= self.relevance_threshold\n",
    "                ]\n",
    "                \n",
    "                # Display relevance information\n",
    "                has_relevant_docs = len(filtered_docs) > 0\n",
    "                if not has_relevant_docs:\n",
    "                    console.print(\"[yellow]No documents met the relevance threshold. Using all retrieved documents but may fall back to LLM knowledge.[/yellow]\")\n",
    "                    filtered_docs = reranked_docs\n",
    "                \n",
    "                # Generate the final answer with fallback enabled\n",
    "                console.print(\"\\n[bold cyan]Generating answer...[/bold cyan]\")\n",
    "                final_answer = self.generate_final_answer(\n",
    "                    query, \n",
    "                    filtered_docs, \n",
    "                    fallback_to_llm=True  # Always enable fallback\n",
    "                )\n",
    "            \n",
    "            return final_answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in search pipeline: {e}\")\n",
    "            # Fall back to direct LLM answer if the pipeline fails\n",
    "            console.print(\"[bold red]Error in search pipeline. Falling back to LLM.[/bold red]\")\n",
    "            \n",
    "            chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "            chat_context = \"\"\n",
    "            if chat_history:\n",
    "                chat_context = \"Previous conversation:\\n\"\n",
    "                for message in chat_history[-3:]:\n",
    "                    if hasattr(message, \"content\"):\n",
    "                        role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                        chat_context += f\"{role}: {message.content}\\n\"\n",
    "            \n",
    "            # Create a direct LLM response as emergency fallback\n",
    "            emergency_prompt = PromptTemplate(\n",
    "                input_variables=[\"question\", \"chat_history\"],\n",
    "                template=\"\"\"You are a helpful scientific research assistant.\n",
    "                \n",
    "                There was an error retrieving information from our research database.\n",
    "                Please answer the question based on your general knowledge.\n",
    "                \n",
    "                {chat_history}\n",
    "                \n",
    "                Question: {question}\n",
    "                \n",
    "                Answer:\"\"\"\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                answer = (emergency_prompt | self.llm).invoke({\n",
    "                    \"question\": query,\n",
    "                    \"chat_history\": chat_context\n",
    "                })\n",
    "                return answer\n",
    "            except:\n",
    "                return \"I apologize, but I'm having technical difficulties. Please try again with a different question.\"\n",
    "\n",
    "# Example usage implementation \n",
    "def create_research_assistant(llm, vector_db, embeddings_model=None):\n",
    "    \"\"\"Factory function to create a research assistant with the specified components.\"\"\"\n",
    "    return ResearchAssistant(\n",
    "        llm=llm,\n",
    "        vector_db=vector_db,\n",
    "        embeddings_model=embeddings_model,\n",
    "        relevance_threshold=0.5,  # Lower threshold to be more lenient with document relevance\n",
    "        max_docs_per_query=3,\n",
    "        always_use_fallback=True  # Always use LLM fallback for general knowledge\n",
    "    )\n",
    "\n",
    "# Interactive command-line interface\n",
    "def run_cli(assistant):\n",
    "    \"\"\"Run an interactive CLI for the research assistant.\"\"\"\n",
    "    console.print(\"[bold green]Research Assistant CLI[/bold green]\")\n",
    "    console.print(\"Type 'exit' to quit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\n[bold blue]Ask a question:[/bold blue] \")\n",
    "        if question.lower() in ('exit', 'quit', 'q'):\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            answer = assistant.answer_query(question)\n",
    "            console.print(\"\\n[bold green]Answer:[/bold green]\")\n",
    "            console.print(Panel(Markdown(answer), width=100))\n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Error:[/bold red] {e}\")\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # You would need to define these before using:\n",
    "    # from langchain.llms import ChatOpenAI\n",
    "    # from langchain.vectorstores import Chroma\n",
    "    # \n",
    "    # llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    # vector_db = Chroma(embedding_function=...)\n",
    "    # \n",
    "    # assistant = create_research_assistant(llm, vector_db)\n",
    "    # run_cli(assistant)\n",
    "    print(\"Import this module to use the ResearchAssistant class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab1fa817-a9b3-4feb-807a-570a2b51eece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:37:52,676 - SageRAG - INFO - Research Assistant initialized successfully\n"
     ]
    }
   ],
   "source": [
    "assistant = create_research_assistant(llm, vector_db, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a6699f1-9f6b-4264-82e7-0c363463319f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query What is operating system ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:38:48,444 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Evaluating your query...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mEvaluating your query\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:38:48,916 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Query Rating: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">/</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mQuery Rating: \u001b[0m\u001b[1;32m4\u001b[0m\u001b[1;32m/\u001b[0m\u001b[1;32m5\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Explanation: The query is clear and concise, with a good balance of specificity and relevance. However, it lacks \n",
       "technical precision and could potentially retrieve more general information on operating systems, rather than \n",
       "scientific research content. This makes it moderately effective for retrieving results from a vector search.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Explanation: The query is clear and concise, with a good balance of specificity and relevance. However, it lacks \n",
       "technical precision and could potentially retrieve more general information on operating systems, rather than \n",
       "scientific research content. This makes it moderately effective for retrieving results from a vector search.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:38:50,718 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">This appears to be a general knowledge question. Proceeding with simplified search.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mThis appears to be a general knowledge question. Proceeding with simplified search.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Generating improved query variations...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mGenerating improved query variations\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:38:51,061 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-04-17 13:38:53,105 - langsmith.client - WARNING - Failed to send compressed multipart ingest: Connection error caused failure to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. Please confirm your internet connection. ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'api.smith.langchain.com\\', port=443): Max retries exceeded with url: /runs/multipart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x3190d48d0>: Failed to resolve \\'api.smith.langchain.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))'))\n",
      "Content-Length: 1176\n",
      "API Key: lsv2_********************************************82\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:38:57,796 - SageRAG - ERROR - Error computing similarity: 'HuggingFaceEmbeddings' object has no attribute 'encode'\n",
      "2025-04-17 13:38:57,798 - SageRAG - ERROR - Error computing similarity: 'HuggingFaceEmbeddings' object has no attribute 'encode'\n",
      "2025-04-17 13:38:57,799 - SageRAG - ERROR - Error computing similarity: 'HuggingFaceEmbeddings' object has no attribute 'encode'\n",
      "2025-04-17 13:38:57,804 - SageRAG - ERROR - Error computing similarity: 'HuggingFaceEmbeddings' object has no attribute 'encode'\n",
      "2025-04-17 13:38:57,805 - SageRAG - ERROR - Error computing similarity: 'HuggingFaceEmbeddings' object has no attribute 'encode'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Available search queries:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mAvailable search queries:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">.</span> Original: What is operating system ?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m0\u001b[0m\u001b[1m.\u001b[0m Original: What is operating system ?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Confidence: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;32mConfidence: \u001b[0m\u001b[1;32m1.00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: **Technical terminology for better vector matching**: <span style=\"color: #008000; text-decoration-color: #008000\">\"Query OS classification: Investigate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">definitions of computer system architecture, specifically those employing process management and resource </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">allocation algorithms.\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m1\u001b[0m: **Technical terminology for better vector matching**: \u001b[32m\"Query OS classification: Investigate \u001b[0m\n",
       "\u001b[32mdefinitions of computer system architecture, specifically those employing process management and resource \u001b[0m\n",
       "\u001b[32mallocation algorithms.\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.70</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.70\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: **Breaking down complex queries into clearer formulations**: <span style=\"color: #008000; text-decoration-color: #008000\">\"Exploring the concept of a 'computer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">operating system': Describe the primary functions, including (i) process scheduling, (ii) memory management, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(iii) I/O handling, in relation to its impact on computer performance and user experience.\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m2\u001b[0m: **Breaking down complex queries into clearer formulations**: \u001b[32m\"Exploring the concept of a 'computer \u001b[0m\n",
       "\u001b[32moperating system': Describe the primary functions, including \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi\u001b[0m\u001b[32m)\u001b[0m\u001b[32m process scheduling, \u001b[0m\u001b[32m(\u001b[0m\u001b[32mii\u001b[0m\u001b[32m)\u001b[0m\u001b[32m memory management, and \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32miii\u001b[0m\u001b[32m)\u001b[0m\u001b[32m I/O handling, in relation to its impact on computer performance and user experience.\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.70</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.70\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: **Adding relevant synonyms or related concepts**: <span style=\"color: #008000; text-decoration-color: #008000\">\"Inquire about the characteristics of an 'operating</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">system' (e.g., OS), particularly those involving multi-user, multi-tasking, and multi-processing support, as well </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">as their implications for security, stability, and efficiency.\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m3\u001b[0m: **Adding relevant synonyms or related concepts**: \u001b[32m\"Inquire about the characteristics of an 'operating\u001b[0m\n",
       "\u001b[32msystem' \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., OS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, particularly those involving multi-user, multi-tasking, and multi-processing support, as well \u001b[0m\n",
       "\u001b[32mas their implications for security, stability, and efficiency.\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.70</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.70\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>: **Varying syntax while preserving semantic meaning**: <span style=\"color: #008000; text-decoration-color: #008000\">\"Describe the nature of a computer operating </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">system: What are its fundamental components? How do they interact with hardware resources? What role does it play </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in facilitating efficient computation and user interaction?\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m4\u001b[0m: **Varying syntax while preserving semantic meaning**: \u001b[32m\"Describe the nature of a computer operating \u001b[0m\n",
       "\u001b[32msystem: What are its fundamental components? How do they interact with hardware resources? What role does it play \u001b[0m\n",
       "\u001b[32min facilitating efficient computation and user interaction?\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.70</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.70\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>: **Including key entities and relationships from the original query**: <span style=\"color: #008000; text-decoration-color: #008000\">\"Examine the relationship </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">between an 'operating system' (OS) and its constituent parts, including device drivers, kernel modules, and system </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">calls: How do these components interact to enable efficient execution of applications and manage system resources?\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m5\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m5\u001b[0m: **Including key entities and relationships from the original query**: \u001b[32m\"Examine the relationship \u001b[0m\n",
       "\u001b[32mbetween an 'operating system' \u001b[0m\u001b[32m(\u001b[0m\u001b[32mOS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and its constituent parts, including device drivers, kernel modules, and system \u001b[0m\n",
       "\u001b[32mcalls: How do these components interact to enable efficient execution of applications and manage system resources?\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.70</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.70\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Select queries to use:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mSelect queries to use:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Enter the numbers of the queries you want to use <span style=\"font-weight: bold\">(</span>comma-separated, e.g., <span style=\"color: #008000; text-decoration-color: #008000\">'0,2,3'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Enter the numbers of the queries you want to use \u001b[1m(\u001b[0mcomma-separated, e.g., \u001b[32m'0,2,3'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Or press Enter to use all queries\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Or press Enter to use all queries\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:39:02,688 - langsmith.client - WARNING - Failed to send compressed multipart ingest: Connection error caused failure to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. Please confirm your internet connection. ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'api.smith.langchain.com\\', port=443): Max retries exceeded with url: /runs/multipart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x3190c2f90>: Failed to resolve \\'api.smith.langchain.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))'))\n",
      "Content-Length: 5615\n",
      "API Key: lsv2_********************************************82\n",
      "2025-04-17 13:39:12,298 - langsmith.client - WARNING - Failed to send compressed multipart ingest: Connection error caused failure to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. Please confirm your internet connection. ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'api.smith.langchain.com\\', port=443): Max retries exceeded with url: /runs/multipart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x31909c850>: Failed to resolve \\'api.smith.langchain.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))'))\n",
      "Content-Length: 6092\n",
      "API Key: lsv2_********************************************82\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  0,1,3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Selected </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> queries for retrieval.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mSelected \u001b[0m\u001b[1;32m3\u001b[0m\u001b[1;32m queries for retrieval.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Retrieving relevant documents...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mRetrieving relevant documents\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Query:</span> <span style=\"color: #008000; text-decoration-color: #008000\">'What is operating system ?'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;34mQuery:\u001b[0m \u001b[32m'What is operating system ?'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.5895</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.5895\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Content Preview \n",
       "  Operating System ConceptsbyAvi Silberschatz, GregGagne, and Peter Baer Galvin                  \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       " Content Preview \n",
       "  Operating System ConceptsbyAvi Silberschatz, GregGagne, and Peter Baer Galvin                  \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2104.09487v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2104.09487v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.4490</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.4490\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Content Preview \n",
       " Android OS                                                                                       \n",
       " Case Study                                                                                       \n",
       " OPERATING  SYSTEMS                                                                               \n",
       " Submitted By: -                                                                                  \n",
       " Mayank Goelmayank.co19@nsut.ac.in                                                                \n",
       " GouravSingalgourav.co19@nsut.ac.in                                                               \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       " Content Preview \n",
       " Android OS                                                                                       \n",
       " Case Study                                                                                       \n",
       " OPERATING  SYSTEMS                                                                               \n",
       " Submitted By: -                                                                                  \n",
       " Mayank Goelmayank.co19@nsut.ac.in                                                                \n",
       " GouravSingalgourav.co19@nsut.ac.in                                                               \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2104.09487v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2104.09487v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3009</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3009\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Content Preview \n",
       " controlled by the file system are ultimately accessed via                                        \n",
       " kernel-resident device d rivers.  A user process can                                             \n",
       " implement any semantics it chooses fo...                                                         \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       " Content Preview \n",
       " controlled by the file system are ultimately accessed via                                        \n",
       " kernel-resident device d rivers.  A user process can                                             \n",
       " implement any semantics it chooses fo...                                                         \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/1403.5976v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/1403.5976v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.2779</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.2779\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Content Preview \n",
       " Symposium on Operating Systems Principles (SOSP 09) . 2944.                                    \n",
       " [11] Andrew Baumann, Gernot Heiser, Jonathan Appavoo, Dilma Da Silva,                            \n",
       " Orran Krieger, Rob...                                                                            \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       " Content Preview \n",
       " Symposium on Operating Systems Principles (SOSP 09) . 2944.                                    \n",
       " [11] Andrew Baumann, Gernot Heiser, Jonathan Appavoo, Dilma Da Silva,                            \n",
       " Orran Krieger, Rob...                                                                            \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2306.15076v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2306.15076v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.2669</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.2669\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Content Preview \n",
       " InternationalJournal of Computer Science &amp; Information Technology (IJCSIT) Vol 4, No 5, October  \n",
       " 2012                                                                                             \n",
       " 123                                                                                              \n",
       " System Time statistics arecalculatedin the fo...                                                 \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       " Content Preview \n",
       " InternationalJournal of Computer Science & Information Technology (IJCSIT) Vol 4, No 5, October  \n",
       " 2012                                                                                             \n",
       " 123                                                                                              \n",
       " System Time statistics arecalculatedin the fo...                                                 \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/1211.4839v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/1211.4839v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Query:</span> <span style=\"color: #008000; text-decoration-color: #008000\">'**Technical terminology for better vector matching**: \"Query OS classification: Investigate definitions of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">computer system architecture, specifically those employing process management and resource allocation algorithms.\"'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;34mQuery:\u001b[0m \u001b[32m'**Technical terminology for better vector matching**: \"Query OS classification: Investigate definitions of \u001b[0m\n",
       "\u001b[32mcomputer system architecture, specifically those employing process management and resource allocation algorithms.\"'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3459</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3459\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Content Preview \n",
       " Neetu Goel,  R.B. Garg| A Comparative Study of CPU Scheduling Algorithms                         \n",
       " International Journal of Graphics &amp; Image Processing |Vol 2|issue 4|November...                  \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       " Content Preview \n",
       " Neetu Goel,  R.B. Garg| A Comparative Study of CPU Scheduling Algorithms                         \n",
       " International Journal of Graphics & Image Processing |Vol 2|issue 4|November...                  \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/1307.4165v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/1307.4165v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3312</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3312\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Content Preview \n",
       " [8] D. Lo, L. Cheng, R. Govindaraju, P. Ranganathan, and C. Kozyrakis,                           \n",
       " Heracles: Improving resource efficiency at scale, in Proceedings of the                        \n",
       " 42nd ...                                                                                         \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       " Content Preview \n",
       " [8] D. Lo, L. Cheng, R. Govindaraju, P. Ranganathan, and C. Kozyrakis,                           \n",
       " Heracles: Improving resource efficiency at scale, in Proceedings of the                        \n",
       " 42nd ...                                                                                         \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2310.14741v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2310.14741v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3179</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3179\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Content Preview \n",
       "  Operating System ConceptsbyAvi Silberschatz, GregGagne, and Peter Baer Galvin                  \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       " Content Preview \n",
       "  Operating System ConceptsbyAvi Silberschatz, GregGagne, and Peter Baer Galvin                  \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2104.09487v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2104.09487v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3082</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3082\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Content Preview \n",
       " I., 2011. Mesos: A platform for ne-grained resource sharing in the data center. In: NSDI. Vol.  \n",
       " 11.                                                                                              \n",
       " pp. 2222.                                                                                       \n",
       " Hussain, H., Malik, S. U. R., Hameed, A...                                                       \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       " Content Preview \n",
       " I., 2011. Mesos: A platform for ne-grained resource sharing in the data center. In: NSDI. Vol.  \n",
       " 11.                                                                                              \n",
       " pp. 2222.                                                                                       \n",
       " Hussain, H., Malik, S. U. R., Hameed, A...                                                       \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/1705.03102v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/1705.03102v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3061</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3061\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Content Preview \n",
       " As can be seen from Figure 12, the observed worst case                                           \n",
       " time recorded in Quest-V is within the bounds, but also                                          \n",
       " close, to the prediction derived from E...                                                       \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       " Content Preview \n",
       " As can be seen from Figure 12, the observed worst case                                           \n",
       " time recorded in Quest-V is within the bounds, but also                                          \n",
       " close, to the prediction derived from E...                                                       \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/1310.6301v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/1310.6301v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Query:</span> <span style=\"color: #008000; text-decoration-color: #008000\">'**Adding relevant synonyms or related concepts**: \"Inquire about the characteristics of an '</span>operating \n",
       "system' <span style=\"font-weight: bold\">(</span>e.g., OS<span style=\"font-weight: bold\">)</span>, particularly those involving multi-user, multi-tasking, and multi-processing support, as well \n",
       "as their implications for security, stability, and efficiency.\"'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;34mQuery:\u001b[0m \u001b[32m'**Adding relevant synonyms or related concepts**: \"Inquire about the characteristics of an '\u001b[0moperating \n",
       "system' \u001b[1m(\u001b[0me.g., OS\u001b[1m)\u001b[0m, particularly those involving multi-user, multi-tasking, and multi-processing support, as well \n",
       "as their implications for security, stability, and efficiency.\"'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.5830</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.5830\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Content Preview \n",
       "  Operating System ConceptsbyAvi Silberschatz, GregGagne, and Peter Baer Galvin                  \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       " Content Preview \n",
       "  Operating System ConceptsbyAvi Silberschatz, GregGagne, and Peter Baer Galvin                  \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2104.09487v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2104.09487v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.4519</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.4519\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Content Preview \n",
       " Android OS                                                                                       \n",
       " Case Study                                                                                       \n",
       " OPERATING  SYSTEMS                                                                               \n",
       " Submitted By: -                                                                                  \n",
       " Mayank Goelmayank.co19@nsut.ac.in                                                                \n",
       " GouravSingalgourav.co19@nsut.ac.in                                                               \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       " Content Preview \n",
       " Android OS                                                                                       \n",
       " Case Study                                                                                       \n",
       " OPERATING  SYSTEMS                                                                               \n",
       " Submitted By: -                                                                                  \n",
       " Mayank Goelmayank.co19@nsut.ac.in                                                                \n",
       " GouravSingalgourav.co19@nsut.ac.in                                                               \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2104.09487v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2104.09487v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.4417</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.4417\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Content Preview \n",
       " Symposium on Operating Systems Principles (SOSP 09) . 2944.                                    \n",
       " [11] Andrew Baumann, Gernot Heiser, Jonathan Appavoo, Dilma Da Silva,                            \n",
       " Orran Krieger, Rob...                                                                            \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       " Content Preview \n",
       " Symposium on Operating Systems Principles (SOSP 09) . 2944.                                    \n",
       " [11] Andrew Baumann, Gernot Heiser, Jonathan Appavoo, Dilma Da Silva,                            \n",
       " Orran Krieger, Rob...                                                                            \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2306.15076v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2306.15076v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3319</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3319\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Content Preview \n",
       " Yew. 2006. Live Updating Operating Systems Using Virtualization. In                              \n",
       " Proceedings of the 2nd International Conference on Virtual Execution En-                         \n",
       " vironment...                                                                                     \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       " Content Preview \n",
       " Yew. 2006. Live Updating Operating Systems Using Virtualization. In                              \n",
       " Proceedings of the 2nd International Conference on Virtual Execution En-                         \n",
       " vironment...                                                                                     \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2306.15076v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2306.15076v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3313</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3313\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Content Preview \n",
       " AIOS: LLM Agent Operating System                                                                 \n",
       " not using AIOS widens as the number of agents increases,                                         \n",
       " underscoring AIOSs effectiveness in managing concurrent                                         \n",
       " ope...                                                                                           \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       " Content Preview \n",
       " AIOS: LLM Agent Operating System                                                                 \n",
       " not using AIOS widens as the number of agents increases,                                         \n",
       " underscoring AIOSs effectiveness in managing concurrent                                         \n",
       " ope...                                                                                           \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2403.16971v3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2403.16971v3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Total unique documents:</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mTotal unique documents:\u001b[0m \u001b[1;36m11\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Generating combined answer based on multiple query results...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mGenerating combined answer based on multiple query results\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:39:19,069 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">This appears to be a general knowledge question. Using LLM knowledge.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mThis appears to be a general knowledge question. Using LLM knowledge.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:39:19,406 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'An operating system (OS) is a software that manages computer hardware resources and provides a platform for running application software. It acts as an intermediary between computer hardware and user-level applications.\\n\\nThe primary functions of an operating system include:\\n\\n1. Process management: Creating, scheduling, and terminating tasks (programs) to optimize system performance.\\n2. Memory management: Allocating and deallocating memory for running programs.\\n3. File system management: Managing files, directories, and storage devices.\\n4. Input/Output operations: Handling user input and output, such as keyboard and mouse interactions, display output, and printer management.\\n5. Security and access control: Regulating access to computer resources based on user identity and permissions.\\n6. Networking: Enabling communication between the computer and other devices on a network.\\n\\nOperating systems can be classified into several types, including:\\n\\n1. Single-user, single-tasking operating systems (e.g., MS-DOS)\\n2. Multi-user, multi-tasking operating systems (e.g., Windows XP, macOS)\\n3. Mobile operating systems (e.g., Android, iOS)\\n\\nThe operating system software provides a layer of abstraction between the hardware and user-level applications, allowing users to interact with the computer using high-level commands and programs, rather than directly accessing low-level hardware components.\\n\\nIn summary, an operating system is essential software that manages computer resources, provides a platform for running applications, and enables users to interact with their computers in a convenient and efficient manner.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:39:28,105 - langsmith.client - WARNING - Failed to send compressed multipart ingest: Connection error caused failure to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. Please confirm your internet connection. ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'api.smith.langchain.com\\', port=443): Max retries exceeded with url: /runs/multipart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x3142727d0>: Failed to resolve \\'api.smith.langchain.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))'))\n",
      "Content-Length: 1176\n",
      "API Key: lsv2_********************************************82\n",
      "2025-04-17 13:39:37,707 - langsmith.client - WARNING - Failed to send compressed multipart ingest: Connection error caused failure to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. Please confirm your internet connection. ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'api.smith.langchain.com\\', port=443): Max retries exceeded with url: /runs/multipart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x3190cf310>: Failed to resolve \\'api.smith.langchain.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))'))\n",
      "Content-Length: 6919\n",
      "API Key: lsv2_********************************************82\n"
     ]
    }
   ],
   "source": [
    "question = input(\"Enter your query\")\n",
    "assistant.answer_query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "065f4cf3-5efd-4f03-b2c0-08167871569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "k= []\n",
    "if k:\n",
    "    preint(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cae4ba5-ecf9-445b-9c1f-45a5d6438aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
