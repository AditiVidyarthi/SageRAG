{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "73c5da5b-056f-49dc-8464-5cfbd1492c5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m443.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m480.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m463.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-19.0.1-cp311-cp311-macosx_12_0_arm64.whl (30.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m279.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Installing collected packages: xxhash, tqdm, pyarrow, dill, multiprocess, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Uninstalling tqdm-4.65.0:\n",
      "      Successfully uninstalled tqdm-4.65.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.2\n",
      "    Uninstalling pyarrow-14.0.2:\n",
      "      Successfully uninstalled pyarrow-14.0.2\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.30.0 requires packaging<24,>=16.8, but you have packaging 24.2 which is incompatible.\n",
      "streamlit 1.30.0 requires protobuf<5,>=3.20, but you have protobuf 5.29.4 which is incompatible.\n",
      "streamlit 1.30.0 requires tenacity<9,>=8.1.0, but you have tenacity 9.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-19.0.1 tqdm-4.67.1 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3b354f4-0b65-4e2e-8c7e-ad6f57c0271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"ResearchPro2\" \n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = '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'\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8f6906f-0a67-4bde-ac97-8d6e3cfeede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import necessary packages\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import BaseOutputParser, StrOutputParser\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "from typing import List, Literal\n",
    "import re\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67465538-9a43-4952-9d20-79b8be8800d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_DIR = \"../../arxiv_data\"  # Go up two levels, then into arxiv_data\n",
    "DB_DIR = \"../../arxiv_vector1_db\"  # Go up two levels, then into arxiv_vector1_db\n",
    "\n",
    "# Embedding & Vector DB\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_db = Chroma(persist_directory=DB_DIR, embedding_function=embedding_model)\n",
    "\n",
    "# Load embedding model once for scoring\n",
    "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# LLM\n",
    "llm = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "# print(os.getcwd())\n",
    "\n",
    "# import os\n",
    "\n",
    "# # Get current working directory\n",
    "# cwd = os.getcwd()\n",
    "\n",
    "# # Move up two levels\n",
    "# target_path = os.path.abspath(os.path.join(cwd, \"../../arxiv_vector1_db\"))\n",
    "\n",
    "# print(\"Current Directory:\", cwd)\n",
    "# print(\"Resolved ../../arxiv_vector1_db :\", target_path)\n",
    "# # List files/directories at that location\n",
    "# files = os.listdir(target_path)\n",
    "\n",
    "# print(f\"Contents of {target_path}:\")\n",
    "# for f in files:\n",
    "#     print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9309a983-5c6d-4777-8d85-5b325502e401",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your query (or 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, BaseOutputParser\n",
    "from langchain.schema import Document, HumanMessage, AIMessage\n",
    "from langchain.vectorstores import VectorStore\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms.base import BaseLLM\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Output Parser\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        lines = re.findall(r\"^\\d+\\.\\s+(.*)\", text, re.MULTILINE)\n",
    "        return [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "class ResearchAssistant:\n",
    "    def __init__(self, llm: BaseLLM, vector_db: VectorStore, embeddings_model: Optional[SentenceTransformer] = None):\n",
    "        \"\"\"Initialize the Research Assistant with LLM and vector database.\"\"\"\n",
    "        self.llm = llm\n",
    "        self.vector_db = vector_db\n",
    "        \n",
    "        # Initialize sentence transformer model for semantic similarity if not provided\n",
    "        if embeddings_model is None:\n",
    "            self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        else:\n",
    "            self.semantic_model = embeddings_model\n",
    "            \n",
    "        # Initialize memory - we'll manage it manually instead of using ConversationalRetrievalChain\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        # Create a retriever for direct use\n",
    "        self.retriever = vector_db.as_retriever()\n",
    "    \n",
    "    def rate_query(self, query: str) -> Dict[str, str | int]:\n",
    "        \"\"\"Rates the query and gives an explanation.\"\"\"\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an intelligent assistant trained to evaluate search queries.\n",
    "        Given the following query: \"{query}\"\n",
    "        1. Rate the query on a scale of 1 (very poor) to 5 (excellent) based on:\n",
    "           - Clarity\n",
    "           - Specificity\n",
    "           - Relevance\n",
    "           - Retrievability\n",
    "        2. Provide a short explanation for the rating (if it is vague, incomplete, or inefficient).\n",
    "        Respond in JSON format:\n",
    "        {{\n",
    "          \"rating\": 3,\n",
    "          \"explanation\": \"Too vague, lacks keywords.\"\n",
    "        }}\n",
    "        \"\"\")\n",
    "        parser = JsonOutputParser()\n",
    "        chain: Runnable = prompt | self.llm | parser\n",
    "        return chain.invoke({\"query\": query})\n",
    "    \n",
    "    def suggest_rewrites(self, query: str) -> List[str]:\n",
    "        \"\"\"Returns 5 rephrased versions of the query.\"\"\"\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\"],\n",
    "            template=\"\"\"You are an AI assistant. Rephrase the question in 5 different ways to improve retrieval from a document store. Number each version starting with 1.\\nQuestion: {question}\"\"\"\n",
    "        )\n",
    "        return (prompt | self.llm | LineListOutputParser()).invoke({\"question\": query})\n",
    "    \n",
    "    def compute_confidence(self, original: str, rewritten: str) -> float:\n",
    "        \"\"\"Computes semantic similarity between original and rewritten queries.\"\"\"\n",
    "        vec_orig = self.semantic_model.encode(original, convert_to_tensor=True)\n",
    "        vec_rewrite = self.semantic_model.encode(rewritten, convert_to_tensor=True)\n",
    "        return util.pytorch_cos_sim(vec_orig, vec_rewrite).item()\n",
    "    \n",
    "    def score_queries(self, original: str, queries: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Scores and sorts rephrased queries by confidence.\"\"\"\n",
    "        scored = [(q, self.compute_confidence(original, q)) for q in queries]\n",
    "        scored.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scored\n",
    "    \n",
    "    def present_query_options(self, original: str, queries: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Presents the original and rewritten queries with confidence scores.\"\"\"\n",
    "        # Add original query at the top\n",
    "        scored_queries = [(\"Original: \" + original, 1.0)]\n",
    "        \n",
    "        # Add scored rewritten queries\n",
    "        rewritten_scores = self.score_queries(original, queries)\n",
    "        for i, (query, score) in enumerate(rewritten_scores, 1):\n",
    "            scored_queries.append((f\"Rewrite {i}: {query}\", score))\n",
    "            \n",
    "        # Print options for user\n",
    "        print(\"Available search queries (with confidence scores):\")\n",
    "        for i, (query, score) in enumerate(scored_queries):\n",
    "            print(f\"{i}. {query} [Confidence: {score:.2f}]\")\n",
    "            \n",
    "        return scored_queries\n",
    "    \n",
    "    def retrieve_documents(self, queries: List[str], k: int = 5) -> Tuple[List[Document], Dict[str, List[Document]]]:\n",
    "        \"\"\"\n",
    "        Retrieves documents using multiple queries, preserving which query returned which documents.\n",
    "        Shows relevance scores for better transparency.\n",
    "        \"\"\"\n",
    "        all_docs = []\n",
    "        unique_docs = {}\n",
    "        query_docs_map = {}\n",
    "        \n",
    "        # Retrieve docs for each query\n",
    "        for query in queries:\n",
    "            print(f\"\\nQuery: '{query}'\")\n",
    "            results = self.vector_db.similarity_search_with_relevance_scores(query, k=k)\n",
    "            docs = []\n",
    "            \n",
    "            for i, (doc, score) in enumerate(results, 1):\n",
    "                print(f\"--- Result {i} ---\")\n",
    "                print(f\"Score: {score:.4f}\")\n",
    "                print(f\"Chunk:\\n{doc.page_content[:150]}...\")\n",
    "                if hasattr(doc, 'metadata') and doc.metadata:\n",
    "                    source = doc.metadata.get('source', 'Unknown')\n",
    "                    print(f\"Source: {source}\")\n",
    "                print()\n",
    "                \n",
    "                docs.append(doc)\n",
    "                \n",
    "                # Add to our overall collection with deduplication\n",
    "                if doc.page_content not in unique_docs:\n",
    "                    unique_docs[doc.page_content] = doc\n",
    "                    all_docs.append(doc)\n",
    "            \n",
    "            query_docs_map[query] = docs\n",
    "        \n",
    "        print(f\"\\nTotal unique documents: {len(all_docs)}\")\n",
    "        return all_docs, query_docs_map\n",
    "\n",
    "    def rerank_docs(self, query: str, docs: List[Document]) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"Reranks documents based on semantic similarity to the query.\"\"\"\n",
    "        query_embedding = self.semantic_model.encode(query, convert_to_tensor=True)\n",
    "        reranked = []\n",
    "        for doc in docs:\n",
    "            doc_embedding = self.semantic_model.encode(doc.page_content, convert_to_tensor=True)\n",
    "            score = util.pytorch_cos_sim(query_embedding, doc_embedding).item()\n",
    "            reranked.append((doc, score))\n",
    "        return sorted(reranked, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    def can_answer_without_retrieval(self, question: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Determines if a question can be answered directly from memory without retrieval.\n",
    "        Returns a tuple: (can_answer, answer_if_available)\n",
    "        \"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        if not chat_history:\n",
    "            return False, None\n",
    "            \n",
    "        # Create the prompt to check if we can answer directly from the conversation\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an AI assistant helping with a conversation.\n",
    "        Given the following conversation history and a new question, determine if the question:\n",
    "        1. Can be answered directly based ONLY on the conversation history (like \"what was my previous question?\")\n",
    "        2. Does NOT require retrieving new information from documents\n",
    "        \n",
    "        If both conditions are true, provide the answer. Otherwise, respond with \"NEEDS_RETRIEVAL\".\n",
    "        \n",
    "        Conversation History:\n",
    "        {chat_history}\n",
    "        \n",
    "        New Question: {question}\n",
    "        \n",
    "        Your assessment (answer directly or respond with \"NEEDS_RETRIEVAL\"):\n",
    "        \"\"\")\n",
    "        \n",
    "        # Format the chat history for context\n",
    "        history_str = \"\"\n",
    "        for message in chat_history:\n",
    "            if hasattr(message, \"content\"):\n",
    "                role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                history_str += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        # Ask the LLM if this can be answered without retrieval\n",
    "        response = self.llm.invoke(\n",
    "            prompt.format(chat_history=history_str, question=question)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # If the response is \"NEEDS_RETRIEVAL\", we need to use retrieval\n",
    "        if \"NEEDS_RETRIEVAL\" in response:\n",
    "            return False, None\n",
    "        else:\n",
    "            return True, response\n",
    "    \n",
    "    def generate_final_answer(self, question: str, docs: List[Document], max_docs: int = 5) -> str:\n",
    "        \"\"\"Generates the final answer from retrieved documents.\"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history:\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "            \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful research assistant. \n",
    "            Use the following retrieved context chunks to answer the user's question thoroughly.\n",
    "            If the information isn't in the context, indicate what's missing rather than making up information.\n",
    "            If the context contains conflicting or uncertain information, highlight the disagreement. \n",
    "            Do not fabricate any facts not grounded in the provided context.\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        context = \"\\n\\n---\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" \n",
    "                                     for i, doc in enumerate(docs[:max_docs])])\n",
    "        \n",
    "        answer = (prompt | self.llm).invoke({\n",
    "            \"question\": question, \n",
    "            \"context\": context,\n",
    "            \"chat_history\": chat_context\n",
    "        })\n",
    "        \n",
    "        # Save the QA pair to memory manually\n",
    "        self.memory.chat_memory.add_messages([\n",
    "            HumanMessage(content=question),\n",
    "            AIMessage(content=answer)\n",
    "        ])\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def generate_combined_answer(self, question: str, query_results: Dict[str, List[Document]], max_docs_per_query: int = 3) -> str:\n",
    "        \"\"\"Generates a combined answer from multiple query results.\"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history:\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        # Combine all relevant documents from different queries\n",
    "        all_context_sections = []\n",
    "        for query, docs in query_results.items():\n",
    "            # Use only the top N documents for each query\n",
    "            docs_for_query = docs[:max_docs_per_query]\n",
    "            if docs_for_query:\n",
    "                all_context_sections.append(f\"Results for query: '{query}'\")\n",
    "                for i, doc in enumerate(docs_for_query, 1):\n",
    "                    all_context_sections.append(f\"Document {i}:\\n{doc.page_content}\")\n",
    "        \n",
    "        # Join all context sections\n",
    "        combined_context = \"\\n\\n---\\n\\n\".join(all_context_sections)\n",
    "        \n",
    "        # Create a prompt that emphasizes synthesizing information across different query results\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful research assistant. \n",
    "            The user's question has been reformulated in several ways, and each formulation returned different documents.\n",
    "            Use the following retrieved context chunks from ALL query variations to comprehensively answer the user's question.\n",
    "            Synthesize information across all retrieved documents to provide the most complete answer possible.\n",
    "            If the information isn't in the context, indicate what's missing rather than making up information.\n",
    "            If the context contains conflicting information from different query results, highlight the disagreement.\n",
    "            Do not fabricate any facts not grounded in the provided context.\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Context from multiple query formulations:\n",
    "            {context}\n",
    "            \n",
    "            Original Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        answer = (prompt | self.llm).invoke({\n",
    "            \"question\": question, \n",
    "            \"context\": combined_context,\n",
    "            \"chat_history\": chat_context\n",
    "        })\n",
    "        \n",
    "        # Save the QA pair to memory manually\n",
    "        self.memory.chat_memory.add_messages([\n",
    "            HumanMessage(content=question),\n",
    "            AIMessage(content=answer)\n",
    "        ])\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def search_with_query_feedback(self, query: str, num_results: int = 5) -> str:\n",
    "        \"\"\"Main pipeline that processes a query and returns an answer.\"\"\"\n",
    "        # Step 1: Rate the original query\n",
    "        print(\"Evaluating your query...\")\n",
    "        rating_result = self.rate_query(query)\n",
    "        print(f\"Query Rating: {rating_result['rating']}/5\")\n",
    "        print(f\"Explanation: {rating_result['explanation']}\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 2: Suggest rewrites if the rating is less than perfect\n",
    "        rewritten_queries = []\n",
    "        if rating_result[\"rating\"] < 5:\n",
    "            print(\"Generating improved query variations...\")\n",
    "            rewritten_queries = self.suggest_rewrites(query)\n",
    "            print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 3: Present options to the user\n",
    "        query_options = self.present_query_options(query, rewritten_queries)\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 4: Get user selection\n",
    "        selected_indices = input(\"Enter the numbers of the queries you want to use (comma-separated, e.g., '0,2,3'): \")\n",
    "        try:\n",
    "            selected_indices = [int(idx.strip()) for idx in selected_indices.split(\",\")]\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter numbers separated by commas.\")\n",
    "            return \"Invalid query selection. Please try again.\"\n",
    "        \n",
    "        # Get the selected queries (without the prefix and score)\n",
    "        selected_queries = []\n",
    "        for idx in selected_indices:\n",
    "            if idx == 0:  # Original query\n",
    "                selected_queries.append(query)\n",
    "            else:  # Rewritten query\n",
    "                # Check if the index is valid\n",
    "                if idx <= len(rewritten_queries):\n",
    "                    query_text = rewritten_queries[idx-1]\n",
    "                    selected_queries.append(query_text)\n",
    "        \n",
    "        if not selected_queries:\n",
    "            print(\"No valid queries selected.\")\n",
    "            return \"No valid queries were selected. Please try again.\"\n",
    "            \n",
    "        print(f\"\\nSelected {len(selected_queries)} queries for retrieval.\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 5: Retrieve documents\n",
    "        print(\"Retrieving relevant documents...\")\n",
    "        docs, query_docs_map = self.retrieve_documents(selected_queries, k=num_results)\n",
    "        \n",
    "        # Step 6: Generate answer\n",
    "        # If using multiple queries, use the combined answer approach\n",
    "        if len(selected_queries) > 1:\n",
    "            print(\"\\nGenerating combined answer based on multiple query results...\")\n",
    "            final_answer = self.generate_combined_answer(query, query_docs_map, max_docs_per_query=3)\n",
    "        else:\n",
    "            # For single query, rerank and use the traditional approach\n",
    "            print(\"\\nReranking documents based on relevance to the original query...\")\n",
    "            reranked_docs_with_scores = self.rerank_docs(query, docs)\n",
    "    \n",
    "            # Apply relevance threshold\n",
    "            relevance_threshold = 0.6  # adjust as needed\n",
    "            filtered_docs_with_scores = [\n",
    "                (doc, score) for doc, score in reranked_docs_with_scores if score >= relevance_threshold\n",
    "            ]\n",
    "            \n",
    "            # If no documents meet the threshold, use all documents\n",
    "            if not filtered_docs_with_scores:\n",
    "                print(\"No documents met the relevance threshold. Using all retrieved documents.\")\n",
    "                filtered_docs_with_scores = reranked_docs_with_scores\n",
    "            \n",
    "            # Sort top N\n",
    "            filtered_docs_with_scores = sorted(filtered_docs_with_scores, key=lambda x: x[1], reverse=True)\n",
    "            filtered_docs = [doc for doc, _ in filtered_docs_with_scores]\n",
    "            \n",
    "            # Display top reranked documents\n",
    "            print(\"\\nTop reranked documents:\")\n",
    "            for i, (doc, score) in enumerate(filtered_docs_with_scores[:num_results], 1):\n",
    "                print(f\"{i}. Score: {score:.4f}\")\n",
    "                print(f\"   Preview: {doc.page_content[:150]}...\")\n",
    "                if hasattr(doc, 'metadata') and doc.metadata:\n",
    "                    print(f\"   Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "                    \n",
    "            # Generate the final answer\n",
    "            print(\"\\nGenerating your answer based on retrieved documents...\")\n",
    "            final_answer = self.generate_final_answer(query, filtered_docs[:num_results])\n",
    "        \n",
    "        return final_answer\n",
    "    \n",
    "    def ask_with_memory(self, question: str, num_results: int = 5) -> str:\n",
    "        \"\"\"\n",
    "        Uses conversation memory to process follow-up questions.\n",
    "        Now includes query improvement and document retrieval.\n",
    "        \"\"\"\n",
    "        # First, check if this is a question we can answer directly from memory\n",
    "        can_direct_answer, direct_answer = self.can_answer_without_retrieval(question)\n",
    "        if can_direct_answer:\n",
    "            print(\"Question can be answered directly from conversation history...\")\n",
    "            # Save the QA pair to memory manually\n",
    "            self.memory.chat_memory.add_messages([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=direct_answer)\n",
    "            ])\n",
    "            return direct_answer\n",
    "        \n",
    "        # Otherwise, use the full query improvement pipeline\n",
    "        # Step 1: Rate the query\n",
    "        print(\"Evaluating your query (with memory context)...\")\n",
    "        rating_result = self.rate_query(question)\n",
    "        print(f\"Query Rating: {rating_result['rating']}/5\")\n",
    "        print(f\"Explanation: {rating_result['explanation']}\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 2: Suggest rewrites if the rating is less than perfect\n",
    "        rewritten_queries = []\n",
    "        if rating_result[\"rating\"] < 5:\n",
    "            print(\"Generating improved query variations...\")\n",
    "            rewritten_queries = self.suggest_rewrites(question)\n",
    "            print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 3: Present options to the user\n",
    "        query_options = self.present_query_options(question, rewritten_queries)\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 4: Get user selection\n",
    "        selected_indices = input(\"Enter the numbers of the queries you want to use (comma-separated, e.g., '0,2,3'): \")\n",
    "        try:\n",
    "            selected_indices = [int(idx.strip()) for idx in selected_indices.split(\",\")]\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter numbers separated by commas.\")\n",
    "            return \"Invalid query selection. Please try again.\"\n",
    "        \n",
    "        # Get the selected queries\n",
    "        selected_queries = []\n",
    "        for idx in selected_indices:\n",
    "            if idx == 0:  # Original query\n",
    "                selected_queries.append(question)\n",
    "            else:  # Rewritten query\n",
    "                # Check if the index is valid\n",
    "                if idx <= len(rewritten_queries):\n",
    "                    query_text = rewritten_queries[idx-1]\n",
    "                    selected_queries.append(query_text)\n",
    "        \n",
    "        if not selected_queries:\n",
    "            print(\"No valid queries selected.\")\n",
    "            return \"No valid queries were selected. Please try again.\"\n",
    "            \n",
    "        print(f\"\\nSelected {len(selected_queries)} queries for retrieval.\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 5: Retrieve documents\n",
    "        print(\"Retrieving relevant documents...\")\n",
    "        if len(selected_queries) > 1:\n",
    "            # If multiple queries were selected, use the multi-query approach\n",
    "            docs, query_docs_map = self.retrieve_documents(selected_queries, k=num_results)\n",
    "            \n",
    "            # Generate a combined answer\n",
    "            print(\"\\nGenerating combined answer with conversation context...\")\n",
    "            answer = self.generate_combined_answer(question, query_docs_map)\n",
    "        else:\n",
    "            # If only one query was selected, use the standard approach\n",
    "            docs = self.retriever.get_relevant_documents(selected_queries[0])\n",
    "            \n",
    "            print(f\"Retrieved {len(docs)} documents\")\n",
    "            \n",
    "            # Show previews of the retrieved documents\n",
    "            print(\"\\nTop retrieved documents:\")\n",
    "            for i, doc in enumerate(docs[:5], 1):\n",
    "                print(f\"{i}. Preview: {doc.page_content[:150]}...\")\n",
    "                if hasattr(doc, 'metadata') and doc.metadata:\n",
    "                    print(f\"   Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "            \n",
    "            # Generate answer with memory context\n",
    "            print(\"\\nGenerating answer with conversation context...\")\n",
    "            answer = self.generate_final_answer(question, docs)\n",
    "        \n",
    "        # The answer is returned but not printed again\n",
    "        return answer\n",
    "    \n",
    "    def process_query(self, query: str, use_memory: bool = False, num_results: int = 5) -> str:\n",
    "        \"\"\"Main entry point that decides whether to use memory or the full pipeline.\"\"\"\n",
    "        # Check if this might be a follow-up question that should use memory\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        if use_memory or (chat_history and self._is_likely_followup(query)):\n",
    "            print(\"Using conversation memory to process this query...\")\n",
    "            return self.ask_with_memory(query, num_results)\n",
    "        else:\n",
    "            print(\"Using full query improvement pipeline...\")\n",
    "            return self.search_with_query_feedback(query, num_results)\n",
    "    \n",
    "    def _is_likely_followup(self, query: str) -> bool:\n",
    "        \"\"\"Heuristically determines if a query is likely a follow-up question.\"\"\"\n",
    "        # Look for pronouns, references, and questions that seem to refer to previous context\n",
    "        followup_indicators = [\n",
    "            \"it\", \"this\", \"that\", \"they\", \"them\", \"these\", \"those\",\n",
    "            \"previous\", \"earlier\", \"above\", \"mentioned\",\n",
    "            \"what about\", \"how about\", \"tell me more\", \"elaborate\",\n",
    "            \"why\", \"how does\", \"can you explain\"\n",
    "        ]\n",
    "        \n",
    "        query_lower = query.lower()\n",
    "        return any(indicator in query_lower for indicator in followup_indicators)\n",
    "\n",
    "# Example usage\n",
    "def main():    \n",
    "    # Initialize the research assistant\n",
    "    assistant = ResearchAssistant(llm=llm, vector_db=vector_db)\n",
    "    \n",
    "    # Main interaction loop\n",
    "    while True:\n",
    "        query = input(\"\\nEnter your query (or 'exit' to quit): \")\n",
    "        if query.lower() == 'exit':\n",
    "            break\n",
    "            \n",
    "        # Ask the user whether to use memory mode or full pipeline\n",
    "        use_memory = input(\"Use conversation memory? (y/n): \").lower() == 'y'\n",
    "        \n",
    "        # Process the query\n",
    "        answer = assistant.process_query(query, use_memory=use_memory)\n",
    "        # Only print the final answer once\n",
    "        print(\"\\nFinal Answer:\", answer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d45fdf5-fe26-4291-9586-11f246b87e65",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your query (or 'exit' to quit):  What is operating system ?\n",
      "Use conversation memory? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using conversation memory to process this query...\n",
      "Evaluating your query (with memory context)...\n",
      "Query Rating: 4/5\n",
      "Explanation: The query is clear and concise. It directly asks about the definition of an operating system, which can be easily understood by most users. However, it could be more specific to improve retrievability. Adding relevant keywords like 'computer software' or 'system management' would increase the chances of getting precise results.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Generating improved query variations...\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Available search queries (with confidence scores):\n",
      "0. Original: What is operating system ? [Confidence: 1.00]\n",
      "1. Rewrite 1: What category of computer systems manages input/output operations, processes instructions, and interacts with users and other devices? [Confidence: 0.58]\n",
      "2. Rewrite 2: What category of program manages computer hardware components, allocates system resources, and facilitates communication between hardware devices and software applications? [Confidence: 0.50]\n",
      "3. Rewrite 3: What type of software controls hardware components and provides a platform for running applications? [Confidence: 0.46]\n",
      "4. Rewrite 4: Which fundamental component of computing systems determines how hardware resources such as memory, storage, and peripherals are allocated to various applications? [Confidence: 0.46]\n",
      "5. Rewrite 5: What classification of software dictates the behavior of a computer's central processing unit (CPU), provides an interface for user interaction, and coordinates system operations? [Confidence: 0.45]\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the numbers of the queries you want to use (comma-separated, e.g., '0,2,3'):  2,3,4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected 3 queries for retrieval.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieving relevant documents...\n",
      "\n",
      "Query: 'What category of computer systems manages input/output operations, processes instructions, and interacts with users and other devices?'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected where operator to be one of $gt, $gte, $lt, $lte, $ne, $eq, $in, $nin, got chunk_type in query.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/chromadb/api/models/CollectionCommon.py:90\u001b[0m, in \u001b[0;36mvalidation_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/chromadb/api/models/CollectionCommon.py:294\u001b[0m, in \u001b[0;36mCollectionCommon._validate_and_prepare_query_request\u001b[0;34m(self, query_embeddings, query_texts, query_images, query_uris, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    293\u001b[0m validate_base_record_set(record_set\u001b[38;5;241m=\u001b[39mquery_records)\n\u001b[0;32m--> 294\u001b[0m validate_filter_set(filter_set\u001b[38;5;241m=\u001b[39mfilters)\n\u001b[1;32m    295\u001b[0m validate_include(include\u001b[38;5;241m=\u001b[39minclude)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/chromadb/api/types.py:345\u001b[0m, in \u001b[0;36mvalidate_filter_set\u001b[0;34m(filter_set)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filter_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m     validate_where(filter_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filter_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere_document\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/chromadb/api/types.py:659\u001b[0m, in \u001b[0;36mvalidate_where\u001b[0;34m(where)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m operator \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$gt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$gte\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$nin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    658\u001b[0m ]:\n\u001b[0;32m--> 659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    660\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected where operator to be one of $gt, $gte, $lt, $lte, $ne, $eq, $in, $nin, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperator\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    662\u001b[0m     )\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(operand, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n",
      "\u001b[0;31mValueError\u001b[0m: Expected where operator to be one of $gt, $gte, $lt, $lte, $ne, $eq, $in, $nin, got chunk_type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m use_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse conversation memory? (y/n): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Process the query\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m answer \u001b[38;5;241m=\u001b[39m assistant\u001b[38;5;241m.\u001b[39mprocess_query(query, use_memory\u001b[38;5;241m=\u001b[39muse_memory)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal Answer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, answer)\n",
      "Cell \u001b[0;32mIn[18], line 507\u001b[0m, in \u001b[0;36mResearchAssistant.process_query\u001b[0;34m(self, query, use_memory, num_results)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_memory \u001b[38;5;129;01mor\u001b[39;00m (chat_history \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_likely_followup(query)):\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing conversation memory to process this query...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_with_memory(query, num_results)\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing full query improvement pipeline...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 475\u001b[0m, in \u001b[0;36mResearchAssistant.ask_with_memory\u001b[0;34m(self, question, num_results)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrieving relevant documents...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(selected_queries) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# If multiple queries were selected, use the multi-query approach\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     docs, query_docs_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve_documents(selected_queries, k\u001b[38;5;241m=\u001b[39mnum_results)\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# Generate a combined answer\u001b[39;00m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerating combined answer with conversation context...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 120\u001b[0m, in \u001b[0;36mResearchAssistant.retrieve_documents\u001b[0;34m(self, queries, k)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_foundational_query:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# First try to get foundational documents\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     filter_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfoundational\u001b[39m\u001b[38;5;124m\"\u001b[39m}}\n\u001b[0;32m--> 120\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_db\u001b[38;5;241m.\u001b[39msimilarity_search_with_relevance_scores(\n\u001b[1;32m    121\u001b[0m         query, k\u001b[38;5;241m=\u001b[39mk, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39mfilter_dict\n\u001b[1;32m    122\u001b[0m     )\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# If no foundational docs, fall back to regular search\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/vectorstores/base.py:555\u001b[0m, in \u001b[0;36mVectorStore.similarity_search_with_relevance_scores\u001b[0;34m(self, query, k, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return docs and relevance scores in the range [0, 1].\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \n\u001b[1;32m    541\u001b[0m \u001b[38;5;124;03m0 is dissimilar, 1 is most similar.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;124;03m    List of Tuples of (doc, similarity_score).\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    553\u001b[0m score_threshold \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 555\u001b[0m docs_and_similarities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_similarity_search_with_relevance_scores(\n\u001b[1;32m    556\u001b[0m     query, k\u001b[38;5;241m=\u001b[39mk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    557\u001b[0m )\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    559\u001b[0m     similarity \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m similarity \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, similarity \u001b[38;5;129;01min\u001b[39;00m docs_and_similarities\n\u001b[1;32m    561\u001b[0m ):\n\u001b[1;32m    562\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelevance scores must be between\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    564\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m 0 and 1, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocs_and_similarities\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    565\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    566\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/vectorstores/base.py:503\u001b[0m, in \u001b[0;36mVectorStore._similarity_search_with_relevance_scores\u001b[0;34m(self, query, k, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Default similarity search with relevance scores.\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \n\u001b[1;32m    487\u001b[0m \u001b[38;5;124;03mModify if necessary in subclass.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;124;03m    List of Tuples of (doc, similarity_score)\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    502\u001b[0m relevance_score_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_relevance_score_fn()\n\u001b[0;32m--> 503\u001b[0m docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score(query, k, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [(doc, relevance_score_fn(score)) \u001b[38;5;28;01mfor\u001b[39;00m doc, score \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:440\u001b[0m, in \u001b[0;36mChroma.similarity_search_with_score\u001b[0;34m(self, query, k, filter, where_document, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    439\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function\u001b[38;5;241m.\u001b[39membed_query(query)\n\u001b[0;32m--> 440\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__query_collection(\n\u001b[1;32m    441\u001b[0m         query_embeddings\u001b[38;5;241m=\u001b[39m[query_embedding],\n\u001b[1;32m    442\u001b[0m         n_results\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m    443\u001b[0m         where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m,\n\u001b[1;32m    444\u001b[0m         where_document\u001b[38;5;241m=\u001b[39mwhere_document,\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    446\u001b[0m     )\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _results_to_docs_and_scores(results)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/utils/utils.py:53\u001b[0m, in \u001b[0;36mxor_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExactly one argument in each of the following\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m groups must be defined:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(invalid_group_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:157\u001b[0m, in \u001b[0;36mChroma.__query_collection\u001b[0;34m(self, query_texts, query_embeddings, n_results, where, where_document, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import chromadb python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install chromadb`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m     )\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection\u001b[38;5;241m.\u001b[39mquery(  \u001b[38;5;66;03m# type: ignore[return-value]\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     query_texts\u001b[38;5;241m=\u001b[39mquery_texts,\n\u001b[1;32m    159\u001b[0m     query_embeddings\u001b[38;5;241m=\u001b[39mquery_embeddings,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     n_results\u001b[38;5;241m=\u001b[39mn_results,\n\u001b[1;32m    161\u001b[0m     where\u001b[38;5;241m=\u001b[39mwhere,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     where_document\u001b[38;5;241m=\u001b[39mwhere_document,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    164\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/chromadb/api/models/Collection.py:211\u001b[0m, in \u001b[0;36mCollection.query\u001b[0;34m(self, query_embeddings, query_texts, query_images, query_uris, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mquery\u001b[39m(\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    170\u001b[0m     query_embeddings: Optional[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m     ],\n\u001b[1;32m    187\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m QueryResult:\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the n_results nearest neighbor embeddings for provided query_embeddings or query_texts.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m \n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     query_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_prepare_query_request(\n\u001b[1;32m    212\u001b[0m         query_embeddings\u001b[38;5;241m=\u001b[39mquery_embeddings,\n\u001b[1;32m    213\u001b[0m         query_texts\u001b[38;5;241m=\u001b[39mquery_texts,\n\u001b[1;32m    214\u001b[0m         query_images\u001b[38;5;241m=\u001b[39mquery_images,\n\u001b[1;32m    215\u001b[0m         query_uris\u001b[38;5;241m=\u001b[39mquery_uris,\n\u001b[1;32m    216\u001b[0m         n_results\u001b[38;5;241m=\u001b[39mn_results,\n\u001b[1;32m    217\u001b[0m         where\u001b[38;5;241m=\u001b[39mwhere,\n\u001b[1;32m    218\u001b[0m         where_document\u001b[38;5;241m=\u001b[39mwhere_document,\n\u001b[1;32m    219\u001b[0m         include\u001b[38;5;241m=\u001b[39minclude,\n\u001b[1;32m    220\u001b[0m     )\n\u001b[1;32m    222\u001b[0m     query_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_query(\n\u001b[1;32m    223\u001b[0m         collection_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m    224\u001b[0m         query_embeddings\u001b[38;5;241m=\u001b[39mquery_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m         database\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase,\n\u001b[1;32m    231\u001b[0m     )\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_query_response(\n\u001b[1;32m    234\u001b[0m         response\u001b[38;5;241m=\u001b[39mquery_results, include\u001b[38;5;241m=\u001b[39mquery_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    235\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/chromadb/api/models/CollectionCommon.py:93\u001b[0m, in \u001b[0;36mvalidation_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     92\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(msg)\u001b[38;5;241m.\u001b[39mwith_traceback(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/chromadb/api/models/CollectionCommon.py:90\u001b[0m, in \u001b[0;36mvalidation_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     92\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/chromadb/api/models/CollectionCommon.py:294\u001b[0m, in \u001b[0;36mCollectionCommon._validate_and_prepare_query_request\u001b[0;34m(self, query_embeddings, query_texts, query_images, query_uris, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[1;32m    293\u001b[0m validate_base_record_set(record_set\u001b[38;5;241m=\u001b[39mquery_records)\n\u001b[0;32m--> 294\u001b[0m validate_filter_set(filter_set\u001b[38;5;241m=\u001b[39mfilters)\n\u001b[1;32m    295\u001b[0m validate_include(include\u001b[38;5;241m=\u001b[39minclude)\n\u001b[1;32m    296\u001b[0m validate_n_results(n_results\u001b[38;5;241m=\u001b[39mn_results)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/chromadb/api/types.py:345\u001b[0m, in \u001b[0;36mvalidate_filter_set\u001b[0;34m(filter_set)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidate_filter_set\u001b[39m(filter_set: FilterSet) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filter_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m         validate_where(filter_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filter_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere_document\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    347\u001b[0m         validate_where_document(filter_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere_document\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/chromadb/api/types.py:659\u001b[0m, in \u001b[0;36mvalidate_where\u001b[0;34m(where)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    647\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected operand value to be an list for operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperator\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperand\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    648\u001b[0m         )\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m operator \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$gt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$gte\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$nin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    658\u001b[0m ]:\n\u001b[0;32m--> 659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    660\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected where operator to be one of $gt, $gte, $lt, $lte, $ne, $eq, $in, $nin, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperator\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    662\u001b[0m     )\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(operand, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    666\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected where operand value to be a str, int, float, or list of those type, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperand\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    667\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected where operator to be one of $gt, $gte, $lt, $lte, $ne, $eq, $in, $nin, got chunk_type in query."
     ]
    }
   ],
   "source": [
    "# Initialize the research assistant\n",
    "assistant = ResearchAssistant(llm=llm, vector_db=vector_db)\n",
    "while True:\n",
    "    \n",
    "    query = input(\"\\nEnter your query (or 'exit' to quit): \")\n",
    "    if query.lower() == 'exit':\n",
    "        break\n",
    "        \n",
    "    # Ask the user whether to use memory mode or full pipeline\n",
    "    use_memory = input(\"Use conversation memory? (y/n): \").lower() == 'y'\n",
    "    \n",
    "    # Process the query\n",
    "    answer = assistant.process_query(query, use_memory=use_memory)\n",
    "    print(\"\\nFinal Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c9f53c-5fc2-4eee-9000-3cfae300122e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Improving DB with Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f10b5bc4-24cb-4096-941a-2a84a67e8908",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class FoundationalKnowledgeManager:\n",
    "    def __init__(self, vector_db, embeddings_model=None):\n",
    "        self.vector_db = vector_db\n",
    "        self.embeddings_model = embeddings_model\n",
    "        self.loaded_sources = set()  # Track which sources have been loaded\n",
    "        self.url_to_source_map = {\n",
    "            \"cs_algorithms\": \"https://raw.githubusercontent.com/OpenGenus/cs-fundamentals/main/algorithms.md\",\n",
    "            \"cs_data_structures\": \"https://raw.githubusercontent.com/OpenGenus/cs-fundamentals/main/data_structures.md\",\n",
    "            # Add more mappings as needed\n",
    "        }\n",
    "        \n",
    "    def load_from_github(self, topic_key):\n",
    "        \"\"\"Load foundational content from pre-defined GitHub sources\"\"\"\n",
    "        if topic_key not in self.url_to_source_map:\n",
    "            return False\n",
    "            \n",
    "        if topic_key in self.loaded_sources:\n",
    "            print(f\"Topic {topic_key} already loaded.\")\n",
    "            return True\n",
    "            \n",
    "        url = self.url_to_source_map[topic_key]\n",
    "        # Use requests to download the content\n",
    "        import requests\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            content = response.text\n",
    "            # Process and chunk the content\n",
    "            chunks = self._chunk_content(content, topic_key)\n",
    "            # Add chunks to vector DB\n",
    "            self._add_to_vector_db(chunks)\n",
    "            self.loaded_sources.add(topic_key)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def load_from_file(self, file_path, topic_name):\n",
    "        \"\"\"Load foundational content from a local file\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            return False\n",
    "            \n",
    "        source_key = f\"file_{topic_name}\"\n",
    "        if source_key in self.loaded_sources:\n",
    "            print(f\"File {file_path} already loaded as {topic_name}.\")\n",
    "            return True\n",
    "            \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            \n",
    "        # Process and chunk the content\n",
    "        chunks = self._chunk_content(content, topic_name)\n",
    "        # Add chunks to vector DB\n",
    "        self._add_to_vector_db(chunks)\n",
    "        self.loaded_sources.add(source_key)\n",
    "        return True\n",
    "        \n",
    "    def _chunk_content(self, content, source_name, chunk_size=1000, overlap=200):\n",
    "        \"\"\"Split content into overlapping chunks\"\"\"\n",
    "        chunks = []\n",
    "        # Simple chunking by character count\n",
    "        start = 0\n",
    "        while start < len(content):\n",
    "            end = min(start + chunk_size, len(content))\n",
    "            if end < len(content) and end - start == chunk_size:\n",
    "                # Try to find a good breaking point\n",
    "                break_point = content.rfind('\\n', start, end)\n",
    "                if break_point > start + chunk_size // 2:\n",
    "                    end = break_point\n",
    "                    \n",
    "            chunk_content = content[start:end]\n",
    "            chunks.append({\n",
    "                \"content\": chunk_content,\n",
    "                \"metadata\": {\n",
    "                    \"source\": source_name,\n",
    "                    \"chunk_type\": \"foundational\",\n",
    "                    \"start_char\": start,\n",
    "                    \"end_char\": end\n",
    "                }\n",
    "            })\n",
    "            start = end - overlap\n",
    "        return chunks\n",
    "        \n",
    "    def _add_to_vector_db(self, chunks):\n",
    "        \"\"\"Add chunks to the vector database\"\"\"\n",
    "        # Convert chunks to Document objects\n",
    "        documents = [\n",
    "            Document(\n",
    "                page_content=chunk[\"content\"],\n",
    "                metadata=chunk[\"metadata\"]\n",
    "            ) for chunk in chunks\n",
    "        ]\n",
    "        \n",
    "        # Use vector_db to add documents\n",
    "        self.vector_db.add_documents(documents)\n",
    "        print(f\"Added {len(documents)} foundational knowledge chunks to vector DB\")\n",
    "        \n",
    "    def load_standard_cs_topics(self):\n",
    "        \"\"\"Load a standard set of CS fundamentals\"\"\"\n",
    "        for topic in self.url_to_source_map.keys():\n",
    "            self.load_from_github(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "445904a8-9cd7-48a0-ab9e-093cfb9bd1a5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class FoundationalKnowledgeManager:\n",
    "    def __init__(self, vector_db, embedding_model):\n",
    "        self.vector_db = vector_db\n",
    "        self.embedding_model = embedding_model\n",
    "        self.github_sources = {\n",
    "            \"algorithms\": \"TheAlgorithms/Python\",\n",
    "            \"os\": \"tuhdo/os01\",\n",
    "            \"system_design\": \"donnemartin/system-design-primer\",\n",
    "            \"python\": \"jakevdp/PythonDataScienceHandbook\",\n",
    "            \"ml\": \"microsoft/ML-For-Beginners\",\n",
    "            \"database\": \"pingcap/awesome-database-learning\",\n",
    "            \"networks\": \"leandromoreira/linux-network-performance-parameters\"\n",
    "        }\n",
    "        \n",
    "    def load_from_github(self, topic_key):\n",
    "        \"\"\"Load content from predefined GitHub repositories with foundational CS knowledge\"\"\"\n",
    "        if topic_key not in self.github_sources:\n",
    "            return f\"Topic {topic_key} not found in available sources\"\n",
    "            \n",
    "        repo = self.github_sources[topic_key]\n",
    "        # Implement GitHub content fetching logic here\n",
    "        # Use GitHub API to get markdown/text content from README files and other docs\n",
    "        \n",
    "        # Process content and add to vector DB with metadata\n",
    "        # Important: Tag these documents as foundational knowledge\n",
    "        # self.vector_db.add_texts(texts, metadatas=[{\"chunk_type\": \"foundational\", \"topic\": topic_key}])\n",
    "        \n",
    "        return f\"Loaded foundational knowledge for {topic_key} from {repo}\"\n",
    "        \n",
    "    def load_from_file(self, file_path, topic_name):\n",
    "        \"\"\"Load content from local file\"\"\"\n",
    "        # Implement file loading logic\n",
    "        # Process content and add to vector DB with metadata \n",
    "        # self.vector_db.add_texts(texts, metadatas=[{\"chunk_type\": \"foundational\", \"topic\": topic_name}])\n",
    "        \n",
    "    def load_standard_cs_topics(self):\n",
    "        \"\"\"Load all standard CS topics\"\"\"\n",
    "        results = []\n",
    "        for topic in self.github_sources:\n",
    "            results.append(self.load_from_github(topic))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a74e76-b2d4-4a1d-9cc7-327f74f0271d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5df57ad7-cddc-446b-a1f2-89602957e755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Initializing SageRAG Research Assistant...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mInitializing SageRAG Research Assistant\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading embedding model<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading embedding model\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading language model<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading language model\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Connecting to vector database<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Connecting to vector database\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Initialization complete!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mInitialization complete!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 23:05:59,520 - SageRAG - INFO - Initializing default embedding model\n",
      "2025-04-18 23:06:03,279 - SageRAG - INFO - Research Assistant initialized successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">╭──────────────────────────────────────────────────── Welcome ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span> ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span> ┃                                         <span style=\"font-weight: bold\">SageRAG Research Assistant</span>                                          ┃ <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span> ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span> Ask questions about scientific papers in the database.                                                          <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m╭─\u001b[0m\u001b[36m───────────────────────────────────────────────────\u001b[0m\u001b[36m Welcome \u001b[0m\u001b[36m───────────────────────────────────────────────────\u001b[0m\u001b[36m─╮\u001b[0m\n",
       "\u001b[36m│\u001b[0m ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ \u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m ┃                                         \u001b[1mSageRAG Research Assistant\u001b[0m                                          ┃ \u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ \u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m                                                                                                                 \u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m Ask questions about scientific papers in the database.                                                          \u001b[36m│\u001b[0m\n",
       "\u001b[36m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Options:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mOptions:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Ask a question\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m. Ask a question\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. View conversation history\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m. View conversation history\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Clear conversation history\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m. Clear conversation history\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. Exit\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4\u001b[0m. Exit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your choice (1-4):  1\n",
      "\n",
      "[bold]Enter your query:[/bold]  What is Agentic RAG(Retrieval Augmented Generation)?\n",
      "Use conversation memory? (y/n):  y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Processing your query...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mProcessing your query\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should_use_memory []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Using full query improvement pipeline...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mUsing full query improvement pipeline\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Evaluating your query...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mEvaluating your query\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 23:06:21,209 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Query Rating: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">/</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mQuery Rating: \u001b[0m\u001b[1;32m4\u001b[0m\u001b[1;32m/\u001b[0m\u001b[1;32m5\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Explanation: The query is clear and concise, with specific technical terms <span style=\"font-weight: bold\">(</span>Agentic RAG, Retrieval Augmented \n",
       "Generation<span style=\"font-weight: bold\">)</span> that indicate a focus on scientific research. However, it may not be perfectly specific, as <span style=\"color: #008000; text-decoration-color: #008000\">'RAG'</span> can \n",
       "also refer to other concepts in the field of artificial intelligence or natural language processing. The relevance \n",
       "to retrieving scientific content is high, and the query should work well with vector search due to its technical \n",
       "specificity.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Explanation: The query is clear and concise, with specific technical terms \u001b[1m(\u001b[0mAgentic RAG, Retrieval Augmented \n",
       "Generation\u001b[1m)\u001b[0m that indicate a focus on scientific research. However, it may not be perfectly specific, as \u001b[32m'RAG'\u001b[0m can \n",
       "also refer to other concepts in the field of artificial intelligence or natural language processing. The relevance \n",
       "to retrieving scientific content is high, and the query should work well with vector search due to its technical \n",
       "specificity.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Generating improved query variations...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mGenerating improved query variations\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 23:06:23,696 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Available search queries:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mAvailable search queries:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">.</span> Original: What is Agentic <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RAG</span><span style=\"font-weight: bold\">(</span>Retrieval Augmented Generation<span style=\"font-weight: bold\">)</span>?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m0\u001b[0m\u001b[1m.\u001b[0m Original: What is Agentic \u001b[1;35mRAG\u001b[0m\u001b[1m(\u001b[0mRetrieval Augmented Generation\u001b[1m)\u001b[0m?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Confidence: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;32mConfidence: \u001b[0m\u001b[1;32m1.00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: **Clearer Formulation Version**: <span style=\"color: #008000; text-decoration-color: #008000\">\"I'm looking for information on a hybrid approach combining </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieval and generation techniques called Agentic RAG, which involves 'augmenting' the process with agential </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">elements, please provide relevant papers or research findings.\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m1\u001b[0m: **Clearer Formulation Version**: \u001b[32m\"I'm looking for information on a hybrid approach combining \u001b[0m\n",
       "\u001b[32mretrieval and generation techniques called Agentic RAG, which involves 'augmenting' the process with agential \u001b[0m\n",
       "\u001b[32melements, please provide relevant papers or research findings.\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.78</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.78\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: **Entity and Relationship Version**: <span style=\"color: #008000; text-decoration-color: #008000\">\"I'm interested in understanding the relationship between </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Retrieval Augmented Generation (RAG) and agential components, specifically Agentic RAG, which seems to incorporate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">concepts from multi-agent systems and cognitive architectures, please provide papers or research findings on this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">topic.\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m2\u001b[0m: **Entity and Relationship Version**: \u001b[32m\"I'm interested in understanding the relationship between \u001b[0m\n",
       "\u001b[32mRetrieval Augmented Generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and agential components, specifically Agentic RAG, which seems to incorporate \u001b[0m\n",
       "\u001b[32mconcepts from multi-agent systems and cognitive architectures, please provide papers or research findings on this \u001b[0m\n",
       "\u001b[32mtopic.\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.77</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.77\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: **Synonym and Related Concepts Version**: <span style=\"color: #008000; text-decoration-color: #008000\">\"I'd like to investigate the concept of 'Agentic RAG', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">which appears to be a variant of Retrieval Augmented Generation incorporating agent-based methods, similar to those</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">used in multi-agent systems or cognitive architectures, can you point me to relevant papers?\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m3\u001b[0m: **Synonym and Related Concepts Version**: \u001b[32m\"I'd like to investigate the concept of 'Agentic RAG', \u001b[0m\n",
       "\u001b[32mwhich appears to be a variant of Retrieval Augmented Generation incorporating agent-based methods, similar to those\u001b[0m\n",
       "\u001b[32mused in multi-agent systems or cognitive architectures, can you point me to relevant papers?\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.75</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.75\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>: **Technical Terminology Version**: <span style=\"color: #008000; text-decoration-color: #008000\">\"Retrieval Augmented Generation (RAG) paradigm utilizing agential </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">components, specifically seeking definitions and applications of Agentic RAG in NLP tasks.\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m4\u001b[0m: **Technical Terminology Version**: \u001b[32m\"Retrieval Augmented Generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m paradigm utilizing agential \u001b[0m\n",
       "\u001b[32mcomponents, specifically seeking definitions and applications of Agentic RAG in NLP tasks.\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.71</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.71\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>: **Varying Syntax Version**: <span style=\"color: #008000; text-decoration-color: #008000\">\"Can you provide an overview of Agentic RAG, a novel NLP technique that </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">combines retrieval and generation, utilizing agents to augment the process, and highlighting its potential </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">applications in information retrieval and text generation.\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m5\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m5\u001b[0m: **Varying Syntax Version**: \u001b[32m\"Can you provide an overview of Agentic RAG, a novel NLP technique that \u001b[0m\n",
       "\u001b[32mcombines retrieval and generation, utilizing agents to augment the process, and highlighting its potential \u001b[0m\n",
       "\u001b[32mapplications in information retrieval and text generation.\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.67</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.67\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>: Using specific technical terminology like <span style=\"color: #008000; text-decoration-color: #008000\">\"Retrieval Augmented Generation\"</span> and <span style=\"color: #008000; text-decoration-color: #008000\">\"agential components\"</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m6\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m6\u001b[0m: Using specific technical terminology like \u001b[32m\"Retrieval Augmented Generation\"\u001b[0m and \u001b[32m\"agential components\"\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.62</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.62\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>: Including key entities <span style=\"font-weight: bold\">(</span>RAG, agential components<span style=\"font-weight: bold\">)</span> and relationships <span style=\"font-weight: bold\">(</span>combinations of retrieval and \n",
       "generation, incorporating agents<span style=\"font-weight: bold\">)</span> from the original query.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m7\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m7\u001b[0m: Including key entities \u001b[1m(\u001b[0mRAG, agential components\u001b[1m)\u001b[0m and relationships \u001b[1m(\u001b[0mcombinations of retrieval and \n",
       "generation, incorporating agents\u001b[1m)\u001b[0m from the original query.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.56</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.56\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>: Breaking down the complex query into clearer formulations using phrases like <span style=\"color: #008000; text-decoration-color: #008000\">\"hybrid approach </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">combining retrieval and generation techniques\"</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m8\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m8\u001b[0m: Breaking down the complex query into clearer formulations using phrases like \u001b[32m\"hybrid approach \u001b[0m\n",
       "\u001b[32mcombining retrieval and generation techniques\"\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.34</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.34\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>: Adding relevant synonyms or related concepts, such as <span style=\"color: #008000; text-decoration-color: #008000\">\"agent-based methods\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"multi-agent systems\"</span>, \n",
       "or <span style=\"color: #008000; text-decoration-color: #008000\">\"cognitive architectures\"</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m9\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m9\u001b[0m: Adding relevant synonyms or related concepts, such as \u001b[32m\"agent-based methods\"\u001b[0m, \u001b[32m\"multi-agent systems\"\u001b[0m, \n",
       "or \u001b[32m\"cognitive architectures\"\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.32</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.32\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>: Varying sentence structure while preserving semantic meaning to avoid repetition.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m10\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m10\u001b[0m: Varying sentence structure while preserving semantic meaning to avoid repetition.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.18</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.18\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Select queries to use:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mSelect queries to use:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Enter the numbers of the queries you want to use <span style=\"font-weight: bold\">(</span>comma-separated, e.g., <span style=\"color: #008000; text-decoration-color: #008000\">'0,2,3'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Enter the numbers of the queries you want to use \u001b[1m(\u001b[0mcomma-separated, e.g., \u001b[32m'0,2,3'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  0,1,2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Selected </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> queries for retrieval.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mSelected \u001b[0m\u001b[1;32m3\u001b[0m\u001b[1;32m queries for retrieval.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Retrieving relevant documents...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mRetrieving relevant documents\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG123: multiple queries this is working\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Query:</span> <span style=\"color: #008000; text-decoration-color: #008000\">'What is Agentic RAG(Retrieval Augmented Generation)?'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;34mQuery:\u001b[0m \u001b[32m'What is Agentic RAG\u001b[0m\u001b[32m(\u001b[0m\u001b[32mRetrieval Augmented Generation\u001b[0m\u001b[32m)\u001b[0m\u001b[32m?'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG123: this is results length 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3778</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3778\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ domain question answering, fact verification, slot filling, and entity                           │\n",
       "│ linking, providing a benchmark for knowledge-intensive tasks.                                    │\n",
       "│ Retrieval-Augment...                                                                             │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ domain question answering, fact verification, slot filling, and entity                           │\n",
       "│ linking, providing a benchmark for knowledge-intensive tasks.                                    │\n",
       "│ Retrieval-Augment...                                                                             │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2410.09942v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2410.09942v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3320</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3320\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ 𝑖 = {(𝑥′                                                                                         │\n",
       "│ 𝑗,𝑦′                                                                                             │\n",
       "│ 𝑗)}|𝐷test                                                                                        │\n",
       "│ 𝑖 |                                                                                              │\n",
       "│ 𝑗=1                                                                                              │\n",
       "│ in the same order. The end-to-end performance of the agent𝑀𝑖 can                                 │\n",
       "│ be measured by a utility function (metric) 𝜇𝑖.                                                   │\n",
       "│ Each R...                                                                                        │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ 𝑖 = {(𝑥′                                                                                         │\n",
       "│ 𝑗,𝑦′                                                                                             │\n",
       "│ 𝑗)}|𝐷test                                                                                        │\n",
       "│ 𝑖 |                                                                                              │\n",
       "│ 𝑗=1                                                                                              │\n",
       "│ in the same order. The end-to-end performance of the agent𝑀𝑖 can                                 │\n",
       "│ be measured by a utility function (metric) 𝜇𝑖.                                                   │\n",
       "│ Each R...                                                                                        │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2410.09942v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2410.09942v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.2856</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.2856\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ to address this limitation is to enhance LLMs by retrieving infor-                               │\n",
       "│ mation from external knowledge sources, a technique known as                                     │\n",
       "│ retrieval-augmented ge...                                                                        │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ to address this limitation is to enhance LLMs by retrieving infor-                               │\n",
       "│ mation from external knowledge sources, a technique known as                                     │\n",
       "│ retrieval-augmented ge...                                                                        │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2410.09942v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2410.09942v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.2759</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.2759\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Preprint, Preprint Alireza Salemi and Hamed Zamani                                               │\n",
       "│ Agents Configuration. Following Salemi and Zamani [42], we                                       │\n",
       "│ utilize 18 diverse RAG agents in our exp...                                                      │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Preprint, Preprint Alireza Salemi and Hamed Zamani                                               │\n",
       "│ Agents Configuration. Following Salemi and Zamani [42], we                                       │\n",
       "│ utilize 18 diverse RAG agents in our exp...                                                      │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2410.09942v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2410.09942v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.2750</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.2750\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ 10 Eger et al.                                                                                   │\n",
       "│ Paper chat and QA. Paper chat and question-answering (QA) systems such as ChatGPT, Deepseek,     │\n",
       "│ NotebookLM,                                                                                      │\n",
       "│ ExplainPaper, ChatPDF, and Doc...                                                                │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ 10 Eger et al.                                                                                   │\n",
       "│ Paper chat and QA. Paper chat and question-answering (QA) systems such as ChatGPT, Deepseek,     │\n",
       "│ NotebookLM,                                                                                      │\n",
       "│ ExplainPaper, ChatPDF, and Doc...                                                                │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2502.05151v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2502.05151v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Query:</span> <span style=\"color: #008000; text-decoration-color: #008000\">'**Technical Terminology Version**: \"Retrieval Augmented Generation (RAG) paradigm utilizing agential </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">components, specifically seeking definitions and applications of Agentic RAG in NLP tasks.\"'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;34mQuery:\u001b[0m \u001b[32m'**Technical Terminology Version**: \"Retrieval Augmented Generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m paradigm utilizing agential \u001b[0m\n",
       "\u001b[32mcomponents, specifically seeking definitions and applications of Agentic RAG in NLP tasks.\"'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG123: this is results length 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.5868</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.5868\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ domain question answering, fact verification, slot filling, and entity                           │\n",
       "│ linking, providing a benchmark for knowledge-intensive tasks.                                    │\n",
       "│ Retrieval-Augment...                                                                             │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ domain question answering, fact verification, slot filling, and entity                           │\n",
       "│ linking, providing a benchmark for knowledge-intensive tasks.                                    │\n",
       "│ Retrieval-Augment...                                                                             │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2410.09942v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2410.09942v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.5233</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.5233\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ 10 Eger et al.                                                                                   │\n",
       "│ Paper chat and QA. Paper chat and question-answering (QA) systems such as ChatGPT, Deepseek,     │\n",
       "│ NotebookLM,                                                                                      │\n",
       "│ ExplainPaper, ChatPDF, and Doc...                                                                │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ 10 Eger et al.                                                                                   │\n",
       "│ Paper chat and QA. Paper chat and question-answering (QA) systems such as ChatGPT, Deepseek,     │\n",
       "│ NotebookLM,                                                                                      │\n",
       "│ ExplainPaper, ChatPDF, and Doc...                                                                │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2502.05151v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2502.05151v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.5122</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.5122\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Retrieval-Augmented Generation (RAG) further enhances ICL by incorporating retrieved docu-       │\n",
       "│ ments into the LLM’s context, supplying it with up-to-date ...                                   │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Retrieval-Augmented Generation (RAG) further enhances ICL by incorporating retrieved docu-       │\n",
       "│ ments into the LLM’s context, supplying it with up-to-date ...                                   │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2411.18947v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2411.18947v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.5033</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.5033\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Computational Linguistics, Brussels, Belgium, 2369–2380. https://doi.org/10.                     │\n",
       "│ 18653/v1/D18-1259                                                                                │\n",
       "│ [57] Hamed Zamani and Michael Bendersky. 2024. Stochast...                                       │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Computational Linguistics, Brussels, Belgium, 2369–2380. https://doi.org/10.                     │\n",
       "│ 18653/v1/D18-1259                                                                                │\n",
       "│ [57] Hamed Zamani and Michael Bendersky. 2024. Stochast...                                       │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2410.09942v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2410.09942v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.4953</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.4953\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Learning to Rank for Multiple Retrieval-Augmented Models                                         │\n",
       "│ through Iterative Utility Maximization                                                           │\n",
       "│ Alireza Salemi                                                                                   │\n",
       "│ University of Massachusetts Amherst                                                              │\n",
       "│ Uni...                                                                                           │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Learning to Rank for Multiple Retrieval-Augmented Models                                         │\n",
       "│ through Iterative Utility Maximization                                                           │\n",
       "│ Alireza Salemi                                                                                   │\n",
       "│ University of Massachusetts Amherst                                                              │\n",
       "│ Uni...                                                                                           │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2410.09942v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2410.09942v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Query:</span> <span style=\"color: #008000; text-decoration-color: #008000\">'**Clearer Formulation Version**: \"I'</span>m looking for information on a hybrid approach combining retrieval and \n",
       "generation techniques called Agentic RAG, which involves <span style=\"color: #008000; text-decoration-color: #008000\">'augmenting'</span> the process with agential elements, please \n",
       "provide relevant papers or research findings.\"'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;34mQuery:\u001b[0m \u001b[32m'**Clearer Formulation Version**: \"I'\u001b[0mm looking for information on a hybrid approach combining retrieval and \n",
       "generation techniques called Agentic RAG, which involves \u001b[32m'augmenting'\u001b[0m the process with agential elements, please \n",
       "provide relevant papers or research findings.\"'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG123: this is results length 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.4484</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.4484\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ domain question answering, fact verification, slot filling, and entity                           │\n",
       "│ linking, providing a benchmark for knowledge-intensive tasks.                                    │\n",
       "│ Retrieval-Augment...                                                                             │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ domain question answering, fact verification, slot filling, and entity                           │\n",
       "│ linking, providing a benchmark for knowledge-intensive tasks.                                    │\n",
       "│ Retrieval-Augment...                                                                             │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2410.09942v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2410.09942v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.4210</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.4210\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Preprint, Preprint Alireza Salemi and Hamed Zamani                                               │\n",
       "│ Agents Configuration. Following Salemi and Zamani [42], we                                       │\n",
       "│ utilize 18 diverse RAG agents in our exp...                                                      │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Preprint, Preprint Alireza Salemi and Hamed Zamani                                               │\n",
       "│ Agents Configuration. Following Salemi and Zamani [42], we                                       │\n",
       "│ utilize 18 diverse RAG agents in our exp...                                                      │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2410.09942v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2410.09942v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.4171</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.4171\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ search engines serve human users, considering the paradigm shift                                 │\n",
       "│ where nowadays LLMs are the main users of the search engines.                                    │\n",
       "│ We propose IUM, an iter...                                                                       │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ search engines serve human users, considering the paradigm shift                                 │\n",
       "│ where nowadays LLMs are the main users of the search engines.                                    │\n",
       "│ We propose IUM, an iter...                                                                       │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2410.09942v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2410.09942v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.4093</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.4093\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Learning to Rank for Multiple Retrieval-Augmented Models                                         │\n",
       "│ through Iterative Utility Maximization                                                           │\n",
       "│ Alireza Salemi                                                                                   │\n",
       "│ University of Massachusetts Amherst                                                              │\n",
       "│ Uni...                                                                                           │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Learning to Rank for Multiple Retrieval-Augmented Models                                         │\n",
       "│ through Iterative Utility Maximization                                                           │\n",
       "│ Alireza Salemi                                                                                   │\n",
       "│ University of Massachusetts Amherst                                                              │\n",
       "│ Uni...                                                                                           │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2410.09942v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2410.09942v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.4012</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.4012\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ to address this limitation is to enhance LLMs by retrieving infor-                               │\n",
       "│ mation from external knowledge sources, a technique known as                                     │\n",
       "│ retrieval-augmented ge...                                                                        │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ to address this limitation is to enhance LLMs by retrieving infor-                               │\n",
       "│ mation from external knowledge sources, a technique known as                                     │\n",
       "│ retrieval-augmented ge...                                                                        │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2410.09942v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2410.09942v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Total unique documents:</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mTotal unique documents:\u001b[0m \u001b[1;36m9\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Generating combined answer with documents context...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mGenerating combined answer with documents context\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 23:07:18,092 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-04-18 23:07:23,886 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">╭───────────────────────────────────────────── Answer ─────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> Agentic RAG, short for Retrieval Augmented Generation, is a method for improving the performance <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> of Large Language Models (LLMs) in tasks such as question answering and text generation. The     <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> core idea behind Agentic RAG is to use an external retrieval system to gather relevant           <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> information from a vast amount of text data and then use this retrieved information to augment   <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> the input to the LLM.                                                                            <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> In traditional RAG, the LLM consumes a fixed number of documents as input, which are typically   <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> retrieved based on relevance scores. However, in Agentic RAG, the LLM is only given access to    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> the most relevant documents for each input, and these documents are used to augment the input in <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> different ways.                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> The \"agentic\" part of Agentic RAG refers to the fact that the model learns to represent the      <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> retrieved information as a sequence of embeddings, which can then be combined with the original  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> input to generate more coherent and accurate outputs. This approach allows for more efficient    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> use of external knowledge and enables the LLM to leverage the strengths of both the internal     <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> model and the external retrieval system.                                                         <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> Agentic RAG has been shown to improve the performance of LLMs on various tasks, including        <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> question answering, text generation, and conversational dialogue systems. The method has also    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> been applied to various domains, such as law, medicine, and finance, where large amounts of text <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> data are available but may not be easily accessible to the LLM.                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> Overall, Agentic RAG represents an important advancement in the field of natural language        <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> processing, enabling LLMs to tap into external knowledge and improve their performance on a wide <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> range of tasks.                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m╭─\u001b[0m\u001b[32m────────────────────────────────────────────\u001b[0m\u001b[32m Answer \u001b[0m\u001b[32m────────────────────────────────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
       "\u001b[32m│\u001b[0m Agentic RAG, short for Retrieval Augmented Generation, is a method for improving the performance \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m of Large Language Models (LLMs) in tasks such as question answering and text generation. The     \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m core idea behind Agentic RAG is to use an external retrieval system to gather relevant           \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m information from a vast amount of text data and then use this retrieved information to augment   \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m the input to the LLM.                                                                            \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m In traditional RAG, the LLM consumes a fixed number of documents as input, which are typically   \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m retrieved based on relevance scores. However, in Agentic RAG, the LLM is only given access to    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m the most relevant documents for each input, and these documents are used to augment the input in \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m different ways.                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m The \"agentic\" part of Agentic RAG refers to the fact that the model learns to represent the      \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m retrieved information as a sequence of embeddings, which can then be combined with the original  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m input to generate more coherent and accurate outputs. This approach allows for more efficient    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m use of external knowledge and enables the LLM to leverage the strengths of both the internal     \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m model and the external retrieval system.                                                         \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m Agentic RAG has been shown to improve the performance of LLMs on various tasks, including        \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m question answering, text generation, and conversational dialogue systems. The method has also    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m been applied to various domains, such as law, medicine, and finance, where large amounts of text \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m data are available but may not be easily accessible to the LLM.                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m Overall, Agentic RAG represents an important advancement in the field of natural language        \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m processing, enabling LLMs to tap into external knowledge and improve their performance on a wide \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m range of tasks.                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Options:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mOptions:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Ask a question\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m. Ask a question\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. View conversation history\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m. View conversation history\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Clear conversation history\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m. Clear conversation history\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. Exit\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4\u001b[0m. Exit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your choice (1-4):  1\n",
      "\n",
      "[bold]Enter your query:[/bold]  which domain will the above topic fall under? is it llm or optimization?\n",
      "Use conversation memory? (y/n):  y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Processing your query...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mProcessing your query\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should_use_memory True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Using conversation memory to process this query...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mUsing conversation memory to process this query\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 23:08:37,281 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-04-18 23:08:38,886 - SageRAG - INFO - Question can be answered from conversation history\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Question can be answered directly from conversation history...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mQuestion can be answered directly from conversation history\u001b[0m\u001b[1;32m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">╭───────────────────────────────────────────── Answer ─────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> The new question can be answered directly based ONLY on the conversation history, as the topic   <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> of Agentic RAG has already been discussed in the context of Large Language Models (LLMs).        <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> Answer: Optimization.                                                                            <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> This is because Agentic RAG is a method that aims to optimize the performance of LLMs by         <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> leveraging external knowledge and improving their coherence and accuracy.                        <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m╭─\u001b[0m\u001b[32m────────────────────────────────────────────\u001b[0m\u001b[32m Answer \u001b[0m\u001b[32m────────────────────────────────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
       "\u001b[32m│\u001b[0m The new question can be answered directly based ONLY on the conversation history, as the topic   \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m of Agentic RAG has already been discussed in the context of Large Language Models (LLMs).        \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m Answer: Optimization.                                                                            \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m This is because Agentic RAG is a method that aims to optimize the performance of LLMs by         \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m leveraging external knowledge and improving their coherence and accuracy.                        \u001b[32m│\u001b[0m\n",
       "\u001b[32m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Options:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mOptions:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Ask a question\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m. Ask a question\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. View conversation history\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m. View conversation history\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Clear conversation history\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m. Clear conversation history\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. Exit\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4\u001b[0m. Exit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your choice (1-4):  2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">╭───────────────────────────────────────────── Conversation History ──────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> Human: What is Agentic RAG(Retrieval Augmented Generation)?                                                     <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> Assistant: Agentic RAG, short for Retrieval Augmented Generation, is a method for improving the performance of  <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> Large Language Models (LLMs) in tasks such as question answering and text generation. The core idea behind      <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> Agentic RAG is to use an external retrieval system to gather relevant information from a vast amount of text    <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> data and then use this retrieved information to augment the input to the LLM.                                   <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> In traditional RAG, the LLM consumes a fixed number of documents as input, which are typically retrieved based  <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> on relevance scores. However, in Agentic RAG, the LLM is only given access to the most relevant documents for   <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> each input, and these documents are used to augment the input in different ways.                                <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> The \"agentic\" part of Agentic RAG refers to the fact that the model learns to represent the retrieved           <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> information as a sequence of embeddings, which can then be combined with the original input to generate more    <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> coherent and accurate outputs. This approach allows for more efficient use of external knowledge and enables    <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> the LLM to leverage the strengths of both the internal model and the external retrieval system.                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> Agentic RAG has been shown to improve the performance of LLMs on various tasks, including question answering,   <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> text generation, and conversational dialogue systems. The method has also been applied to various domains, such <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> as law, medicine, and finance, where large amounts of text data are available but may not be easily accessible  <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> to the LLM.                                                                                                     <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> Overall, Agentic RAG represents an important advancement in the field of natural language processing, enabling  <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> LLMs to tap into external knowledge and improve their performance on a wide range of tasks.                     <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> Human: which domain will the above topic fall under? is it llm or optimization?                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> Assistant: The new question can be answered directly based ONLY on the conversation history, as the topic of    <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> Agentic RAG has already been discussed in the context of Large Language Models (LLMs).                          <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> Answer: Optimization.                                                                                           <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> This is because Agentic RAG is a method that aims to optimize the performance of LLMs by leveraging external    <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> knowledge and improving their coherence and accuracy.                                                           <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m╭─\u001b[0m\u001b[34m────────────────────────────────────────────\u001b[0m\u001b[34m Conversation History \u001b[0m\u001b[34m─────────────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m\n",
       "\u001b[34m│\u001b[0m Human: What is Agentic RAG(Retrieval Augmented Generation)?                                                     \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m Assistant: Agentic RAG, short for Retrieval Augmented Generation, is a method for improving the performance of  \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m Large Language Models (LLMs) in tasks such as question answering and text generation. The core idea behind      \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m Agentic RAG is to use an external retrieval system to gather relevant information from a vast amount of text    \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m data and then use this retrieved information to augment the input to the LLM.                                   \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m In traditional RAG, the LLM consumes a fixed number of documents as input, which are typically retrieved based  \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m on relevance scores. However, in Agentic RAG, the LLM is only given access to the most relevant documents for   \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m each input, and these documents are used to augment the input in different ways.                                \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m The \"agentic\" part of Agentic RAG refers to the fact that the model learns to represent the retrieved           \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m information as a sequence of embeddings, which can then be combined with the original input to generate more    \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m coherent and accurate outputs. This approach allows for more efficient use of external knowledge and enables    \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m the LLM to leverage the strengths of both the internal model and the external retrieval system.                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m Agentic RAG has been shown to improve the performance of LLMs on various tasks, including question answering,   \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m text generation, and conversational dialogue systems. The method has also been applied to various domains, such \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m as law, medicine, and finance, where large amounts of text data are available but may not be easily accessible  \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m to the LLM.                                                                                                     \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m Overall, Agentic RAG represents an important advancement in the field of natural language processing, enabling  \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m LLMs to tap into external knowledge and improve their performance on a wide range of tasks.                     \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m Human: which domain will the above topic fall under? is it llm or optimization?                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m Assistant: The new question can be answered directly based ONLY on the conversation history, as the topic of    \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m Agentic RAG has already been discussed in the context of Large Language Models (LLMs).                          \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m Answer: Optimization.                                                                                           \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m This is because Agentic RAG is a method that aims to optimize the performance of LLMs by leveraging external    \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m knowledge and improving their coherence and accuracy.                                                           \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Options:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mOptions:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Ask a question\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m. Ask a question\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. View conversation history\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m. View conversation history\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Clear conversation history\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m. Clear conversation history\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. Exit\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4\u001b[0m. Exit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your choice (1-4):  1\n",
      "\n",
      "[bold]Enter your query:[/bold]  How exactly RAG work?\n",
      "Use conversation memory? (y/n):  y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Processing your query...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mProcessing your query\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should_use_memory False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Using full query improvement pipeline...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mUsing full query improvement pipeline\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Evaluating your query...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mEvaluating your query\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 23:09:19,731 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Query Rating: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">/</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mQuery Rating: \u001b[0m\u001b[1;32m4\u001b[0m\u001b[1;32m/\u001b[0m\u001b[1;32m5\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Explanation: The query is clear and unambiguous, but lacks specificity. The term RAG refers to <span style=\"color: #008000; text-decoration-color: #008000\">'Recombinant </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Activation Gene'</span> or <span style=\"color: #008000; text-decoration-color: #008000\">'Recombinant Array Generator'</span>, which are specific scientific concepts. However, the query does \n",
       "not provide enough context for a precise search. To improve retrievability with vector search, additional terms \n",
       "could be included to narrow down the results.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Explanation: The query is clear and unambiguous, but lacks specificity. The term RAG refers to \u001b[32m'Recombinant \u001b[0m\n",
       "\u001b[32mActivation Gene'\u001b[0m or \u001b[32m'Recombinant Array Generator'\u001b[0m, which are specific scientific concepts. However, the query does \n",
       "not provide enough context for a precise search. To improve retrievability with vector search, additional terms \n",
       "could be included to narrow down the results.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Generating improved query variations...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mGenerating improved query variations\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 23:09:21,888 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Available search queries:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mAvailable search queries:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">.</span> Original: How exactly RAG work?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m0\u001b[0m\u001b[1m.\u001b[0m Original: How exactly RAG work?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Confidence: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;32mConfidence: \u001b[0m\u001b[1;32m1.00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: **Using technical terminology for better vector matching**\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m1\u001b[0m: **Using technical terminology for better vector matching**\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.13</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.13\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: **Breaking down complex queries into clearer formulations**\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m2\u001b[0m: **Breaking down complex queries into clearer formulations**\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.05</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.05\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: **Varying syntax while preserving semantic meaning**\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m3\u001b[0m: **Varying syntax while preserving semantic meaning**\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.05</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.05\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>: **Adding relevant synonyms or related concepts**\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m4\u001b[0m: **Adding relevant synonyms or related concepts**\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.01</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.01\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>: **Including key entities and relationships from the original query**\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m5\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m5\u001b[0m: **Including key entities and relationships from the original query**\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">-0.01</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m-0.01\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Select queries to use:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mSelect queries to use:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Enter the numbers of the queries you want to use <span style=\"font-weight: bold\">(</span>comma-separated, e.g., <span style=\"color: #008000; text-decoration-color: #008000\">'0,2,3'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Enter the numbers of the queries you want to use \u001b[1m(\u001b[0mcomma-separated, e.g., \u001b[32m'0,2,3'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Selected </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> queries for retrieval.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mSelected \u001b[0m\u001b[1;32m1\u001b[0m\u001b[1;32m queries for retrieval.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Retrieving relevant documents...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mRetrieving relevant documents\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG123: only this is working\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5n/8m4t00412h97v592vh5y0pgc0000gn/T/ipykernel_1231/3684311431.py:940: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'creationdate': 'D:20131206091100', 'creator': 'Microsoft® Office Word 2007', 'moddate': 'D:20131206091100', 'page': 2, 'page_label': '3', 'producer': 'Microsoft® Office Word 2007', 'source': 'https://arxiv.org/pdf/1312.1810v1', 'subdomain': 'operating_systems', 'total_pages': 5}, page_content='Table 1: The buffer cache interface  \\nSystem Call Description \\n  \\nBread Read a block from a file. \\nBreadn Read a block from a file and read n \\n additional blocks asynchronously. \\nBaread Read a block asynchronously. \\nBwrite Write a block to the file. \\nBawrite Write a block asynchronous to a file. \\nBdwirte Place the buffer on the dirty queue but \\n don’t start any I/O. \\nBrelse Release a buffer back to the buffer \\n cache.  \\nThe buffer cache interface is presented in Figure 1. In \\ngeneral a file system reads data with bread or if the file is \\nbeing read sequentially with breadn. This means that if a \\nfile is read in logical block number order when block \\nnumber n is read it also starts to read blo ck number n + 1 \\nasynchronous so that when the user actually read block n + \\n1 it is already in memory.  \\nFile systems write data with the bdwrite function to \\nincrease performance but critical data is written with \\nbwrite to maintain file system consistency in  case of a \\ncrash. The bawrite is used by the buffer cleaner to start \\nwrite operations on dirty buffers.  \\nHARD DISKS  \\nHard disk drives are classified as non -volatile, random \\naccess, digital, magnetic, data storage devices. Introduced \\nby IBM in 1956, hard disk drives have decreased in cost \\nand physical size over the years while dramatically \\nincreasing in capacity and speed.  \\nA hard disk drive (HDD) is also named as a hard drive, or a \\nhard disk, or disk drive. It is a device for storing and \\nretrieving digita l information, primarily computer data. It \\nconsists of one or more rigid (hard) rapidly rotating discs \\n(platters) which are coated with magnetic material and \\nwith magnetic heads arranged to write data to the surfaces \\nand read it from them.  \\nIn this subsect ion we introduce the terminology related to \\nhard disks. In the Figure 5.1, important parts of the hard \\ndisk are shown. It consists of one or more circular \\naluminum platters, of which either or both surfaces are \\ncoated with a magnetic substance used for rec ording the \\ndata. For each surface, there is a read -write head that \\nexamines or alters the recorded data. The platters rotate on \\na common axis; typical rotation speed is 5400 or 7200 \\nrotations per minute, although high -performance hard \\ndisks have higher speeds and older disks may have lower'), 0.049919418840150165), (Document(metadata={'creationdate': 'D:20131206091100', 'creator': 'Microsoft® Office Word 2007', 'moddate': 'D:20131206091100', 'page': 2, 'page_label': '3', 'producer': 'Microsoft® Office Word 2007', 'source': 'https://arxiv.org/pdf/1312.1810v1', 'subdomain': 'operating_systems', 'total_pages': 5}, page_content='disk I/O to complete. It does this by caching recently used \\ndata in the systems memory (read caching) and by delaying \\nwrite operation (write caching).  \\nThe buffer cache is implemented as four queues  of buffers, \\nthe locked queue, the clean queue, the dirty queue and the \\nempty queue. A buffer contains information about the \\ncached data such as the logical block number (The logical \\nblock number is an offset into the file, i. e. zero for the first \\nblock a nd one for the second etc.) and the physical disk \\nblock number and a pointer to the cached data. Each buffer \\ncan be used to cache between 512 byte (a sector) and 64 \\nkilobyte worth of data but the file systems usually only \\nreads and writes in complete file system blocks (defaults to \\n8 kilo byte for Fast File System).  \\nThe locked queue is used for important data and the \\nbuffers on this queue can’t be recycled they are always in \\nmemory. The clean queue is for buffers that have invalid \\ndata, valid but unchanged data or data that has already \\n \\nbeen committed to the disk. The buffers in the clean queue \\nare stored in Least recently Used (LRU) order which means \\nthat heavily accessed buffers will be stored longer in the \\ncache and less used buffers will be recycled qu ickly. The \\nbuffers in the dirty queue are also stored in LRU -order and \\nit contains data that needs to be written to disk. Write \\noperations are delayed because a buffer that has been \\nwritten to are likely to be written to again and by \\nsubmitting all dirty b uffers to the device driver \\nsimultaneous it can sort the write operation to minimize \\nthe seek time. The drawback of write caching is that in case \\nof a system crash all unwritten data is lost. The risk of data \\nloss is minimized by a special synchronization process that \\nflushes all dirty blocks to disk every 30 seconds. Data in the \\ndirty queue can also be flushed by a buffer cleaner process \\nif the number of dirty pages exceeds 25 percent or the \\nnumber of clean pages is less than 5 percent. The empty \\nqueue is for buffers that don’t have any memory pages tied \\nto them.  \\nTable 1: The buffer cache interface  \\nSystem Call Description \\n  \\nBread Read a block from a file. \\nBreadn Read a block from a file and read n \\n additional blocks asynchronously. \\nBaread Read a block asynchronously.'), 0.009628464656584845), (Document(metadata={'author': 'Gabriel Radanne, Jérôme Vouillon, and Vincent Balat', 'creationdate': '2019-01-31T21:37:31-05:00', 'creator': 'LaTeX with acmart 2018/01/24 v1.49 Typesetting articles for the Association for Computing Machinery and hyperref 2016/06/24 v6.83q Hypertext links for LaTeX', 'keywords': 'Web, client/server, OCaml, ML, Eliom, functional, module', 'moddate': '2019-01-31T21:37:31-05:00', 'page': 37, 'page_label': '38', 'producer': 'dvips + GPL Ghostscript GIT PRERELEASE 9.22', 'source': 'https://arxiv.org/pdf/1901.11411v1', 'subdomain': 'programming_languages', 'subject': '-  Software and its engineering  ->  Functional languages; Modules / packages; Formal language definitions; Compilers; Distributed programming languages;', 'title': 'Eliom: A Language for Modular Tierless Web Programming', 'total_pages': 88}, page_content='that, while the client does the printing, the values and the execu tion order are completely deter-\\nmined by the server. The server code ( Example 12a) calls f with 2 as argument, injects the result\\nand then calls f with 3 as argument. The client code, in Example 12b , declares a fragment clo-\\nsure f, which simply adds one to its arguments, and exec both fragments. In-between both execu-\\ntions, it prints the content of the injection v. During the execution of the server, the list of calls\\n(Example 12c) and the mapping of injections ( Example 12d) are built. First, when fragment f (ints 2)\\nis executed, a fresh reference r1 is generated, the call to the fragment is added to the list and r1\\nis returned. The injection adds the association v1 ↦→r1 to the mapping of injections. The call to\\nend () then adds the token end to the list of fragments. The second fragment proceeds similarl y\\nto the ﬁrst, with a fresh identiﬁers r2. Once server execution is over, the newly generated list of\\nfragments and mapping of injections are sent to the client. During the client execution, the execu-\\ntion of the list is controlled by the exec calls. First, (f 2) emits ⟨2⟩ and is evaluated to 3, and the\\nmapping r1 ↦→3 is added to a global environment. Then v1 is resolved to r1 and printed (which\\nshows ⟨3⟩). Finally (f 3) emits ⟨3⟩ and is evaluated to 4.\\nThe important thing to note here is that both the injection mapping and the list of fragments\\nare completely dynamic. We could add complicated control ﬂow t o the server program that would\\ndrive the client execution according to some dynamic values. Th e only static elements are the\\nnames f and v1, the behavior of f and the number of call to exec (). We cannot, however, make\\nthe server-side control ﬂow depend on client-side values, since we obey a strict phase separation\\nbetween server and client execution.\\nFinally, remark that we do not need the bind env construct introduced in Section 3.6.1 . Instead,\\nwe directly capture the environment using closures that are extr acted in advance. We will see how'), 0.004687589605652742), (Document(metadata={'author': '', 'creationdate': '2019-08-29T00:44:39+00:00', 'creator': 'LaTeX with hyperref package', 'keywords': '', 'moddate': '2019-08-29T00:44:39+00:00', 'page': 5, 'page_label': '6', 'producer': 'pdfTeX-1.40.17', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'source': 'https://arxiv.org/pdf/1908.10740v1', 'subdomain': 'operating_systems', 'subject': '', 'title': '', 'total_pages': 14, 'trapped': '/False'}, page_content='for example, when the master updates the block mapping,\\nthe header of three mapping items are constructed with the\\nfollowing layout: 1|V1|0 0|V1|0 0|V1|1. Note that all the\\nthree items share the sameversion (i.e., V1), which is provided\\nby Ulib when it acquires the range lock (in Section 4.3). The\\nstart bit of the ﬁrst item and the end bit of the last item are\\nset to 1. We only reserve 40-bit for pointer ﬁeld since it\\nalways points to a 4 KB-aligned page (the lower 12 bits can\\nbe discarded). It’s easy to understand that when there are no\\nconcurrent writers, the block mapping items should satisfy\\nany of the conditions in Figure 5:\\n(a) Writes without overlapping. The items with the same\\nversion are enclosed with a start bit and an end bit,\\nindicating that multiple threads have updated the same\\nﬁle but different data pages.\\n(b) Overlapping in the tail. The reader sees a start bit\\nwhen the version increases, indicating that a thread has\\noverwritten the end part of the pages that are updated by\\na former thread.\\n(c) Overlapping in the head. The reader sees an end bit\\nbefore the version decreases, indicating that a thread has\\noverwritten the front part of the pages that are updated by\\na former thread.\\nIf Ulib meets any cases that violate the above conditions,\\nwe assert that the master is updating the block mappings for\\nother concurrent write threads. In this case, Ulib needs to\\nreload the metadata again and checks its validity. To reduce\\nthe overhead of retrying, the read thread copies the ﬁle data\\nto user’s buffer only after it has successfully collected a\\nconsistent version of the block mapping. This is achievable\\nbecause the obsolete NVM pages are lazily reclaimed. When\\nthe modiﬁed mapping items span to multiple cachelines, the\\nmaster also adds extra mfence to serialize the updates. By this\\nway, the read threads can see the updates in order.\\nRead Protection. Leveraging the permission bits to enforce\\nread protection is more challenging, since metadata have\\n6'), -0.03543216407926142), (Document(metadata={'author': '', 'creationdate': '2022-03-20T10:14:42-04:00', 'creator': 'LaTeX with hyperref package', 'keywords': '', 'moddate': '2022-03-20T10:14:42-04:00', 'page': 21, 'page_label': '20', 'producer': 'dvips + GPL Ghostscript GIT PRERELEASE 9.22', 'source': 'https://arxiv.org/pdf/1004.4474v1', 'subdomain': 'operating_systems', 'subject': '', 'title': '', 'total_pages': 23}, page_content='it passes through the out (or subout, if any) hook. Each item from the subin\\nhook (if any) passes untouched through the out hook. Any ﬁlter procedure can be\\nderegistered by the delflt control message (see below) at any time, which leads to\\nﬁlter deleting from the ﬁlter procedure chain and the correspondin g KLD module\\nunloading (if no longer referred to by anybody).\\nThe following speciﬁc control messages were added:\\naddflt <struct ng_filter_addflt> – adds the ﬁlter procedure with supplied\\nname <char *name> as the last procedure of the ﬁlter chain and ﬁlls the array\\nof its arguments from the supplied <union arg arg_arr[]>;\\ndelflt <char *name> – removes the ﬁlter procedure named <name> (if any) from\\nthe ﬁlter chain and unloads the corresponding KLD module if no longer used by\\nother instance(s) of ng ﬁlter(4) ;\\ngetflt – returns the array of <struct ng_filter_addflt> (the full ﬁlter chain\\nconﬁguration);\\nclrflt– clears the whole ﬁlter chain.\\nAs it has already been mentioned, the user context process partic ipation in ﬁlter-\\ning will be unavoidable in some cases (f.e., events conversion to ROOT r epresenta-\\ntion). To allow a more eﬃcient way for the packets to cross the cont ext boundaries\\ntwice – from the kernel to the user and back again – the ng\\nmm(4) node can be\\napplied. This node can be connected to ng ﬁlter(4) subout and subin hooks by\\nits in and out hooks, correspondingly. A user context process can map the bot h\\nbuﬀers (for raw and converted packets) into the own address sp ace by mmap(2)\\nmechanism, supported by ng mm(4)’s /dev/mmr<N> and /dev/mmc<N> devices.\\nAfter that the process can directly communicate 12 with these buﬀers as with regular\\npieces of memory. Of course, some synchronization is required and can be done by\\ncalling ioctl(2) to such devices before and after the buﬀer reading and writing.\\n3.4 User context utilities'), -0.03766476693858034)]\n",
      "  results = self.vector_db.similarity_search_with_relevance_scores(selected_queries[0], k=num_results)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Retrieved </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> documents</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mRetrieved \u001b[0m\u001b[1;32m5\u001b[0m\u001b[1;32m documents\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Top retrieved documents:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mTop retrieved documents:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.0499</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m. \u001b[1;31mScore: \u001b[0m\u001b[1;31m0.0499\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Preview: Table <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: The buffer cache interface  \n",
       "System Call Description \n",
       "  \n",
       "Bread Read a block from a file. \n",
       "Breadn Read a block from a file and read n \n",
       " additi<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Preview: Table \u001b[1;36m1\u001b[0m: The buffer cache interface  \n",
       "System Call Description \n",
       "  \n",
       "Bread Read a block from a file. \n",
       "Breadn Read a block from a file and read n \n",
       " additi\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Source: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/1312.1810v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Source: \u001b[4;94mhttps://arxiv.org/pdf/1312.1810v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.0096</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m. \u001b[1;31mScore: \u001b[0m\u001b[1;31m0.0096\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Preview: disk I/O to complete. It does this by caching recently used \n",
       "data in the systems memory <span style=\"font-weight: bold\">(</span>read caching<span style=\"font-weight: bold\">)</span> and by delaying \n",
       "write operation <span style=\"font-weight: bold\">(</span>write caching<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Preview: disk I/O to complete. It does this by caching recently used \n",
       "data in the systems memory \u001b[1m(\u001b[0mread caching\u001b[1m)\u001b[0m and by delaying \n",
       "write operation \u001b[1m(\u001b[0mwrite caching\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Source: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/1312.1810v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Source: \u001b[4;94mhttps://arxiv.org/pdf/1312.1810v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.0047</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m. \u001b[1;31mScore: \u001b[0m\u001b[1;31m0.0047\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Preview: that, while the client does the printing, the values and the execu tion order are completely deter-\n",
       "mined by the server. The server code <span style=\"font-weight: bold\">(</span> Example 12a<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Preview: that, while the client does the printing, the values and the execu tion order are completely deter-\n",
       "mined by the server. The server code \u001b[1m(\u001b[0m Example 12a\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Source: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/1901.11411v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Source: \u001b[4;94mhttps://arxiv.org/pdf/1901.11411v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">-0.0354</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4\u001b[0m. \u001b[1;31mScore: \u001b[0m\u001b[1;31m-0.0354\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Preview: for example, when the master updates the block mapping,\n",
       "the header of three mapping items are constructed with the\n",
       "following layout: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>|V1|<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>|V1|<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>|V<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Preview: for example, when the master updates the block mapping,\n",
       "the header of three mapping items are constructed with the\n",
       "following layout: \u001b[1;36m1\u001b[0m|V1|\u001b[1;36m0\u001b[0m \u001b[1;36m0\u001b[0m|V1|\u001b[1;36m0\u001b[0m \u001b[1;36m0\u001b[0m|V\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Source: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/1908.10740v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Source: \u001b[4;94mhttps://arxiv.org/pdf/1908.10740v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">-0.0377</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m5\u001b[0m. \u001b[1;31mScore: \u001b[0m\u001b[1;31m-0.0377\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Preview: it passes through the out <span style=\"font-weight: bold\">(</span>or subout, if any<span style=\"font-weight: bold\">)</span> hook. Each item from the subin\n",
       "hook <span style=\"font-weight: bold\">(</span>if any<span style=\"font-weight: bold\">)</span> passes untouched through the out hook. Any ﬁlter procedure <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Preview: it passes through the out \u001b[1m(\u001b[0mor subout, if any\u001b[1m)\u001b[0m hook. Each item from the subin\n",
       "hook \u001b[1m(\u001b[0mif any\u001b[1m)\u001b[0m passes untouched through the out hook. Any ﬁlter procedure \u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Source: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/1004.4474v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Source: \u001b[4;94mhttps://arxiv.org/pdf/1004.4474v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Generating answer with document context...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mGenerating answer with document context\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 23:09:47,493 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-04-18 23:09:48,641 - SageRAG - INFO - Document relevance: Sufficient=False, Confidence=0.12\n",
      "2025-04-18 23:09:48,642 - SageRAG - INFO - Explanation: The retrieved documents contain some information about caching and buffer management, but they do not provide a clear explanation of how Request-Any-Reply (RAG) works.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Retrieved documents insufficient. Using LLM fallback...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mRetrieved documents insufficient. Using LLM fallback\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 23:09:49,877 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">╭───────────────────────────────────────────── Answer ─────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> RAG (Retrieval-Augmented Generation) is a method used in natural language processing (NLP) to    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> improve the performance of Large Language Models (LLMs). Here's how it works:                    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span><span style=\"font-weight: bold\">Retrieval Phase</span>: The LLM first searches for relevant documents that match the input query or  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>context. This is typically done using a retrieval system, such as a search engine or a        <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>knowledge graph.                                                                              <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span><span style=\"font-weight: bold\">Ranking and Filtering</span>: The retrieved documents are then ranked based on their relevance to    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>the input query. Only the top-ranked documents are selected and used in the next phase.       <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 3 </span><span style=\"font-weight: bold\">Document Embeddings</span>: Each of the selected documents is represented as a sequence of           <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>embeddings, which are dense vectors that capture the document's semantic meaning.             <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 4 </span><span style=\"font-weight: bold\">Input Augmentation</span>: The LLM combines the original input with the retrieved document           <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>embeddings to generate an augmented input. This augmented input can include features such as: <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>Document summaries or key phrases                                                          <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>Entity recognition and extraction                                                          <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>Sentiment analysis or opinion mining                                                       <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 5 </span><span style=\"font-weight: bold\">Generation Phase</span>: The LLM generates a response based on the augmented input, using its        <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>internal language model capabilities.                                                         <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 6 </span><span style=\"font-weight: bold\">Post-processing</span>: The final response may undergo post-processing techniques such as            <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>spell-checking, grammar correction, or fluency evaluation to improve its quality.             <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> The overall goal of RAG is to leverage external knowledge and retrieve relevant information to   <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> augment the LLM's input, improving its performance on various NLP tasks.                         <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m╭─\u001b[0m\u001b[32m────────────────────────────────────────────\u001b[0m\u001b[32m Answer \u001b[0m\u001b[32m────────────────────────────────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
       "\u001b[32m│\u001b[0m RAG (Retrieval-Augmented Generation) is a method used in natural language processing (NLP) to    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m improve the performance of Large Language Models (LLMs). Here's how it works:                    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m 1 \u001b[0m\u001b[1mRetrieval Phase\u001b[0m: The LLM first searches for relevant documents that match the input query or  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m   \u001b[0mcontext. This is typically done using a retrieval system, such as a search engine or a        \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m   \u001b[0mknowledge graph.                                                                              \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m 2 \u001b[0m\u001b[1mRanking and Filtering\u001b[0m: The retrieved documents are then ranked based on their relevance to    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m   \u001b[0mthe input query. Only the top-ranked documents are selected and used in the next phase.       \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m 3 \u001b[0m\u001b[1mDocument Embeddings\u001b[0m: Each of the selected documents is represented as a sequence of           \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m   \u001b[0membeddings, which are dense vectors that capture the document's semantic meaning.             \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m 4 \u001b[0m\u001b[1mInput Augmentation\u001b[0m: The LLM combines the original input with the retrieved document           \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m   \u001b[0membeddings to generate an augmented input. This augmented input can include features such as: \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mDocument summaries or key phrases                                                          \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mEntity recognition and extraction                                                          \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mSentiment analysis or opinion mining                                                       \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m 5 \u001b[0m\u001b[1mGeneration Phase\u001b[0m: The LLM generates a response based on the augmented input, using its        \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m   \u001b[0minternal language model capabilities.                                                         \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m 6 \u001b[0m\u001b[1mPost-processing\u001b[0m: The final response may undergo post-processing techniques such as            \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m   \u001b[0mspell-checking, grammar correction, or fluency evaluation to improve its quality.             \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m The overall goal of RAG is to leverage external knowledge and retrieve relevant information to   \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m augment the LLM's input, improving its performance on various NLP tasks.                         \u001b[32m│\u001b[0m\n",
       "\u001b[32m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Options:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mOptions:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Ask a question\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m. Ask a question\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. View conversation history\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m. View conversation history\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Clear conversation history\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m. Clear conversation history\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. Exit\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4\u001b[0m. Exit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your choice (1-4):  4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Thank you for using SageRAG Research Assistant. Goodbye!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mThank you for using SageRAG Research Assistant. Goodbye!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import os\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, BaseOutputParser\n",
    "from langchain.schema import Document, HumanMessage, AIMessage\n",
    "from langchain.vectorstores import VectorStore\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms.base import BaseLLM\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from pydantic import BaseModel, Field\n",
    "import logging\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "# Suppress library-specific logging\n",
    "import logging\n",
    "logging.getLogger('sentence_transformers').setLevel(logging.WARNING)\n",
    "logging.getLogger('transformers').setLevel(logging.WARNING)\n",
    "logging.getLogger('chromadb').setLevel(logging.WARNING)\n",
    "\n",
    "# Then set up your own logging as before\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"SageRAG\")\n",
    "\n",
    "# Rich console for prettier output\n",
    "console = Console()\n",
    "\n",
    "# Enhanced output parsers with better error handling\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Parse output that contains a numbered list and return as a list of strings.\"\"\"\n",
    "    \n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        \"\"\"Parse text into a list of strings.\"\"\"\n",
    "        # Updated regex to be more robust with various numbering styles\n",
    "        lines = re.findall(r\"^\\s*\\d+\\.?\\s+(.*?)$\", text, re.MULTILINE)\n",
    "        return [line.strip() for line in lines if line.strip()]\n",
    "    \n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"line_list\"\n",
    "\n",
    "class QueryRating(BaseModel):\n",
    "    \"\"\"Schema for query rating output.\"\"\"\n",
    "    rating: int = Field(description=\"Rating from 1-5\")\n",
    "    explanation: str = Field(description=\"Explanation for the rating\")\n",
    "\n",
    "@dataclass\n",
    "class RetrievedDocument:\n",
    "    \"\"\"Dataclass for tracking retrieved document info.\"\"\"\n",
    "    document: Document\n",
    "    score: float\n",
    "    query: str = \"\"\n",
    "    rank: int = 0\n",
    "\n",
    "class ResearchAssistant:\n",
    "    \"\"\"AI Research Assistant using RAG for scientific paper queries.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        llm: BaseLLM, \n",
    "        vector_db: VectorStore, \n",
    "        embeddings_model: Optional[SentenceTransformer] = None,\n",
    "        relevance_threshold: float = 0.2,\n",
    "        max_docs_per_query: int = 3\n",
    "    ):\n",
    "        \"\"\"Initialize the Research Assistant with LLM and vector database.\n",
    "        \n",
    "        Args:\n",
    "            llm: The language model to use for generation\n",
    "            vector_db: Vector database for document retrieval\n",
    "            embeddings_model: Optional SentenceTransformer model for semantic similarity\n",
    "            relevance_threshold: Minimum relevance score for documents\n",
    "            max_docs_per_query: Maximum documents to use per query\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.vector_db = vector_db\n",
    "        self.relevance_threshold = relevance_threshold\n",
    "        self.max_docs_per_query = max_docs_per_query\n",
    "        \n",
    "        # Initialize sentence transformer model for semantic similarity if not provided\n",
    "        if embeddings_model is None:\n",
    "            logger.info(\"Initializing default embedding model\")\n",
    "            self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        else:\n",
    "            self.semantic_model = embeddings_model\n",
    "            \n",
    "        # Initialize memory\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        # Create a retriever for direct use\n",
    "        self.retriever = vector_db.as_retriever()\n",
    "        \n",
    "        # Define parsers\n",
    "        self.json_parser = JsonOutputParser(pydantic_object=QueryRating)\n",
    "        self.list_parser = LineListOutputParser()\n",
    "        \n",
    "        logger.info(\"Research Assistant initialized successfully\")\n",
    "    \n",
    "    def rate_query(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Rates the query and gives an explanation.\n",
    "        \n",
    "        Args:\n",
    "            query: The user's query to be evaluated\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing rating and explanation\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an intelligent assistant trained to evaluate search queries for a scientific research database.\n",
    "        Given the following query: \"{query}\"\n",
    "        \n",
    "        1. Rate the query on a scale of 1 (very poor) to 5 (excellent) based on:\n",
    "           - Clarity: Is the query clear and unambiguous?\n",
    "           - Specificity: Does it contain specific technical terms or concepts?\n",
    "           - Relevance: Is it focused on retrieving scientific content?\n",
    "           - Retrievability: Will it work well with vector search?\n",
    "        \n",
    "        2. Provide a short explanation for the rating (what makes it effective or ineffective).\n",
    "        \n",
    "        Respond in JSON format:\n",
    "        {{\n",
    "          \"rating\": <number between 1-5>,\n",
    "          \"explanation\": \"<your explanation>\"\n",
    "        }}\n",
    "        \"\"\")\n",
    "        \n",
    "        chain: Runnable = prompt | self.llm | self.json_parser\n",
    "        \n",
    "        try:\n",
    "            return chain.invoke({\"query\": query})\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error rating query: {e}\")\n",
    "            # Fallback response if parsing fails\n",
    "            return {\n",
    "                \"rating\": 3, \n",
    "                \"explanation\": \"Unable to rate query properly. Consider adding more specific terms.\"\n",
    "            }\n",
    "    \n",
    "    def suggest_rewrites(self, query: str, chat_history: Optional[List] = None) -> List[str]:\n",
    "        \"\"\"Returns rephrased versions of the query optimized for retrieval.\n",
    "        \n",
    "        Args:\n",
    "            query: Original query to rewrite\n",
    "            chat_history: Optional conversation history for context\n",
    "            \n",
    "        Returns:\n",
    "            List of rewritten queries\n",
    "        \"\"\"\n",
    "        history_context = \"\"\n",
    "        if chat_history:\n",
    "            history_context = \"Consider this conversation context when rewriting the query:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use only last 3 messages for brevity\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    history_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"history_context\"],\n",
    "            template=\"\"\"You are an AI assistant specializing in scientific research queries.\n",
    "            \n",
    "            {history_context}\n",
    "            \n",
    "            Rephrase the question in 5 different ways to improve retrieval from a scientific paper database. \n",
    "            Focus on:\n",
    "            1. Using technical terminology for better vector matching\n",
    "            2. Breaking down complex queries into clearer formulations\n",
    "            3. Adding relevant synonyms or related concepts\n",
    "            4. Varying syntax while preserving semantic meaning\n",
    "            5. Including key entities and relationships from the original query\n",
    "            \n",
    "            Number each version starting with 1.\n",
    "            \n",
    "            Question: {question}\"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            result = (prompt | self.llm | self.list_parser).invoke({\n",
    "                \"question\": query,\n",
    "                \"history_context\": history_context\n",
    "            })\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error suggesting rewrites: {e}\")\n",
    "            # Return a minimal set of rewrites if parsing fails\n",
    "            return [\n",
    "                query,  # Original query\n",
    "                f\"research about {query}\",\n",
    "                f\"papers discussing {query}\"\n",
    "            ]\n",
    "    \n",
    "    def compute_confidence(self, original: str, rewritten: str) -> float:\n",
    "        \"\"\"Computes semantic similarity between original and rewritten queries.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            rewritten: Rewritten version\n",
    "            \n",
    "        Returns:\n",
    "            Similarity score between 0-1\n",
    "        \"\"\"\n",
    "        try:\n",
    "            vec_orig = self.semantic_model.encode(original, convert_to_tensor=True)\n",
    "            vec_rewrite = self.semantic_model.encode(rewritten, convert_to_tensor=True)\n",
    "            return float(util.pytorch_cos_sim(vec_orig, vec_rewrite).item())\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error computing similarity: {e}\")\n",
    "            return 0.7  # Default reasonable value\n",
    "    \n",
    "    def score_queries(self, original: str, queries: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Scores and sorts rephrased queries by confidence.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            queries: List of rewritten queries\n",
    "            \n",
    "        Returns:\n",
    "            List of (query, score) tuples sorted by score\n",
    "        \"\"\"\n",
    "        scored = [(q, self.compute_confidence(original, q)) for q in queries]\n",
    "        return sorted(scored, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    def present_query_options(self, original: str, queries: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Presents the original and rewritten queries with confidence scores.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            queries: List of rewritten queries\n",
    "            \n",
    "        Returns:\n",
    "            List of (query, score) tuples including original query\n",
    "        \"\"\"\n",
    "        # Add original query at the top\n",
    "        scored_queries = [(\"Original: \" + original, 1.0)]\n",
    "        \n",
    "        # Add scored rewritten queries\n",
    "        rewritten_scores = self.score_queries(original, queries)\n",
    "        for i, (query, score) in enumerate(rewritten_scores, 1):\n",
    "            scored_queries.append((f\"Rewrite {i}: {query}\", score))\n",
    "            \n",
    "        # Display options in a nice format\n",
    "        console.print(\"\\n[bold cyan]Available search queries:[/bold cyan]\")\n",
    "        for i, (query, score) in enumerate(scored_queries):\n",
    "            confidence_color = \"green\" if score > 0.8 else \"yellow\" if score > 0.6 else \"red\"\n",
    "            console.print(f\"[bold]{i}.[/bold] {query}\")\n",
    "            console.print(f\"   [bold {confidence_color}]Confidence: {score:.2f}[/bold {confidence_color}]\")\n",
    "            \n",
    "        return scored_queries\n",
    "    \n",
    "    def retrieve_documents(\n",
    "        self, \n",
    "        queries: List[str], \n",
    "        k: int = 5\n",
    "    ) -> Tuple[List[RetrievedDocument], Dict[str, List[RetrievedDocument]]]:\n",
    "        \"\"\"Retrieves documents using multiple queries, preserving query information.\n",
    "        \n",
    "        Args:\n",
    "            queries: List of queries to retrieve documents for\n",
    "            k: Number of documents to retrieve per query\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (all unique documents, query->documents mapping)\n",
    "        \"\"\"\n",
    "        all_docs: List[RetrievedDocument] = []\n",
    "        unique_content = set()\n",
    "        query_docs_map: Dict[str, List[RetrievedDocument]] = {}\n",
    "        \n",
    "        # Retrieve docs for each query\n",
    "        for query in queries:\n",
    "            console.print(f\"\\n[bold blue]Query:[/bold blue] '{query}'\")\n",
    "            \n",
    "            try:\n",
    "                results = self.vector_db.similarity_search_with_relevance_scores(query, k=k)\n",
    "                print(\"LOG123: this is results length\",len(results))\n",
    "                docs_for_query: List[RetrievedDocument] = []\n",
    "                \n",
    "                for i, (doc, score) in enumerate(results, 1):\n",
    "                    # Create retrieved document object\n",
    "                    retrieved_doc = RetrievedDocument(\n",
    "                        document=doc,\n",
    "                        score=score,\n",
    "                        query=query,\n",
    "                        rank=i\n",
    "                    )\n",
    "                    \n",
    "                    # Display result info\n",
    "                    score_color = \"green\" if score > 0.8 else \"yellow\" if score > 0.6 else \"red\"\n",
    "                    console.print(f\"[bold]--- Result {i} ---[/bold]\")\n",
    "                    console.print(f\"[bold {score_color}]Score: {score:.4f}[/bold {score_color}]\")\n",
    "                    \n",
    "                    # Show document preview\n",
    "                    preview = doc.page_content[:150] + \"...\" if len(doc.page_content) > 150 else doc.page_content\n",
    "                    console.print(Panel(preview, title=\"Content Preview\", width=100))\n",
    "                    \n",
    "                    # Show metadata if available\n",
    "                    if hasattr(doc, 'metadata') and doc.metadata:\n",
    "                        source = doc.metadata.get('source', 'Unknown')\n",
    "                        console.print(f\"[dim]Source: {source}[/dim]\")\n",
    "                    \n",
    "                    # Add to results for this query\n",
    "                    docs_for_query.append(retrieved_doc)\n",
    "                    \n",
    "                    # Only add unique documents to overall collection\n",
    "                    if doc.page_content not in unique_content:\n",
    "                        unique_content.add(doc.page_content)\n",
    "                        all_docs.append(retrieved_doc)\n",
    "                \n",
    "                query_docs_map[query] = docs_for_query\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error retrieving documents for query '{query}': {e}\")\n",
    "                console.print(f\"[bold red]Error retrieving documents for query: {query}[/bold red]\")\n",
    "        \n",
    "        console.print(f\"\\n[bold green]Total unique documents:[/bold green] {len(all_docs)}\")\n",
    "        return all_docs, query_docs_map\n",
    "\n",
    "    def rerank_docs(self, query: str, docs: List[RetrievedDocument]) -> List[RetrievedDocument]:\n",
    "        \"\"\"Reranks documents based on semantic similarity to the query.\n",
    "        \n",
    "        Args:\n",
    "            query: Query to compare documents against\n",
    "            docs: List of retrieved documents\n",
    "            \n",
    "        Returns:\n",
    "            Reranked list of documents with updated scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            query_embedding = self.semantic_model.encode(query, convert_to_tensor=True)\n",
    "            reranked = []\n",
    "            \n",
    "            for doc in docs:\n",
    "                doc_embedding = self.semantic_model.encode(doc.document.page_content, convert_to_tensor=True)\n",
    "                new_score = float(util.pytorch_cos_sim(query_embedding, doc_embedding).item())\n",
    "                \n",
    "                # Create new RetrievedDocument with updated score\n",
    "                reranked_doc = RetrievedDocument(\n",
    "                    document=doc.document,\n",
    "                    score=new_score,\n",
    "                    query=doc.query,\n",
    "                    rank=0  # Will be updated after sorting\n",
    "                )\n",
    "                reranked.append(reranked_doc)\n",
    "            \n",
    "            # Sort by score and update ranks\n",
    "            reranked.sort(key=lambda x: x.score, reverse=True)\n",
    "            for i, doc in enumerate(reranked, 1):\n",
    "                doc.rank = i\n",
    "                \n",
    "            return reranked\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reranking documents: {e}\")\n",
    "            return docs  # Return original documents if reranking fails\n",
    "    \n",
    "    def can_answer_without_retrieval(self, question: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"Determines if a question can be answered directly from memory without retrieval.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (can_answer, answer_if_available)\n",
    "        \"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        if not chat_history:\n",
    "            return False, None\n",
    "            \n",
    "        # Create the prompt to check if we can answer directly from the conversation\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an AI assistant helping with a scientific research conversation.\n",
    "        Given the following conversation history and a new question, determine if the question:\n",
    "        1. Can be answered directly based ONLY on the conversation history (like \"what was my previous question?\")\n",
    "        2. Does NOT require retrieving new information from scientific papers\n",
    "        \n",
    "        If both conditions are true, provide the answer. Otherwise, respond with \"NEEDS_RETRIEVAL\".\n",
    "        \n",
    "        Conversation History:\n",
    "        {chat_history}\n",
    "        \n",
    "        New Question: {question}\n",
    "        \n",
    "        Your assessment (answer directly or respond with \"NEEDS_RETRIEVAL\"):\n",
    "        \"\"\")\n",
    "        \n",
    "        # Format the chat history for context\n",
    "        history_str = \"\"\n",
    "        for message in chat_history[-5:]:  # Use only last 5 messages for brevity\n",
    "            if hasattr(message, \"content\"):\n",
    "                role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                history_str += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        try:\n",
    "            # Ask the LLM if this can be answered without retrieval\n",
    "            response = self.llm.invoke(\n",
    "                prompt.format(chat_history=history_str, question=question)\n",
    "            )\n",
    "            \n",
    "            # If the response is \"NEEDS_RETRIEVAL\", we need to use retrieval\n",
    "            if \"NEEDS_RETRIEVAL\" in response:\n",
    "                return False, None\n",
    "            else:\n",
    "                logger.info(\"Question can be answered from conversation history\")\n",
    "                return True, response\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error determining if retrieval needed: {e}\")\n",
    "            return False, None  \n",
    "    \n",
    "    def generate_final_answer(\n",
    "        self, \n",
    "        question: str, \n",
    "        docs: List[RetrievedDocument], \n",
    "        max_docs: int = 5\n",
    "    ) -> str:\n",
    "        \"\"\"Generates the final answer from retrieved documents with LLM fallback.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            docs: List of retrieved documents\n",
    "            max_docs: Maximum number of documents to include\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # First, evaluate if the documents are sufficient\n",
    "        is_sufficient, confidence = self.evaluate_document_relevance(question, docs)\n",
    "        \n",
    "        # If documents are insufficient, use LLM fallback\n",
    "        if not is_sufficient or confidence < 0.4:  # Adjust threshold as needed\n",
    "            console.print(\"[yellow]Retrieved documents insufficient. Using LLM fallback...[/yellow]\")\n",
    "            return self.generate_llm_answer(question)\n",
    "        \n",
    "        # Otherwise, continue with RAG as before\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "                \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful scientific research assistant analyzing scientific papers. \n",
    "            Use the following retrieved context chunks to answer the user's question thoroughly.\n",
    "            \n",
    "            Guidelines:\n",
    "            - Focus on providing accurate information from the papers\n",
    "            - Synthesize information across documents when appropriate\n",
    "            - Cite the sources of information in your answer (e.g., \"According to Document 1...\")\n",
    "            - If the information isn't in the context, indicate what's missing rather than making up information\n",
    "            - If the context contains conflicting information, highlight the disagreement and possible reasons\n",
    "            - Maintain scientific accuracy above all else\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Format document content with metadata\n",
    "        context_pieces = []\n",
    "        for i, doc in enumerate(docs[:max_docs]):\n",
    "            chunk = f\"Document {i+1} [Relevance: {doc.score:.2f}]:\\n{doc.document.page_content}\"\n",
    "            \n",
    "            # Add metadata if available\n",
    "            if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                source = doc.document.metadata.get('source', 'Unknown')\n",
    "                chunk += f\"\\nSource: {source}\"\n",
    "                \n",
    "            context_pieces.append(chunk)\n",
    "            \n",
    "        context = \"\\n\\n---\\n\\n\".join(context_pieces)\n",
    "        \n",
    "        try:\n",
    "            # Generate answer\n",
    "            answer = (prompt | self.llm).invoke({\n",
    "                \"question\": question, \n",
    "                \"context\": context,\n",
    "                \"chat_history\": chat_context\n",
    "            })\n",
    "            \n",
    "            # Save the QA pair to memory\n",
    "            self.memory.chat_memory.add_messages([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=answer)\n",
    "            ])\n",
    "            \n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating answer: {e}\")\n",
    "            return \"I'm sorry, I encountered an error while generating your answer. Please try rephrasing your question.\"\n",
    "    \n",
    "    def generate_combined_answer(\n",
    "        self, \n",
    "        question: str, \n",
    "        query_results: Dict[str, List[RetrievedDocument]]\n",
    "    ) -> str:\n",
    "        \"\"\"Generates a combined answer from multiple query results with LLM fallback.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            query_results: Dictionary mapping queries to retrieved documents\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Combine all relevant documents from different queries\n",
    "        all_docs = []\n",
    "        for query, docs in query_results.items():\n",
    "            all_docs.extend(docs[:self.max_docs_per_query])\n",
    "        \n",
    "        # Evaluate if the documents are sufficient\n",
    "        is_sufficient, confidence = self.evaluate_document_relevance(question, all_docs)\n",
    "        \n",
    "        # If documents are insufficient, use LLM fallback\n",
    "        if not is_sufficient or confidence < 0.4:  # Adjust threshold as needed\n",
    "            console.print(\"[yellow]Retrieved documents insufficient. Using LLM fallback...[/yellow]\")\n",
    "            return self.generate_llm_answer(question)\n",
    "        \n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        # Combine all context sections\n",
    "        all_context_sections = []\n",
    "        \n",
    "        for query, docs in query_results.items():\n",
    "            # Use only the top N documents for each query\n",
    "            docs_for_query = docs[:self.max_docs_per_query]\n",
    "            if docs_for_query:\n",
    "                all_context_sections.append(f\"Results for query: '{query}'\")\n",
    "                for i, doc in enumerate(docs_for_query, 1):\n",
    "                    section = f\"Document {i} [Relevance: {doc.score:.2f}]:\\n{doc.document.page_content}\"\n",
    "                    \n",
    "                    # Add metadata if available\n",
    "                    if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                        source = doc.document.metadata.get('source', 'Unknown')\n",
    "                        section += f\"\\nSource: {source}\"\n",
    "                        \n",
    "                    all_context_sections.append(section)\n",
    "        \n",
    "        # Join all context sections\n",
    "        combined_context = \"\\n\\n---\\n\\n\".join(all_context_sections)\n",
    "        \n",
    "        # Create a prompt that emphasizes synthesizing information across different query results\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful scientific research assistant analyzing scientific papers. \n",
    "            The user's question has been reformulated in several ways, and each formulation returned different documents.\n",
    "            \n",
    "            Guidelines:\n",
    "            - Synthesize information across ALL retrieved documents to provide a comprehensive answer\n",
    "            - Compare and contrast findings from different sources\n",
    "            - Highlight the most relevant information from each source\n",
    "            - Cite the specific documents you're referencing (e.g., \"According to the paper in Document 3...\")\n",
    "            - If information conflicts across sources, explain the different perspectives\n",
    "            - If the information isn't in the context, indicate what's missing\n",
    "            - Maintain scientific accuracy above all else\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Context from multiple query formulations:\n",
    "            {context}\n",
    "            \n",
    "            Original Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Generate answer\n",
    "            answer = (prompt | self.llm).invoke({\n",
    "                \"question\": question, \n",
    "                \"context\": combined_context,\n",
    "                \"chat_history\": chat_context\n",
    "            })\n",
    "            \n",
    "            # Save the QA pair to memory\n",
    "            self.memory.chat_memory.add_messages([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=answer)\n",
    "            ])\n",
    "            \n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating combined answer: {e}\")\n",
    "            return \"I'm sorry, I encountered an error while generating your answer. Please try rephrasing your question.\"\n",
    "            \n",
    "    def evaluate_document_relevance(self, question: str, docs: List[RetrievedDocument]) -> Tuple[bool, float]:\n",
    "        \"\"\"Evaluates whether documents are sufficient to answer the question.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            docs: List of retrieved documents\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (is_sufficient, confidence_score)\n",
    "        \"\"\"\n",
    "        if not docs:\n",
    "            return False, 0.0\n",
    "            \n",
    "        # If all docs have very low scores, they're likely not relevant\n",
    "        avg_score = sum(doc.score for doc in docs) / len(docs)\n",
    "        \n",
    "        # Create a prompt to evaluate document relevance\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are evaluating whether a set of retrieved documents contains information \n",
    "        to answer a user's question.\n",
    "        \n",
    "        User Question: {question}\n",
    "        \n",
    "        Retrieved Information:\n",
    "        {doc_summaries}\n",
    "        \n",
    "        Please analyze whether the retrieved documents contain enough information to answer the question.\n",
    "        Respond with a JSON object:\n",
    "        {{\n",
    "          \"is_sufficient\": true/false,\n",
    "          \"confidence\": <number between 0-1>,\n",
    "          \"explanation\": \"<brief explanation>\"\n",
    "        }}\n",
    "        \n",
    "        Only respond with the JSON object.\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create short summaries of each document\n",
    "        doc_summaries = []\n",
    "        for i, doc in enumerate(docs[:3]):  # Use top 3 docs for evaluation\n",
    "            summary = doc.document.page_content[:200] + \"...\" if len(doc.document.page_content) > 200 else doc.document.page_content\n",
    "            doc_summaries.append(f\"Document {i+1} [Score: {doc.score:.2f}]: {summary}\")\n",
    "        \n",
    "        doc_summaries_text = \"\\n\\n\".join(doc_summaries)\n",
    "        \n",
    "        try:\n",
    "            # Parse the LLM response as JSON\n",
    "            response = self.llm.invoke(\n",
    "                prompt.format(question=question, doc_summaries=doc_summaries_text)\n",
    "            )\n",
    "            \n",
    "            # Extract JSON from the response\n",
    "            json_match = re.search(r'\\{.*\\}', response.replace('\\n', ' '), re.DOTALL)\n",
    "            if json_match:\n",
    "                result = json.loads(json_match.group(0))\n",
    "                is_sufficient = result.get(\"is_sufficient\", False)\n",
    "                confidence = result.get(\"confidence\", 0.0)\n",
    "                \n",
    "                logger.info(f\"Document relevance: Sufficient={is_sufficient}, Confidence={confidence}\")\n",
    "                logger.info(f\"Explanation: {result.get('explanation', 'No explanation provided')}\")\n",
    "                \n",
    "                return is_sufficient, confidence\n",
    "            else:\n",
    "                # If JSON parsing fails, use heuristics based on average score\n",
    "                return avg_score > self.relevance_threshold, avg_score\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating document relevance: {e}\")\n",
    "            # Fall back to using avg score\n",
    "            return avg_score > self.relevance_threshold, avg_score\n",
    "\n",
    "    def generate_llm_answer(self, question: str) -> str:\n",
    "        \"\"\"Generates an answer directly from the LLM when documents aren't sufficient.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful scientific research assistant. \n",
    "            \n",
    "            The user has asked a question, but we couldn't find relevant documents in our scientific database.\n",
    "            Since this appears to be a question you can answer from your general knowledge, please provide\n",
    "            a helpful response.\n",
    "            \n",
    "            Guidelines:\n",
    "            - Be clear that you're answering from general knowledge rather than specific papers\n",
    "            - Provide accurate, factual information\n",
    "            - If the question is about a very specialized scientific topic that requires references to papers,\n",
    "              indicate that you lack specific citations but can provide general information\n",
    "            - If it's a basic concept, provide a thorough explanation\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Generate answer\n",
    "            answer = (prompt | self.llm).invoke({\n",
    "                \"question\": question,\n",
    "                \"chat_history\": chat_context\n",
    "            })\n",
    "            \n",
    "            # Save the QA pair to memory\n",
    "            self.memory.chat_memory.add_messages([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=answer)\n",
    "            ])\n",
    "            \n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating LLM answer: {e}\")\n",
    "            return \"I'm sorry, I couldn't find relevant information in my scientific database and encountered an error trying to provide a general answer. Please try rephrasing your question.\"\n",
    "    \n",
    "    def ask_with_memory(self, question: str, num_results: int = 5) -> str:\n",
    "        \"\"\"Uses conversation memory to process follow-up questions.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            num_results: Number of results to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # First, check if this is a question we can answer directly from memory\n",
    "            can_direct_answer, direct_answer = self.can_answer_without_retrieval(question)\n",
    "            if can_direct_answer:\n",
    "                console.print(\"[bold green]Question can be answered directly from conversation history...[/bold green]\")\n",
    "                # Save the QA pair to memory manually\n",
    "                self.memory.chat_memory.add_messages([\n",
    "                    HumanMessage(content=question),\n",
    "                    AIMessage(content=direct_answer)\n",
    "                ])\n",
    "                return direct_answer\n",
    "# Otherwise, use the full query improvement pipeline\n",
    "            \n",
    "            # Step 1: Rate the query\n",
    "            console.print(\"\\n[bold cyan]Evaluating your query (with memory context)...[/bold cyan]\")\n",
    "            rating_result = self.rate_query(question)\n",
    "            \n",
    "            rating_color = \"green\" if rating_result['rating'] >= 4 else \"yellow\" if rating_result['rating'] >= 3 else \"red\"\n",
    "            console.print(f\"[bold {rating_color}]Query Rating: {rating_result['rating']}/5[/bold {rating_color}]\")\n",
    "            console.print(f\"Explanation: {rating_result['explanation']}\")\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 2: Suggest rewrites if the rating is less than perfect\n",
    "            rewritten_queries = []\n",
    "            if rating_result[\"rating\"] < 5:\n",
    "                console.print(\"[bold cyan]Generating improved query variations...[/bold cyan]\")\n",
    "                # Get chat history for context in rewrites\n",
    "                chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "                rewritten_queries = self.suggest_rewrites(question, chat_history)\n",
    "                console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 3: Present options to the user\n",
    "            query_options = self.present_query_options(question, rewritten_queries)\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 4: Get user selection\n",
    "            console.print(\"[bold cyan]Select queries to use:[/bold cyan]\")\n",
    "            console.print(\"Enter the numbers of the queries you want to use (comma-separated, e.g., '0,2,3')\")\n",
    "            selected_indices_input = input(\"> \")\n",
    "            \n",
    "            try:\n",
    "                selected_indices = [int(idx.strip()) for idx in selected_indices_input.split(\",\")]\n",
    "            except ValueError:\n",
    "                console.print(\"[bold red]Invalid input. Using original query only.[/bold red]\")\n",
    "                selected_indices = [0]  # Default to original query\n",
    "            \n",
    "            # Get the selected queries\n",
    "            selected_queries = []\n",
    "            for idx in selected_indices:\n",
    "                if idx == 0:  # Original query\n",
    "                    selected_queries.append(question)\n",
    "                elif 0 < idx <= len(rewritten_queries):  # Rewritten query\n",
    "                    query_text = rewritten_queries[idx-1]\n",
    "                    selected_queries.append(query_text)\n",
    "            \n",
    "            if not selected_queries:\n",
    "                console.print(\"[bold red]No valid queries selected. Using original query.[/bold red]\")\n",
    "                selected_queries = [question]\n",
    "                \n",
    "            console.print(f\"\\n[bold green]Selected {len(selected_queries)} queries for retrieval.[/bold green]\")\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 5: Retrieve documents\n",
    "            console.print(\"[bold cyan]Retrieving relevant documents...[/bold cyan]\")\n",
    "            \n",
    "            if len(selected_queries) > 1:\n",
    "                # If multiple queries were selected, use the multi-query approach\n",
    "                docs, query_docs_map = self.retrieve_documents(selected_queries, k=num_results)\n",
    "                \n",
    "                # Generate a combined answer\n",
    "                console.print(\"\\n[bold cyan]Generating combined answer with conversation+docs context...[/bold cyan]\")\n",
    "                answer = self.generate_combined_answer(question, query_docs_map)\n",
    "            else:\n",
    "                # If only one query was selected, use the standard approach\n",
    "                try:\n",
    "                    retrieved_docs = []\n",
    "                    results = self.vector_db.similarity_search_with_relevance_scores(selected_queries[0], k=num_results)\n",
    "                    \n",
    "                    for i, (doc, score) in enumerate(results, 1):\n",
    "                        retrieved_docs.append(RetrievedDocument(\n",
    "                            document=doc,\n",
    "                            score=score,\n",
    "                            query=selected_queries[0],\n",
    "                            rank=i\n",
    "                        ))\n",
    "                    \n",
    "                    console.print(f\"[bold green]Retrieved {len(retrieved_docs)} documents[/bold green]\")\n",
    "                    \n",
    "                    # Show previews of the retrieved documents\n",
    "                    console.print(\"\\n[bold cyan]Top retrieved documents:[/bold cyan]\")\n",
    "                    for i, doc in enumerate(retrieved_docs[:5], 1):\n",
    "                        score_color = \"green\" if doc.score > 0.8 else \"yellow\" if doc.score > 0.6 else \"red\"\n",
    "                        console.print(f\"{i}. [bold {score_color}]Score: {doc.score:.4f}[/bold {score_color}]\")\n",
    "                        \n",
    "                        preview = doc.document.page_content[:150] + \"...\" if len(doc.document.page_content) > 150 else doc.document.page_content\n",
    "                        console.print(f\"   Preview: {preview}\")\n",
    "                        \n",
    "                        if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                            console.print(f\"   Source: {doc.document.metadata.get('source', 'Unknown')}\")\n",
    "                    \n",
    "                    # Generate answer with memory context\n",
    "                    console.print(\"\\n[bold cyan]Generating answer with conversation + doc context...[/bold cyan]\")\n",
    "                    answer = self.generate_final_answer(question, retrieved_docs)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in retrieval: {e}\")\n",
    "                    return f\"I'm sorry, I encountered an error while retrieving documents: {str(e)}\"\n",
    "            \n",
    "            return answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in ask_with_memory: {e}\")\n",
    "            return f\"I'm sorry, I encountered an error while processing your question: {str(e)}\"\n",
    "\n",
    "    def search_with_query_feedback(self, query: str, num_results: int = 5) -> str:\n",
    "        \"\"\"Main pipeline that processes a query and returns an answer.\"\"\"\n",
    "        # Step 1: Rate the query\n",
    "        console.print(\"\\n[bold cyan]Evaluating your query...[/bold cyan]\")\n",
    "        rating_result = self.rate_query(query)\n",
    "            \n",
    "        rating_color = \"green\" if rating_result['rating'] >= 4 else \"yellow\" if rating_result['rating'] >= 3 else \"red\"\n",
    "        console.print(f\"[bold {rating_color}]Query Rating: {rating_result['rating']}/5[/bold {rating_color}]\")\n",
    "        console.print(f\"Explanation: {rating_result['explanation']}\")\n",
    "        console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "        # Step 2: Suggest rewrites if the rating is less than perfect\n",
    "        rewritten_queries = []\n",
    "        if rating_result[\"rating\"] < 5:\n",
    "            console.print(\"[bold cyan]Generating improved query variations...[/bold cyan]\")\n",
    "            rewritten_queries = self.suggest_rewrites(query)\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "                \n",
    "        # Step 3: Present options to the user\n",
    "            query_options = self.present_query_options(query, rewritten_queries)\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 4: Get user selection\n",
    "            console.print(\"[bold cyan]Select queries to use:[/bold cyan]\")\n",
    "            console.print(\"Enter the numbers of the queries you want to use (comma-separated, e.g., '0,2,3')\")\n",
    "            selected_indices_input = input(\"> \")\n",
    "            \n",
    "        try:\n",
    "            selected_indices = [int(idx.strip()) for idx in selected_indices_input.split(\",\")]\n",
    "        except ValueError:\n",
    "            console.print(\"[bold red]Invalid input. Using original query only.[/bold red]\")\n",
    "            selected_indices = [0]  # Default to original query\n",
    "        \n",
    "        # Get the selected queries\n",
    "        selected_queries = []\n",
    "        for idx in selected_indices:\n",
    "            if idx == 0:  # Original query\n",
    "                selected_queries.append(query)\n",
    "            elif 0 < idx <= len(rewritten_queries):  # Rewritten query\n",
    "                query_text = rewritten_queries[idx-1]\n",
    "                selected_queries.append(query_text)\n",
    "                \n",
    "        if not selected_queries:\n",
    "            console.print(\"[bold red]No valid queries selected. Using original query.[/bold red]\")\n",
    "            selected_queries = [query]\n",
    "            \n",
    "        console.print(f\"\\n[bold green]Selected {len(selected_queries)} queries for retrieval.[/bold green]\")\n",
    "        console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 5: Retrieve documents\n",
    "        console.print(\"[bold cyan]Retrieving relevant documents...[/bold cyan]\")\n",
    "        \n",
    "        if len(selected_queries) > 1:\n",
    "            # If multiple queries were selected, use the multi-query approach\n",
    "            print(\"LOG123: multiple queries this is working\")\n",
    "            docs, query_docs_map = self.retrieve_documents(selected_queries, k=num_results)\n",
    "            \n",
    "            # Generate a combined answer\n",
    "            console.print(\"\\n[bold cyan]Generating combined answer with documents context...[/bold cyan]\")\n",
    "            answer = self.generate_combined_answer(query, query_docs_map)\n",
    "        else:\n",
    "            # If only one query was selected, use the standard approach\n",
    "            print(\"LOG123: only this is working\")\n",
    "            try:\n",
    "                retrieved_docs = []\n",
    "                results = self.vector_db.similarity_search_with_relevance_scores(selected_queries[0], k=num_results)\n",
    "                \n",
    "                for i, (doc, score) in enumerate(results, 1):\n",
    "                    retrieved_docs.append(RetrievedDocument(\n",
    "                        document=doc,\n",
    "                        score=score,\n",
    "                        query=selected_queries[0],\n",
    "                        rank=i\n",
    "                    ))\n",
    "                \n",
    "                console.print(f\"[bold green]Retrieved {len(retrieved_docs)} documents[/bold green]\")\n",
    "                \n",
    "                # Show previews of the retrieved documents\n",
    "                console.print(\"\\n[bold cyan]Top retrieved documents:[/bold cyan]\")\n",
    "                for i, doc in enumerate(retrieved_docs[:5], 1):\n",
    "                    score_color = \"green\" if doc.score > 0.8 else \"yellow\" if doc.score > 0.6 else \"red\"\n",
    "                    console.print(f\"{i}. [bold {score_color}]Score: {doc.score:.4f}[/bold {score_color}]\")\n",
    "                    \n",
    "                    preview = doc.document.page_content[:150] + \"...\" if len(doc.document.page_content) > 150 else doc.document.page_content\n",
    "                    console.print(f\"   Preview: {preview}\")\n",
    "                    \n",
    "                    if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                        console.print(f\"   Source: {doc.document.metadata.get('source', 'Unknown')}\")\n",
    "                \n",
    "                # Generate answer with memory context\n",
    "                console.print(\"\\n[bold cyan]Generating answer with document context...[/bold cyan]\")\n",
    "                answer = self.generate_final_answer(query, retrieved_docs)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in retrieval: {e}\")\n",
    "                return f\"I'm sorry, I encountered an error while retrieving documents: {str(e)}\"\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def process_query(self, query: str, use_memory: bool = False, num_results: int = 5) -> str:\n",
    "        \"\"\"Main entry point that decides whether to use memory or the full pipeline.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            use_memory: Whether to explicitly use memory mode\n",
    "            num_results: Number of results to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Check if this might be a follow-up question that should use memory\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        should_use_memory = use_memory and (chat_history and self._is_likely_followup(query))\n",
    "        print(\"should_use_memory\",should_use_memory)\n",
    "        if should_use_memory:\n",
    "            console.print(\"[bold cyan]Using conversation memory to process this query...[/bold cyan]\")\n",
    "            return self.ask_with_memory(query, num_results)\n",
    "        else:\n",
    "            console.print(\"[bold cyan]Using full query improvement pipeline...[/bold cyan]\")\n",
    "            return self.search_with_query_feedback(query, num_results)\n",
    "    \n",
    "    def _is_likely_followup(self, query: str) -> bool:\n",
    "        \"\"\"Heuristically determines if a query is likely a follow-up question.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating if this is likely a follow-up\n",
    "        \"\"\"\n",
    "        # Look for pronouns, references, and questions that seem to refer to previous context\n",
    "        followup_indicators = [\n",
    "            # Pronouns\n",
    "            \"it\", \"this\", \"that\", \"they\", \"them\", \"these\", \"those\",\n",
    "            # Reference terms\n",
    "            \"previous\", \"earlier\", \"above\", \"mentioned\", \"last\", \"former\",\n",
    "            # Follow-up phrases\n",
    "            \"what about\", \"how about\", \"tell me more\", \"elaborate\", \"clarify\", \"explain further\",\n",
    "            \"can you expand\", \"additionally\", \"furthermore\", \"also\", \"related to that\",\n",
    "            # Short questions that likely need context\n",
    "            \"why\", \"how does\", \"can you explain\", \"what does\", \"what is\", \"who is\"\n",
    "        ]\n",
    "        \n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Check for any of the indicators\n",
    "        if any(indicator in query_lower for indicator in followup_indicators):\n",
    "            return True\n",
    "            \n",
    "        # # Check for very short queries (likely follow-ups)\n",
    "        # if len(query.split()) < 4:\n",
    "        #     return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    def clear_memory(self) -> None:\n",
    "        \"\"\"Clears the conversation memory.\"\"\"\n",
    "        self.memory.clear()\n",
    "        console.print(\"[bold green]Conversation memory cleared.[/bold green]\")\n",
    "        \n",
    "    def get_chat_history(self) -> str:\n",
    "        \"\"\"Returns the formatted chat history.\n",
    "        \n",
    "        Returns:\n",
    "            String representation of chat history\n",
    "        \"\"\"\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        if not chat_history:\n",
    "            return \"No conversation history.\"\n",
    "            \n",
    "        history_str = \"\"\n",
    "        for i, message in enumerate(chat_history):\n",
    "            if hasattr(message, \"content\"):\n",
    "                role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                history_str += f\"{role}: {message.content}\\n\\n\"\n",
    "                \n",
    "        return history_str\n",
    "\n",
    "\n",
    "def main():    \n",
    "    \"\"\"Main function to run the research assistant.\"\"\"\n",
    "    try:\n",
    "        from langchain_community.llms import OpenAI\n",
    "        from langchain_community.vectorstores import Chroma\n",
    "        from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "        \n",
    "        # Check for API key\n",
    "        if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "            console.print(\"[bold red]ERROR: OPENAI_API_KEY environment variable not set.[/bold red]\")\n",
    "            console.print(f\"Please set your API key with: export OPENAI_API_KEY={os.environ['OPENAI_API_KEY']}\")\n",
    "            return\n",
    "            \n",
    "        # Load models and vector database\n",
    "        console.print(\"[bold cyan]Initializing SageRAG Research Assistant...[/bold cyan]\")\n",
    "        \n",
    "        console.print(\"Loading embedding model...\")\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        \n",
    "        console.print(\"Loading language model...\")\n",
    "        llm = OllamaLLM(model=\"llama3.2\")\n",
    "        \n",
    "        console.print(\"Connecting to vector database...\")\n",
    "\n",
    "        vector_db = Chroma(persist_directory= DB_DIR, embedding_function=embeddings)\n",
    "        \n",
    "        console.print(\"[bold green]Initialization complete![/bold green]\")\n",
    "        \n",
    "        # Initialize the research assistant\n",
    "        assistant = ResearchAssistant(llm=llm, vector_db=vector_db)\n",
    "        \n",
    "        # Display welcome message\n",
    "        console.print(Panel.fit(\n",
    "            Markdown(\"# SageRAG Research Assistant\\n\\nAsk questions about scientific papers in the database.\"),\n",
    "            title=\"Welcome\",\n",
    "            border_style=\"cyan\"\n",
    "        ))\n",
    "        \n",
    "        # Main interaction loop\n",
    "        while True:\n",
    "            console.print(\"\\n[bold cyan]Options:[/bold cyan]\")\n",
    "            console.print(\"1. Ask a question\")\n",
    "            console.print(\"2. View conversation history\")\n",
    "            console.print(\"3. Clear conversation history\")\n",
    "            console.print(\"4. Exit\")\n",
    "            \n",
    "            choice = input(\"\\nEnter your choice (1-4): \").strip()\n",
    "            \n",
    "            if choice == \"1\":\n",
    "                query = input(f\"\\n[bold]Enter your query:[/bold] \")\n",
    "                if not query.strip():\n",
    "                    console.print(f\"[yellow]Empty query. Please try again.[/yellow]\")\n",
    "                    continue\n",
    "                    \n",
    "                # Ask the user whether to use memory mode or full pipeline\n",
    "                use_memory_input = input(\"Use conversation memory? (y/n): \").lower()\n",
    "                use_memory = use_memory_input.startswith('y')\n",
    "                \n",
    "                # Process the query\n",
    "                console.print(f\"\\n[bold cyan]Processing your query...[/bold cyan]\")\n",
    "                answer = assistant.process_query(query, use_memory=use_memory)\n",
    "                \n",
    "                # Display the final answer nicely formatted\n",
    "                console.print(Panel(\n",
    "                    Markdown(answer),\n",
    "                    title=\"Answer\",\n",
    "                    border_style=\"green\",\n",
    "                    width=100\n",
    "                ))\n",
    "                \n",
    "            elif choice == \"2\":\n",
    "                history = assistant.get_chat_history()\n",
    "                console.print(Panel(\n",
    "                    history if history else \"No conversation history.\",\n",
    "                    title=\"Conversation History\",\n",
    "                    border_style=\"blue\"\n",
    "                ))\n",
    "                \n",
    "            elif choice == \"3\":\n",
    "                assistant.clear_memory()\n",
    "                \n",
    "            elif choice == \"4\":\n",
    "                console.print(\"[bold green]Thank you for using SageRAG Research Assistant. Goodbye![/bold green]\")\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                console.print(\"[yellow]Invalid choice. Please enter a number from 1-4.[/yellow]\")\n",
    "                \n",
    "    except ImportError as e:\n",
    "        console.print(f\"[bold red]Error: Missing required packages: {e}[/bold red]\")\n",
    "        console.print(\"Please install the required packages with pip:\")\n",
    "        console.print(\"pip install langchain langchain_community sentence-transformers rich openai chromadb\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]An unexpected error occurred: {e}[/bold red]\")\n",
    "        import traceback\n",
    "        console.print(traceback.format_exc())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a417a99-a5cc-49e1-95b0-bb92e086b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "0,1,4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc9d2221-b31a-41f0-90ed-90f51994a0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import this module to use the ResearchAssistant class\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, BaseOutputParser\n",
    "from langchain.schema import Document, HumanMessage, AIMessage\n",
    "from langchain.vectorstores import VectorStore\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms.base import BaseLLM\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from pydantic import BaseModel, Field\n",
    "import logging\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"SageRAG\")\n",
    "\n",
    "# Rich console for prettier output\n",
    "console = Console()\n",
    "\n",
    "# Enhanced output parsers with better error handling\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Parse output that contains a numbered list and return as a list of strings.\"\"\"\n",
    "    \n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        \"\"\"Parse text into a list of strings.\"\"\"\n",
    "        # Updated regex to be more robust with various numbering styles\n",
    "        lines = re.findall(r\"^\\s*\\d+\\.?\\s+(.*?)$\", text, re.MULTILINE)\n",
    "        return [line.strip() for line in lines if line.strip()]\n",
    "    \n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"line_list\"\n",
    "\n",
    "class QueryRating(BaseModel):\n",
    "    \"\"\"Schema for query rating output.\"\"\"\n",
    "    rating: int = Field(description=\"Rating from 1-5\")\n",
    "    explanation: str = Field(description=\"Explanation for the rating\")\n",
    "\n",
    "@dataclass\n",
    "class RetrievedDocument:\n",
    "    \"\"\"Dataclass for tracking retrieved document info.\"\"\"\n",
    "    document: Document\n",
    "    score: float\n",
    "    query: str = \"\"\n",
    "    rank: int = 0\n",
    "\n",
    "class ResearchAssistant:\n",
    "    \"\"\"AI Research Assistant using RAG for scientific paper queries.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        llm: BaseLLM, \n",
    "        vector_db: VectorStore, \n",
    "        embeddings_model: Optional[SentenceTransformer] = None,\n",
    "        relevance_threshold: float = 0.6,\n",
    "        max_docs_per_query: int = 3,\n",
    "        always_use_fallback: bool = True  # New parameter to control fallback behavior\n",
    "    ):\n",
    "        \"\"\"Initialize the Research Assistant with LLM and vector database.\n",
    "        \n",
    "        Args:\n",
    "            llm: The language model to use for generation\n",
    "            vector_db: Vector database for document retrieval\n",
    "            embeddings_model: Optional SentenceTransformer model for semantic similarity\n",
    "            relevance_threshold: Minimum relevance score for documents\n",
    "            max_docs_per_query: Maximum documents to use per query\n",
    "            always_use_fallback: Whether to always use LLM fallback for general knowledge questions\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.vector_db = vector_db\n",
    "        self.relevance_threshold = relevance_threshold\n",
    "        self.max_docs_per_query = max_docs_per_query\n",
    "        self.always_use_fallback = always_use_fallback\n",
    "        \n",
    "        # Initialize sentence transformer model for semantic similarity if not provided\n",
    "        if embeddings_model is None:\n",
    "            logger.info(\"Initializing default embedding model\")\n",
    "            self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        else:\n",
    "            self.semantic_model = embeddings_model\n",
    "            \n",
    "        # Initialize memory\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        # Create a retriever for direct use\n",
    "        self.retriever = vector_db.as_retriever()\n",
    "        \n",
    "        # Define parsers\n",
    "        self.json_parser = JsonOutputParser(pydantic_object=QueryRating)\n",
    "        self.list_parser = LineListOutputParser()\n",
    "        \n",
    "        logger.info(\"Research Assistant initialized successfully\")\n",
    "    \n",
    "    def rate_query(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Rates the query and gives an explanation.\n",
    "        \n",
    "        Args:\n",
    "            query: The user's query to be evaluated\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing rating and explanation\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an intelligent assistant trained to evaluate search queries for a scientific research database.\n",
    "        Given the following query: \"{query}\"\n",
    "        \n",
    "        1. Rate the query on a scale of 1 (very poor) to 5 (excellent) based on:\n",
    "           - Clarity: Is the query clear and unambiguous?\n",
    "           - Specificity: Does it contain specific technical terms or concepts?\n",
    "           - Relevance: Is it focused on retrieving scientific content?\n",
    "           - Retrievability: Will it work well with vector search?\n",
    "        \n",
    "        2. Provide a short explanation for the rating (what makes it effective or ineffective).\n",
    "        \n",
    "        Respond in JSON format:\n",
    "        {{\n",
    "          \"rating\": <number between 1-5>,\n",
    "          \"explanation\": \"<your explanation>\"\n",
    "        }}\n",
    "        \"\"\")\n",
    "        \n",
    "        chain: Runnable = prompt | self.llm | self.json_parser\n",
    "        \n",
    "        try:\n",
    "            return chain.invoke({\"query\": query})\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error rating query: {e}\")\n",
    "            # Fallback response if parsing fails\n",
    "            return {\n",
    "                \"rating\": 3, \n",
    "                \"explanation\": \"Unable to rate query properly. Consider adding more specific terms.\"\n",
    "            }\n",
    "    \n",
    "    def is_general_knowledge_question(self, query: str) -> bool:\n",
    "        \"\"\"Determines if a question is likely a general knowledge question rather than research-specific.\n",
    "        \n",
    "        Args:\n",
    "            query: The user's query to evaluate\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating if this is likely a general knowledge question\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an intelligent assistant analyzing a question. Determine if this is a general knowledge question\n",
    "        that doesn't require specific scientific papers to answer properly.\n",
    "        \n",
    "        Question: \"{query}\"\n",
    "        \n",
    "        Examples of general knowledge questions:\n",
    "        - What is an operating system?\n",
    "        - Who invented the telephone?\n",
    "        - How does gravity work?\n",
    "        - What is the difference between RAM and ROM?\n",
    "        \n",
    "        Examples of research-specific questions:\n",
    "        - What are the latest developments in CRISPR gene editing?\n",
    "        - How does the transformer architecture improve machine translation accuracy?\n",
    "        - What methodologies are used to measure quantum entanglement?\n",
    "        - What were the findings of the 2023 paper on climate change impact on coral reefs?\n",
    "        \n",
    "        Is this a general knowledge question that could be answered without specific research papers?\n",
    "        Answer only YES or NO.\n",
    "        \"\"\")\n",
    "        \n",
    "        try:\n",
    "            result = self.llm.invoke(prompt.format(query=query))\n",
    "            return \"YES\" in result.upper()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error determining if general knowledge question: {e}\")\n",
    "            return False  # Default to assuming it's not general knowledge\n",
    "    \n",
    "    def suggest_rewrites(self, query: str, chat_history: Optional[List] = None) -> List[str]:\n",
    "        \"\"\"Returns rephrased versions of the query optimized for retrieval.\n",
    "        \n",
    "        Args:\n",
    "            query: Original query to rewrite\n",
    "            chat_history: Optional conversation history for context\n",
    "            \n",
    "        Returns:\n",
    "            List of rewritten queries\n",
    "        \"\"\"\n",
    "        history_context = \"\"\n",
    "        if chat_history:\n",
    "            history_context = \"Consider this conversation context when rewriting the query:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use only last 3 messages for brevity\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    history_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"history_context\"],\n",
    "            template=\"\"\"You are an AI assistant specializing in scientific research queries.\n",
    "            \n",
    "            {history_context}\n",
    "            \n",
    "            Rephrase the question in 5 different ways to improve retrieval from a scientific paper database. \n",
    "            Focus on:\n",
    "            1. Using technical terminology for better vector matching\n",
    "            2. Breaking down complex queries into clearer formulations\n",
    "            3. Adding relevant synonyms or related concepts\n",
    "            4. Varying syntax while preserving semantic meaning\n",
    "            5. Including key entities and relationships from the original query\n",
    "            \n",
    "            Number each version starting with 1.\n",
    "            \n",
    "            Question: {question}\"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            result = (prompt | self.llm | self.list_parser).invoke({\n",
    "                \"question\": query,\n",
    "                \"history_context\": history_context\n",
    "            })\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error suggesting rewrites: {e}\")\n",
    "            # Return a minimal set of rewrites if parsing fails\n",
    "            return [\n",
    "                query,  # Original query\n",
    "                f\"research about {query}\",\n",
    "                f\"papers discussing {query}\"\n",
    "            ]\n",
    "    \n",
    "    def compute_confidence(self, original: str, rewritten: str) -> float:\n",
    "        \"\"\"Computes semantic similarity between original and rewritten queries.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            rewritten: Rewritten version\n",
    "            \n",
    "        Returns:\n",
    "            Similarity score between 0-1\n",
    "        \"\"\"\n",
    "        try:\n",
    "            vec_orig = self.semantic_model.encode(original, convert_to_tensor=True)\n",
    "            vec_rewrite = self.semantic_model.encode(rewritten, convert_to_tensor=True)\n",
    "            return float(util.pytorch_cos_sim(vec_orig, vec_rewrite).item())\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error computing similarity: {e}\")\n",
    "            return 0.7  # Default reasonable value\n",
    "    \n",
    "    def score_queries(self, original: str, queries: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Scores and sorts rephrased queries by confidence.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            queries: List of rewritten queries\n",
    "            \n",
    "        Returns:\n",
    "            List of (query, score) tuples sorted by score\n",
    "        \"\"\"\n",
    "        scored = [(q, self.compute_confidence(original, q)) for q in queries]\n",
    "        return sorted(scored, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    def present_query_options(self, original: str, queries: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Presents the original and rewritten queries with confidence scores.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            queries: List of rewritten queries\n",
    "            \n",
    "        Returns:\n",
    "            List of (query, score) tuples including original query\n",
    "        \"\"\"\n",
    "        # Add original query at the top\n",
    "        scored_queries = [(\"Original: \" + original, 1.0)]\n",
    "        \n",
    "        # Add scored rewritten queries\n",
    "        rewritten_scores = self.score_queries(original, queries)\n",
    "        for i, (query, score) in enumerate(rewritten_scores, 1):\n",
    "            scored_queries.append((f\"Rewrite {i}: {query}\", score))\n",
    "            \n",
    "        # Display options in a nice format\n",
    "        console.print(\"\\n[bold cyan]Available search queries:[/bold cyan]\")\n",
    "        for i, (query, score) in enumerate(scored_queries):\n",
    "            confidence_color = \"green\" if score > 0.8 else \"yellow\" if score > 0.6 else \"red\"\n",
    "            console.print(f\"[bold]{i}.[/bold] {query}\")\n",
    "            console.print(f\"   [bold {confidence_color}]Confidence: {score:.2f}[/bold {confidence_color}]\")\n",
    "            \n",
    "        return scored_queries\n",
    "    \n",
    "    def retrieve_documents(\n",
    "        self, \n",
    "        queries: List[str], \n",
    "        k: int = 5\n",
    "    ) -> Tuple[List[RetrievedDocument], Dict[str, List[RetrievedDocument]]]:\n",
    "        \"\"\"Retrieves documents using multiple queries, preserving query information.\n",
    "        \n",
    "        Args:\n",
    "            queries: List of queries to retrieve documents for\n",
    "            k: Number of documents to retrieve per query\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (all unique documents, query->documents mapping)\n",
    "        \"\"\"\n",
    "        all_docs: List[RetrievedDocument] = []\n",
    "        unique_content = set()\n",
    "        query_docs_map: Dict[str, List[RetrievedDocument]] = {}\n",
    "        \n",
    "        # Retrieve docs for each query\n",
    "        for query in queries:\n",
    "            console.print(f\"\\n[bold blue]Query:[/bold blue] '{query}'\")\n",
    "            \n",
    "            try:\n",
    "                results = self.vector_db.similarity_search_with_relevance_scores(query, k=k)\n",
    "                docs_for_query: List[RetrievedDocument] = []\n",
    "                \n",
    "                for i, (doc, score) in enumerate(results, 1):\n",
    "                    # Create retrieved document object\n",
    "                    retrieved_doc = RetrievedDocument(\n",
    "                        document=doc,\n",
    "                        score=score,\n",
    "                        query=query,\n",
    "                        rank=i\n",
    "                    )\n",
    "                    \n",
    "                    # Display result info\n",
    "                    score_color = \"green\" if score > 0.8 else \"yellow\" if score > 0.6 else \"red\"\n",
    "                    console.print(f\"[bold]--- Result {i} ---[/bold]\")\n",
    "                    console.print(f\"[bold {score_color}]Score: {score:.4f}[/bold {score_color}]\")\n",
    "                    \n",
    "                    # Show document preview\n",
    "                    preview = doc.page_content[:150] + \"...\" if len(doc.page_content) > 150 else doc.page_content\n",
    "                    console.print(Panel(preview, title=\"Content Preview\", width=100))\n",
    "                    \n",
    "                    # Show metadata if available\n",
    "                    if hasattr(doc, 'metadata') and doc.metadata:\n",
    "                        source = doc.metadata.get('source', 'Unknown')\n",
    "                        console.print(f\"[dim]Source: {source}[/dim]\")\n",
    "                    \n",
    "                    # Add to results for this query\n",
    "                    docs_for_query.append(retrieved_doc)\n",
    "                    \n",
    "                    # Only add unique documents to overall collection\n",
    "                    if doc.page_content not in unique_content:\n",
    "                        unique_content.add(doc.page_content)\n",
    "                        all_docs.append(retrieved_doc)\n",
    "                \n",
    "                query_docs_map[query] = docs_for_query\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error retrieving documents for query '{query}': {e}\")\n",
    "                console.print(f\"[bold red]Error retrieving documents for query: {query}[/bold red]\")\n",
    "        \n",
    "        console.print(f\"\\n[bold green]Total unique documents:[/bold green] {len(all_docs)}\")\n",
    "        return all_docs, query_docs_map\n",
    "\n",
    "    def rerank_docs(self, query: str, docs: List[RetrievedDocument]) -> List[RetrievedDocument]:\n",
    "        \"\"\"Reranks documents based on semantic similarity to the query.\n",
    "        \n",
    "        Args:\n",
    "            query: Query to compare documents against\n",
    "            docs: List of retrieved documents\n",
    "            \n",
    "        Returns:\n",
    "            Reranked list of documents with updated scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            query_embedding = self.semantic_model.encode(query, convert_to_tensor=True)\n",
    "            reranked = []\n",
    "            \n",
    "            for doc in docs:\n",
    "                doc_embedding = self.semantic_model.encode(doc.document.page_content, convert_to_tensor=True)\n",
    "                new_score = float(util.pytorch_cos_sim(query_embedding, doc_embedding).item())\n",
    "                \n",
    "                # Create new RetrievedDocument with updated score\n",
    "                reranked_doc = RetrievedDocument(\n",
    "                    document=doc.document,\n",
    "                    score=new_score,\n",
    "                    query=doc.query,\n",
    "                    rank=0  # Will be updated after sorting\n",
    "                )\n",
    "                reranked.append(reranked_doc)\n",
    "            \n",
    "            # Sort by score and update ranks\n",
    "            reranked.sort(key=lambda x: x.score, reverse=True)\n",
    "            for i, doc in enumerate(reranked, 1):\n",
    "                doc.rank = i\n",
    "                \n",
    "            return reranked\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reranking documents: {e}\")\n",
    "            return docs  # Return original documents if reranking fails\n",
    "    \n",
    "    def can_answer_without_retrieval(self, question: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"Determines if a question can be answered directly from memory without retrieval.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (can_answer, answer_if_available)\n",
    "        \"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        if not chat_history:\n",
    "            return False, None\n",
    "            \n",
    "        # Create the prompt to check if we can answer directly from the conversation\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an AI assistant helping with a scientific research conversation.\n",
    "        Given the following conversation history and a new question, determine if the question:\n",
    "        1. Can be answered directly based ONLY on the conversation history (like \"what was my previous question?\")\n",
    "        2. Does NOT require retrieving new information from scientific papers\n",
    "        \n",
    "        If both conditions are true, provide the answer. Otherwise, respond with \"NEEDS_RETRIEVAL\".\n",
    "        \n",
    "        Conversation History:\n",
    "        {chat_history}\n",
    "        \n",
    "        New Question: {question}\n",
    "        \n",
    "        Your assessment (answer directly or respond with \"NEEDS_RETRIEVAL\"):\n",
    "        \"\"\")\n",
    "        \n",
    "        # Format the chat history for context\n",
    "        history_str = \"\"\n",
    "        for message in chat_history[-5:]:  # Use only last 5 messages for brevity\n",
    "            if hasattr(message, \"content\"):\n",
    "                role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                history_str += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        try:\n",
    "            # Ask the LLM if this can be answered without retrieval\n",
    "            response = self.llm.invoke(\n",
    "                prompt.format(chat_history=history_str, question=question)\n",
    "            )\n",
    "            \n",
    "            # If the response is \"NEEDS_RETRIEVAL\", we need to use retrieval\n",
    "            if \"NEEDS_RETRIEVAL\" in response:\n",
    "                return False, None\n",
    "            else:\n",
    "                logger.info(\"Question can be answered from conversation history\")\n",
    "                return True, response\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error determining if retrieval needed: {e}\")\n",
    "            return False, None  # Default to retrieval if there's an error\n",
    "    \n",
    "    def generate_final_answer(\n",
    "        self, \n",
    "        question: str, \n",
    "        docs: List[RetrievedDocument], \n",
    "        max_docs: int = 5,\n",
    "        fallback_to_llm: bool = True\n",
    "    ) -> str:\n",
    "        \"\"\"Generates the final answer from retrieved documents with fallback to LLM.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            docs: List of retrieved documents\n",
    "            max_docs: Maximum number of documents to include\n",
    "            fallback_to_llm: Whether to fall back to the LLM when docs are insufficient\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        # First, check if this might be a general knowledge question\n",
    "        is_general = self.is_general_knowledge_question(question)\n",
    "        \n",
    "        # First, check if we have any relevant documents\n",
    "        has_relevant_docs = any(doc.score >= self.relevance_threshold for doc in docs)\n",
    "        \n",
    "        # If no relevant docs were found and fallback is enabled, use LLM directly\n",
    "        if (not has_relevant_docs and fallback_to_llm) or (is_general and self.always_use_fallback):\n",
    "            if is_general:\n",
    "                console.print(\"[yellow]This appears to be a general knowledge question. Using LLM knowledge.[/yellow]\")\n",
    "            else:\n",
    "                console.print(\"[yellow]No relevant documents found. Falling back to LLM knowledge.[/yellow]\")\n",
    "            \n",
    "            # Create a direct LLM response prompt\n",
    "            fallback_prompt = PromptTemplate(\n",
    "                input_variables=[\"question\", \"chat_history\"],\n",
    "                template=\"\"\"You are a helpful scientific research assistant.\n",
    "                \n",
    "                The user has asked a question that either:\n",
    "                1. Could not be answered using our research paper database, or\n",
    "                2. Is a general knowledge question that doesn't require specific papers\n",
    "                \n",
    "                Please answer based on your general knowledge.\n",
    "                \n",
    "                {chat_history}\n",
    "                \n",
    "                Question: {question}\n",
    "                \n",
    "                Provide a helpful, informative answer. If this is a scientific or technical question,\n",
    "                explain concepts clearly and accurately. Don't apologize for not using specific papers \n",
    "                - just provide your best answer directly.\n",
    "                \n",
    "                Answer:\"\"\"\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                answer = (fallback_prompt | self.llm).invoke({\n",
    "                    \"question\": question,\n",
    "                    \"chat_history\": chat_context\n",
    "                })\n",
    "                \n",
    "                # Save the QA pair to memory\n",
    "                self.memory.chat_memory.add_messages([\n",
    "                    HumanMessage(content=question),\n",
    "                    AIMessage(content=answer)\n",
    "                ])\n",
    "                \n",
    "                return answer\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating fallback answer: {e}\")\n",
    "                return \"I'm sorry, I don't have enough information to answer your question accurately.\"\n",
    "        \n",
    "        # Format document content with metadata\n",
    "        context_pieces = []\n",
    "        for i, doc in enumerate(docs[:max_docs]):\n",
    "            chunk = f\"Document {i+1} [Relevance: {doc.score:.2f}]:\\n{doc.document.page_content}\"\n",
    "            \n",
    "            # Add metadata if available\n",
    "            if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                source = doc.document.metadata.get('source', 'Unknown')\n",
    "                chunk += f\"\\nSource: {source}\"\n",
    "                \n",
    "            context_pieces.append(chunk)\n",
    "            \n",
    "        context = \"\\n\\n---\\n\\n\".join(context_pieces)\n",
    "        \n",
    "        # If we have context but it might not be sufficient, use a prompt that can fall back to general knowledge\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful scientific research assistant analyzing scientific papers. \n",
    "            Use the following retrieved context chunks to answer the user's question thoroughly.\n",
    "            \n",
    "            Guidelines:\n",
    "            - Focus on providing accurate information from the papers\n",
    "            - Synthesize information across documents when appropriate\n",
    "            - Cite the sources of information in your answer (e.g., \"According to Document 1...\")\n",
    "            - If the information isn't sufficient in the context, supplement with your general knowledge,\n",
    "              clearly indicating which parts of your answer come from the papers and which parts are from your general knowledge\n",
    "            - If the context contains conflicting information, highlight the disagreement and possible reasons\n",
    "            - Maintain scientific accuracy above all else\n",
    "            - Don't apologize for using general knowledge - just be clear about what comes from papers vs. your knowledge\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Generate answer\n",
    "            answer = (prompt | self.llm).invoke({\n",
    "                \"question\": question, \n",
    "                \"context\": context,\n",
    "                \"chat_history\": chat_context\n",
    "            })\n",
    "            \n",
    "            # Save the QA pair to memory\n",
    "            self.memory.chat_memory.add_messages([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=answer)\n",
    "            ])\n",
    "            \n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating answer: {e}\")\n",
    "            return \"I'm sorry, I encountered an error while generating your answer. Please try rephrasing your question.\"\n",
    "    \n",
    "    def generate_combined_answer(\n",
    "        self, \n",
    "        question: str, \n",
    "        query_results: Dict[str, List[RetrievedDocument]],\n",
    "        fallback_to_llm: bool = True\n",
    "    ) -> str:\n",
    "        \"\"\"Generates a combined answer from multiple query results with fallback.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            query_results: Dictionary mapping queries to retrieved documents\n",
    "            fallback_to_llm: Whether to fall back to the LLM when docs are insufficient\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        # Check if this might be a general knowledge question\n",
    "        is_general = self.is_general_knowledge_question(question)\n",
    "        \n",
    "        # Check if we have any relevant documents at all\n",
    "        all_docs = []\n",
    "        for docs_list in query_results.values():\n",
    "            all_docs.extend(docs_list)\n",
    "            \n",
    "        has_relevant_docs = any(doc.score >= self.relevance_threshold for doc in all_docs)\n",
    "        \n",
    "        # If no relevant documents and fallback enabled, use LLM directly\n",
    "        if (not has_relevant_docs and fallback_to_llm) or (is_general and self.always_use_fallback):\n",
    "            if is_general:\n",
    "                console.print(\"[yellow]This appears to be a general knowledge question. Using LLM knowledge.[/yellow]\")\n",
    "            else:\n",
    "                console.print(\"[yellow]No relevant documents found across all queries. Falling back to LLM knowledge.[/yellow]\")\n",
    "            \n",
    "            # Create a direct LLM response prompt\n",
    "            fallback_prompt = PromptTemplate(\n",
    "                input_variables=[\"question\", \"chat_history\"],\n",
    "                template=\"\"\"You are a helpful scientific research assistant.\n",
    "                \n",
    "                The user has asked a question that either:\n",
    "                1. Could not be answered using our research paper database, or\n",
    "                2. Is a general knowledge question that doesn't require specific papers\n",
    "                \n",
    "                Please answer based on your general knowledge.\n",
    "                \n",
    "                {chat_history}\n",
    "                \n",
    "                Question: {question}\n",
    "                \n",
    "                Provide a helpful, informative answer. If this is a scientific or technical question,\n",
    "                explain concepts clearly and accurately. Don't apologize for not using specific papers \n",
    "                - just provide your best answer directly.\n",
    "                \n",
    "                Answer:\"\"\"\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                answer = (fallback_prompt | self.llm).invoke({\n",
    "                    \"question\": question,\n",
    "                    \"chat_history\": chat_context\n",
    "                })\n",
    "                \n",
    "                # Save the QA pair to memory\n",
    "                self.memory.chat_memory.add_messages([\n",
    "                    HumanMessage(content=question),\n",
    "                    AIMessage(content=answer)\n",
    "                ])\n",
    "                \n",
    "                return answer\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating fallback answer: {e}\")\n",
    "                return \"I'm sorry, I don't have enough information to answer your question accurately.\"\n",
    "        \n",
    "        # Combine all relevant documents from different queries\n",
    "        all_context_sections = []\n",
    "        \n",
    "        for query, docs in query_results.items():\n",
    "            # Use only the top N documents for each query\n",
    "            docs_for_query = docs[:self.max_docs_per_query]\n",
    "            if docs_for_query:\n",
    "                all_context_sections.append(f\"Results for query: '{query}'\")\n",
    "                for i, doc in enumerate(docs_for_query, 1):\n",
    "                    section = f\"Document {i} [Relevance: {doc.score:.2f}]:\\n{doc.document.page_content}\"\n",
    "                    \n",
    "                    # Add metadata if available\n",
    "                    if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                        source = doc.document.metadata.get('source', 'Unknown')\n",
    "                        section += f\"\\nSource: {source}\"\n",
    "                        \n",
    "                    all_context_sections.append(section)\n",
    "        \n",
    "        # Join all context sections\n",
    "        combined_context = \"\\n\\n---\\n\\n\".join(all_context_sections)\n",
    "        \n",
    "        # Create a prompt that emphasizes synthesizing information and can fall back to general knowledge\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful scientific research assistant analyzing scientific papers. \n",
    "            The user's question has been reformulated in several ways, and each formulation returned different documents.\n",
    "            \n",
    "            Guidelines:\n",
    "            - Synthesize information across ALL retrieved documents to provide a comprehensive answer\n",
    "            - Compare and contrast findings from different sources\n",
    "            - Highlight the most relevant information from each source\n",
    "            - Cite the specific documents you're referencing (e.g., \"According to the paper in Document 3...\")\n",
    "            - If information conflicts across sources, explain the different perspectives\n",
    "            - If the information is insufficient, supplement with your general knowledge,\n",
    "              clearly indicating which parts of your answer come from the papers and which are from your general knowledge\n",
    "            - Maintain scientific accuracy above all else\n",
    "            - Don't apologize for using general knowledge - just be clear about what information comes from papers vs. your knowledge\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Context from multiple query formulations:\n",
    "            {context}\n",
    "            \n",
    "            Original Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Generate answer\n",
    "            answer = (prompt | self.llm).invoke({\n",
    "                \"question\": question, \n",
    "                \"context\": combined_context,\n",
    "                \"chat_history\": chat_context\n",
    "            })\n",
    "            \n",
    "            # Save the QA pair to memory\n",
    "            self.memory.chat_memory.add_messages([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=answer)\n",
    "            ])\n",
    "            \n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating combined answer: {e}\")\n",
    "            return \"I'm sorry, I encountered an error while generating your answer. Please try rephrasing your question.\"\n",
    "    \n",
    "    def answer_query(self, query: str, use_retrieval: bool = True) -> str:\n",
    "        \"\"\"Direct answer method that either does retrieval or uses LLM fallback.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            use_retrieval: Whether to use retrieval process or direct LLM\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # First check if we can answer directly from conversation history\n",
    "        can_answer, direct_answer = self.can_answer_without_retrieval(query)\n",
    "        if can_answer:\n",
    "            console.print(\"[green]Answering from conversation history[/green]\")\n",
    "            return direct_answer\n",
    "        \n",
    "        # Check if this is a general knowledge question\n",
    "        is_general = self.is_general_knowledge_question(query)\n",
    "        \n",
    "        # If it's general knowledge and we're set to always use fallback for those\n",
    "        if is_general and self.always_use_fallback and not use_retrieval:\n",
    "            console.print(\"[yellow]General knowledge question detected. Using LLM directly.[/yellow]\")\n",
    "            # Create a direct LLM response prompt\n",
    "            chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "            chat_context = \"\"\n",
    "            if chat_history:\n",
    "                chat_context = \"Previous conversation:\\n\"\n",
    "                for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                    if hasattr(message, \"content\"):\n",
    "                        role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                        chat_context += f\"{role}: {message.content}\\n\"\n",
    "            \n",
    "            fallback_prompt = PromptTemplate(\n",
    "                input_variables=[\"question\", \"chat_history\"],\n",
    "                template=\"\"\"You are a helpful scientific research assistant.\n",
    "                \n",
    "                The user has asked a general knowledge question that doesn't require specific papers.\n",
    "                Please answer based on your general knowledge.\n",
    "                \n",
    "                {chat_history}\n",
    "                \n",
    "                Question: {question}\n",
    "                \n",
    "                Provide a helpful, informative answer. If this is a scientific or technical question,\n",
    "                explain concepts clearly and accurately.\n",
    "                \n",
    "                Answer:\"\"\"\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                answer = (fallback_prompt | self.llm).invoke({\n",
    "                    \"question\": query,\n",
    "                    \"chat_history\": chat_context\n",
    "                })\n",
    "                \n",
    "                # Save the QA pair to memory\n",
    "                self.memory.chat_memory.add_messages([\n",
    "                    HumanMessage(content=query),\n",
    "                    AIMessage(content=answer)\n",
    "                ])\n",
    "                \n",
    "                return answer\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating direct answer: {e}\")\n",
    "                return \"I'm sorry, I encountered an error while generating your answer.\"\n",
    "        \n",
    "        # Otherwise, proceed with full retrieval pipeline\n",
    "        return self.search_with_query_feedback(query)\n",
    "    \n",
    "    def search_with_query_feedback(self, query: str, num_results: int = 5) -> str:\n",
    "        \"\"\"Main pipeline that processes a query and returns an answer.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            num_results: Number of results to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Rate the original query\n",
    "            console.print(\"\\n[bold cyan]Evaluating your query...[/bold cyan]\")\n",
    "            rating_result = self.rate_query(query)\n",
    "            \n",
    "            rating_color = \"green\" if rating_result['rating'] >= 4 else \"yellow\" if rating_result['rating'] >= 3 else \"red\"\n",
    "            console.print(f\"[bold {rating_color}]Query Rating: {rating_result['rating']}/5[/bold {rating_color}]\")\n",
    "            console.print(f\"Explanation: {rating_result['explanation']}\")\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Check if this is a general knowledge question\n",
    "            is_general = self.is_general_knowledge_question(query)\n",
    "            if is_general and self.always_use_fallback:\n",
    "                console.print(\"[yellow]This appears to be a general knowledge question. Proceeding with simplified search.[/yellow]\")\n",
    "                # For general knowledge questions, we might still want to check the database briefly\n",
    "                # but we'll rely more on the LLM fallback\n",
    "            \n",
    "            # Step 2: Suggest rewrites if the rating is less than perfect\n",
    "            rewritten_queries = []\n",
    "            if rating_result[\"rating\"] < 5:\n",
    "                console.print(\"[bold cyan]Generating improved query variations...[/bold cyan]\")\n",
    "                # Get chat history for context-aware rewrites\n",
    "                chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "                rewritten_queries = self.suggest_rewrites(query, chat_history)\n",
    "                console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 3: Present options to the user\n",
    "            query_options = self.present_query_options(query, rewritten_queries)\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 4: Get user selection\n",
    "            console.print(\"[bold cyan]Select queries to use:[/bold cyan]\")\n",
    "            console.print(\"Enter the numbers of the queries you want to use (comma-separated, e.g., '0,2,3')\")\n",
    "            console.print(\"Or press Enter to use all queries\")\n",
    "            selected_indices_input = input(\"> \")\n",
    "            \n",
    "            if selected_indices_input.strip() == \"\":\n",
    "                # Use all queries if no selection\n",
    "                selected_indices = list(range(len(query_options)))\n",
    "            else:\n",
    "                try:\n",
    "                    selected_indices = [int(idx.strip()) for idx in selected_indices_input.split(\",\")]\n",
    "                except ValueError:\n",
    "                    console.print(\"[bold red]Invalid input. Using original query only.[/bold red]\")\n",
    "                    selected_indices = [0]  # Default to original query\n",
    "            \n",
    "            # Get the selected queries (without the prefix and score)\n",
    "            selected_queries = []\n",
    "            for idx in selected_indices:\n",
    "                if 0 <= idx < len(query_options):\n",
    "                    query_text = query_options[idx][0]\n",
    "                    # Remove the \"Original: \" or \"Rewrite N: \" prefix\n",
    "                    if \"Original: \" in query_text:\n",
    "                        query_text = query_text.replace(\"Original: \", \"\")\n",
    "                    elif \"Rewrite \" in query_text:\n",
    "                        query_text = query_text.split(\": \", 1)[1] if \": \" in query_text else query_text\n",
    "                    selected_queries.append(query_text)\n",
    "            \n",
    "            if not selected_queries:\n",
    "                console.print(\"[bold red]No valid queries selected. Using original query.[/bold red]\")\n",
    "                selected_queries = [query]\n",
    "                \n",
    "            console.print(f\"\\n[bold green]Selected {len(selected_queries)} queries for retrieval.[/bold green]\")\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 5: Retrieve documents\n",
    "            console.print(\"[bold cyan]Retrieving relevant documents...[/bold cyan]\")\n",
    "            docs, query_docs_map = self.retrieve_documents(selected_queries, k=num_results)\n",
    "            \n",
    "            # Step 6: Generate answer\n",
    "            if len(selected_queries) > 1:\n",
    "                console.print(\"\\n[bold cyan]Generating combined answer based on multiple query results...[/bold cyan]\")\n",
    "                final_answer = self.generate_combined_answer(\n",
    "                    query, \n",
    "                    query_docs_map,\n",
    "                    fallback_to_llm=True  # Always enable fallback\n",
    "                )\n",
    "            else:\n",
    "                # For single query, rerank and use the traditional approach\n",
    "                console.print(\"\\n[bold cyan]Reranking documents based on relevance to the original query...[/bold cyan]\")\n",
    "                reranked_docs = self.rerank_docs(query, docs)\n",
    "                \n",
    "                # Apply relevance threshold\n",
    "                filtered_docs = [\n",
    "                    doc for doc in reranked_docs if doc.score >= self.relevance_threshold\n",
    "                ]\n",
    "                \n",
    "                # Display relevance information\n",
    "                has_relevant_docs = len(filtered_docs) > 0\n",
    "                if not has_relevant_docs:\n",
    "                    console.print(\"[yellow]No documents met the relevance threshold. Using all retrieved documents but may fall back to LLM knowledge.[/yellow]\")\n",
    "                    filtered_docs = reranked_docs\n",
    "                \n",
    "                # Generate the final answer with fallback enabled\n",
    "                console.print(\"\\n[bold cyan]Generating answer...[/bold cyan]\")\n",
    "                final_answer = self.generate_final_answer(\n",
    "                    query, \n",
    "                    filtered_docs, \n",
    "                    fallback_to_llm=True  # Always enable fallback\n",
    "                )\n",
    "            \n",
    "            return final_answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in search pipeline: {e}\")\n",
    "            # Fall back to direct LLM answer if the pipeline fails\n",
    "            console.print(\"[bold red]Error in search pipeline. Falling back to LLM.[/bold red]\")\n",
    "            \n",
    "            chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "            chat_context = \"\"\n",
    "            if chat_history:\n",
    "                chat_context = \"Previous conversation:\\n\"\n",
    "                for message in chat_history[-3:]:\n",
    "                    if hasattr(message, \"content\"):\n",
    "                        role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                        chat_context += f\"{role}: {message.content}\\n\"\n",
    "            \n",
    "            # Create a direct LLM response as emergency fallback\n",
    "            emergency_prompt = PromptTemplate(\n",
    "                input_variables=[\"question\", \"chat_history\"],\n",
    "                template=\"\"\"You are a helpful scientific research assistant.\n",
    "                \n",
    "                There was an error retrieving information from our research database.\n",
    "                Please answer the question based on your general knowledge.\n",
    "                \n",
    "                {chat_history}\n",
    "                \n",
    "                Question: {question}\n",
    "                \n",
    "                Answer:\"\"\"\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                answer = (emergency_prompt | self.llm).invoke({\n",
    "                    \"question\": query,\n",
    "                    \"chat_history\": chat_context\n",
    "                })\n",
    "                return answer\n",
    "            except:\n",
    "                return \"I apologize, but I'm having technical difficulties. Please try again with a different question.\"\n",
    "\n",
    "# Example usage implementation \n",
    "def create_research_assistant(llm, vector_db, embeddings_model=None):\n",
    "    \"\"\"Factory function to create a research assistant with the specified components.\"\"\"\n",
    "    return ResearchAssistant(\n",
    "        llm=llm,\n",
    "        vector_db=vector_db,\n",
    "        embeddings_model=embeddings_model,\n",
    "        relevance_threshold=0.5,  # Lower threshold to be more lenient with document relevance\n",
    "        max_docs_per_query=3,\n",
    "        always_use_fallback=True  # Always use LLM fallback for general knowledge\n",
    "    )\n",
    "\n",
    "# Interactive command-line interface\n",
    "def run_cli(assistant):\n",
    "    \"\"\"Run an interactive CLI for the research assistant.\"\"\"\n",
    "    console.print(\"[bold green]Research Assistant CLI[/bold green]\")\n",
    "    console.print(\"Type 'exit' to quit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\n[bold blue]Ask a question:[/bold blue] \")\n",
    "        if question.lower() in ('exit', 'quit', 'q'):\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            answer = assistant.answer_query(question)\n",
    "            console.print(\"\\n[bold green]Answer:[/bold green]\")\n",
    "            console.print(Panel(Markdown(answer), width=100))\n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Error:[/bold red] {e}\")\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # You would need to define these before using:\n",
    "    # from langchain.llms import ChatOpenAI\n",
    "    # from langchain.vectorstores import Chroma\n",
    "    # \n",
    "    # llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    # vector_db = Chroma(embedding_function=...)\n",
    "    # \n",
    "    # assistant = create_research_assistant(llm, vector_db)\n",
    "    # run_cli(assistant)\n",
    "    print(\"Import this module to use the ResearchAssistant class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab1fa817-a9b3-4feb-807a-570a2b51eece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:37:52,676 - SageRAG - INFO - Research Assistant initialized successfully\n"
     ]
    }
   ],
   "source": [
    "assistant = create_research_assistant(llm, vector_db, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a6699f1-9f6b-4264-82e7-0c363463319f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query What is operating system ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:38:48,444 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Evaluating your query...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mEvaluating your query\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:38:48,916 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Query Rating: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">/</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mQuery Rating: \u001b[0m\u001b[1;32m4\u001b[0m\u001b[1;32m/\u001b[0m\u001b[1;32m5\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Explanation: The query is clear and concise, with a good balance of specificity and relevance. However, it lacks \n",
       "technical precision and could potentially retrieve more general information on operating systems, rather than \n",
       "scientific research content. This makes it moderately effective for retrieving results from a vector search.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Explanation: The query is clear and concise, with a good balance of specificity and relevance. However, it lacks \n",
       "technical precision and could potentially retrieve more general information on operating systems, rather than \n",
       "scientific research content. This makes it moderately effective for retrieving results from a vector search.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:38:50,718 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">This appears to be a general knowledge question. Proceeding with simplified search.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mThis appears to be a general knowledge question. Proceeding with simplified search.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Generating improved query variations...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mGenerating improved query variations\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:38:51,061 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-04-17 13:38:53,105 - langsmith.client - WARNING - Failed to send compressed multipart ingest: Connection error caused failure to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. Please confirm your internet connection. ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'api.smith.langchain.com\\', port=443): Max retries exceeded with url: /runs/multipart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x3190d48d0>: Failed to resolve \\'api.smith.langchain.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))'))\n",
      "Content-Length: 1176\n",
      "API Key: lsv2_********************************************82\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:38:57,796 - SageRAG - ERROR - Error computing similarity: 'HuggingFaceEmbeddings' object has no attribute 'encode'\n",
      "2025-04-17 13:38:57,798 - SageRAG - ERROR - Error computing similarity: 'HuggingFaceEmbeddings' object has no attribute 'encode'\n",
      "2025-04-17 13:38:57,799 - SageRAG - ERROR - Error computing similarity: 'HuggingFaceEmbeddings' object has no attribute 'encode'\n",
      "2025-04-17 13:38:57,804 - SageRAG - ERROR - Error computing similarity: 'HuggingFaceEmbeddings' object has no attribute 'encode'\n",
      "2025-04-17 13:38:57,805 - SageRAG - ERROR - Error computing similarity: 'HuggingFaceEmbeddings' object has no attribute 'encode'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Available search queries:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mAvailable search queries:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">.</span> Original: What is operating system ?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m0\u001b[0m\u001b[1m.\u001b[0m Original: What is operating system ?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Confidence: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;32mConfidence: \u001b[0m\u001b[1;32m1.00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: **Technical terminology for better vector matching**: <span style=\"color: #008000; text-decoration-color: #008000\">\"Query OS classification: Investigate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">definitions of computer system architecture, specifically those employing process management and resource </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">allocation algorithms.\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m1\u001b[0m: **Technical terminology for better vector matching**: \u001b[32m\"Query OS classification: Investigate \u001b[0m\n",
       "\u001b[32mdefinitions of computer system architecture, specifically those employing process management and resource \u001b[0m\n",
       "\u001b[32mallocation algorithms.\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.70</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.70\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: **Breaking down complex queries into clearer formulations**: <span style=\"color: #008000; text-decoration-color: #008000\">\"Exploring the concept of a 'computer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">operating system': Describe the primary functions, including (i) process scheduling, (ii) memory management, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(iii) I/O handling, in relation to its impact on computer performance and user experience.\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m2\u001b[0m: **Breaking down complex queries into clearer formulations**: \u001b[32m\"Exploring the concept of a 'computer \u001b[0m\n",
       "\u001b[32moperating system': Describe the primary functions, including \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi\u001b[0m\u001b[32m)\u001b[0m\u001b[32m process scheduling, \u001b[0m\u001b[32m(\u001b[0m\u001b[32mii\u001b[0m\u001b[32m)\u001b[0m\u001b[32m memory management, and \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32miii\u001b[0m\u001b[32m)\u001b[0m\u001b[32m I/O handling, in relation to its impact on computer performance and user experience.\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.70</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.70\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: **Adding relevant synonyms or related concepts**: <span style=\"color: #008000; text-decoration-color: #008000\">\"Inquire about the characteristics of an 'operating</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">system' (e.g., OS), particularly those involving multi-user, multi-tasking, and multi-processing support, as well </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">as their implications for security, stability, and efficiency.\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m3\u001b[0m: **Adding relevant synonyms or related concepts**: \u001b[32m\"Inquire about the characteristics of an 'operating\u001b[0m\n",
       "\u001b[32msystem' \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., OS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, particularly those involving multi-user, multi-tasking, and multi-processing support, as well \u001b[0m\n",
       "\u001b[32mas their implications for security, stability, and efficiency.\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.70</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.70\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>: **Varying syntax while preserving semantic meaning**: <span style=\"color: #008000; text-decoration-color: #008000\">\"Describe the nature of a computer operating </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">system: What are its fundamental components? How do they interact with hardware resources? What role does it play </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in facilitating efficient computation and user interaction?\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m4\u001b[0m: **Varying syntax while preserving semantic meaning**: \u001b[32m\"Describe the nature of a computer operating \u001b[0m\n",
       "\u001b[32msystem: What are its fundamental components? How do they interact with hardware resources? What role does it play \u001b[0m\n",
       "\u001b[32min facilitating efficient computation and user interaction?\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.70</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.70\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>: **Including key entities and relationships from the original query**: <span style=\"color: #008000; text-decoration-color: #008000\">\"Examine the relationship </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">between an 'operating system' (OS) and its constituent parts, including device drivers, kernel modules, and system </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">calls: How do these components interact to enable efficient execution of applications and manage system resources?\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m5\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m5\u001b[0m: **Including key entities and relationships from the original query**: \u001b[32m\"Examine the relationship \u001b[0m\n",
       "\u001b[32mbetween an 'operating system' \u001b[0m\u001b[32m(\u001b[0m\u001b[32mOS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and its constituent parts, including device drivers, kernel modules, and system \u001b[0m\n",
       "\u001b[32mcalls: How do these components interact to enable efficient execution of applications and manage system resources?\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.70</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.70\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Select queries to use:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mSelect queries to use:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Enter the numbers of the queries you want to use <span style=\"font-weight: bold\">(</span>comma-separated, e.g., <span style=\"color: #008000; text-decoration-color: #008000\">'0,2,3'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Enter the numbers of the queries you want to use \u001b[1m(\u001b[0mcomma-separated, e.g., \u001b[32m'0,2,3'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Or press Enter to use all queries\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Or press Enter to use all queries\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:39:02,688 - langsmith.client - WARNING - Failed to send compressed multipart ingest: Connection error caused failure to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. Please confirm your internet connection. ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'api.smith.langchain.com\\', port=443): Max retries exceeded with url: /runs/multipart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x3190c2f90>: Failed to resolve \\'api.smith.langchain.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))'))\n",
      "Content-Length: 5615\n",
      "API Key: lsv2_********************************************82\n",
      "2025-04-17 13:39:12,298 - langsmith.client - WARNING - Failed to send compressed multipart ingest: Connection error caused failure to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. Please confirm your internet connection. ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'api.smith.langchain.com\\', port=443): Max retries exceeded with url: /runs/multipart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x31909c850>: Failed to resolve \\'api.smith.langchain.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))'))\n",
      "Content-Length: 6092\n",
      "API Key: lsv2_********************************************82\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  0,1,3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Selected </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> queries for retrieval.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mSelected \u001b[0m\u001b[1;32m3\u001b[0m\u001b[1;32m queries for retrieval.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Retrieving relevant documents...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mRetrieving relevant documents\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Query:</span> <span style=\"color: #008000; text-decoration-color: #008000\">'What is operating system ?'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;34mQuery:\u001b[0m \u001b[32m'What is operating system ?'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.5895</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.5895\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ ● Operating System ConceptsbyAvi Silberschatz, GregGagne, and Peter Baer Galvin                  │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ ● Operating System ConceptsbyAvi Silberschatz, GregGagne, and Peter Baer Galvin                  │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2104.09487v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2104.09487v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.4490</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.4490\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Android OS                                                                                       │\n",
       "│ Case Study                                                                                       │\n",
       "│ OPERATING  SYSTEMS                                                                               │\n",
       "│ Submitted By: -                                                                                  │\n",
       "│ Mayank Goelmayank.co19@nsut.ac.in                                                                │\n",
       "│ GouravSingalgourav.co19@nsut.ac.in                                                               │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Android OS                                                                                       │\n",
       "│ Case Study                                                                                       │\n",
       "│ OPERATING  SYSTEMS                                                                               │\n",
       "│ Submitted By: -                                                                                  │\n",
       "│ Mayank Goelmayank.co19@nsut.ac.in                                                                │\n",
       "│ GouravSingalgourav.co19@nsut.ac.in                                                               │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2104.09487v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2104.09487v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3009</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3009\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ controlled by the file system are ultimately accessed via                                        │\n",
       "│ kernel-resident device d rivers.  A user process can                                             │\n",
       "│ implement any semantics it chooses fo...                                                         │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ controlled by the file system are ultimately accessed via                                        │\n",
       "│ kernel-resident device d rivers.  A user process can                                             │\n",
       "│ implement any semantics it chooses fo...                                                         │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/1403.5976v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/1403.5976v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.2779</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.2779\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Symposium on Operating Systems Principles (SOSP ’09) . 29–44.                                    │\n",
       "│ [11] Andrew Baumann, Gernot Heiser, Jonathan Appavoo, Dilma Da Silva,                            │\n",
       "│ Orran Krieger, Rob...                                                                            │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Symposium on Operating Systems Principles (SOSP ’09) . 29–44.                                    │\n",
       "│ [11] Andrew Baumann, Gernot Heiser, Jonathan Appavoo, Dilma Da Silva,                            │\n",
       "│ Orran Krieger, Rob...                                                                            │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2306.15076v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2306.15076v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.2669</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.2669\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ InternationalJournal of Computer Science &amp; Information Technology (IJCSIT) Vol 4, No 5, October  │\n",
       "│ 2012                                                                                             │\n",
       "│ 123                                                                                              │\n",
       "│ System Time statistics arecalculatedin the fo...                                                 │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ InternationalJournal of Computer Science & Information Technology (IJCSIT) Vol 4, No 5, October  │\n",
       "│ 2012                                                                                             │\n",
       "│ 123                                                                                              │\n",
       "│ System Time statistics arecalculatedin the fo...                                                 │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/1211.4839v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/1211.4839v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Query:</span> <span style=\"color: #008000; text-decoration-color: #008000\">'**Technical terminology for better vector matching**: \"Query OS classification: Investigate definitions of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">computer system architecture, specifically those employing process management and resource allocation algorithms.\"'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;34mQuery:\u001b[0m \u001b[32m'**Technical terminology for better vector matching**: \"Query OS classification: Investigate definitions of \u001b[0m\n",
       "\u001b[32mcomputer system architecture, specifically those employing process management and resource allocation algorithms.\"'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3459</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3459\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Neetu Goel,  R.B. Garg| A Comparative Study of CPU Scheduling Algorithms                         │\n",
       "│ International Journal of Graphics &amp; Image Processing |Vol 2|issue 4|November...                  │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Neetu Goel,  R.B. Garg| A Comparative Study of CPU Scheduling Algorithms                         │\n",
       "│ International Journal of Graphics & Image Processing |Vol 2|issue 4|November...                  │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/1307.4165v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/1307.4165v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3312</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3312\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ [8] D. Lo, L. Cheng, R. Govindaraju, P. Ranganathan, and C. Kozyrakis,                           │\n",
       "│ “Heracles: Improving resource efficiency at scale,” in Proceedings of the                        │\n",
       "│ 42nd ...                                                                                         │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ [8] D. Lo, L. Cheng, R. Govindaraju, P. Ranganathan, and C. Kozyrakis,                           │\n",
       "│ “Heracles: Improving resource efficiency at scale,” in Proceedings of the                        │\n",
       "│ 42nd ...                                                                                         │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2310.14741v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2310.14741v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3179</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3179\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ ● Operating System ConceptsbyAvi Silberschatz, GregGagne, and Peter Baer Galvin                  │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ ● Operating System ConceptsbyAvi Silberschatz, GregGagne, and Peter Baer Galvin                  │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2104.09487v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2104.09487v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3082</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3082\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ I., 2011. Mesos: A platform for ﬁne-grained resource sharing in the data center. In: NSDI. Vol.  │\n",
       "│ 11.                                                                                              │\n",
       "│ pp. 22–22.                                                                                       │\n",
       "│ Hussain, H., Malik, S. U. R., Hameed, A...                                                       │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ I., 2011. Mesos: A platform for ﬁne-grained resource sharing in the data center. In: NSDI. Vol.  │\n",
       "│ 11.                                                                                              │\n",
       "│ pp. 22–22.                                                                                       │\n",
       "│ Hussain, H., Malik, S. U. R., Hameed, A...                                                       │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/1705.03102v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/1705.03102v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3061</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3061\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ As can be seen from Figure 12, the observed worst case                                           │\n",
       "│ time recorded in Quest-V is within the bounds, but also                                          │\n",
       "│ close, to the prediction derived from E...                                                       │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ As can be seen from Figure 12, the observed worst case                                           │\n",
       "│ time recorded in Quest-V is within the bounds, but also                                          │\n",
       "│ close, to the prediction derived from E...                                                       │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/1310.6301v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/1310.6301v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Query:</span> <span style=\"color: #008000; text-decoration-color: #008000\">'**Adding relevant synonyms or related concepts**: \"Inquire about the characteristics of an '</span>operating \n",
       "system' <span style=\"font-weight: bold\">(</span>e.g., OS<span style=\"font-weight: bold\">)</span>, particularly those involving multi-user, multi-tasking, and multi-processing support, as well \n",
       "as their implications for security, stability, and efficiency.\"'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;34mQuery:\u001b[0m \u001b[32m'**Adding relevant synonyms or related concepts**: \"Inquire about the characteristics of an '\u001b[0moperating \n",
       "system' \u001b[1m(\u001b[0me.g., OS\u001b[1m)\u001b[0m, particularly those involving multi-user, multi-tasking, and multi-processing support, as well \n",
       "as their implications for security, stability, and efficiency.\"'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.5830</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.5830\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ ● Operating System ConceptsbyAvi Silberschatz, GregGagne, and Peter Baer Galvin                  │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ ● Operating System ConceptsbyAvi Silberschatz, GregGagne, and Peter Baer Galvin                  │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2104.09487v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2104.09487v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.4519</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.4519\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Android OS                                                                                       │\n",
       "│ Case Study                                                                                       │\n",
       "│ OPERATING  SYSTEMS                                                                               │\n",
       "│ Submitted By: -                                                                                  │\n",
       "│ Mayank Goelmayank.co19@nsut.ac.in                                                                │\n",
       "│ GouravSingalgourav.co19@nsut.ac.in                                                               │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Android OS                                                                                       │\n",
       "│ Case Study                                                                                       │\n",
       "│ OPERATING  SYSTEMS                                                                               │\n",
       "│ Submitted By: -                                                                                  │\n",
       "│ Mayank Goelmayank.co19@nsut.ac.in                                                                │\n",
       "│ GouravSingalgourav.co19@nsut.ac.in                                                               │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2104.09487v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2104.09487v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.4417</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.4417\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Symposium on Operating Systems Principles (SOSP ’09) . 29–44.                                    │\n",
       "│ [11] Andrew Baumann, Gernot Heiser, Jonathan Appavoo, Dilma Da Silva,                            │\n",
       "│ Orran Krieger, Rob...                                                                            │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Symposium on Operating Systems Principles (SOSP ’09) . 29–44.                                    │\n",
       "│ [11] Andrew Baumann, Gernot Heiser, Jonathan Appavoo, Dilma Da Silva,                            │\n",
       "│ Orran Krieger, Rob...                                                                            │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2306.15076v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2306.15076v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3319</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3319\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Yew. 2006. Live Updating Operating Systems Using Virtualization. In                              │\n",
       "│ Proceedings of the 2nd International Conference on Virtual Execution En-                         │\n",
       "│ vironment...                                                                                     │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Yew. 2006. Live Updating Operating Systems Using Virtualization. In                              │\n",
       "│ Proceedings of the 2nd International Conference on Virtual Execution En-                         │\n",
       "│ vironment...                                                                                     │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2306.15076v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2306.15076v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3313</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3313\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ AIOS: LLM Agent Operating System                                                                 │\n",
       "│ not using AIOS widens as the number of agents increases,                                         │\n",
       "│ underscoring AIOS’s effectiveness in managing concurrent                                         │\n",
       "│ ope...                                                                                           │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ AIOS: LLM Agent Operating System                                                                 │\n",
       "│ not using AIOS widens as the number of agents increases,                                         │\n",
       "│ underscoring AIOS’s effectiveness in managing concurrent                                         │\n",
       "│ ope...                                                                                           │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2403.16971v3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2403.16971v3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Total unique documents:</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mTotal unique documents:\u001b[0m \u001b[1;36m11\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Generating combined answer based on multiple query results...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mGenerating combined answer based on multiple query results\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:39:19,069 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">This appears to be a general knowledge question. Using LLM knowledge.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mThis appears to be a general knowledge question. Using LLM knowledge.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:39:19,406 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'An operating system (OS) is a software that manages computer hardware resources and provides a platform for running application software. It acts as an intermediary between computer hardware and user-level applications.\\n\\nThe primary functions of an operating system include:\\n\\n1. Process management: Creating, scheduling, and terminating tasks (programs) to optimize system performance.\\n2. Memory management: Allocating and deallocating memory for running programs.\\n3. File system management: Managing files, directories, and storage devices.\\n4. Input/Output operations: Handling user input and output, such as keyboard and mouse interactions, display output, and printer management.\\n5. Security and access control: Regulating access to computer resources based on user identity and permissions.\\n6. Networking: Enabling communication between the computer and other devices on a network.\\n\\nOperating systems can be classified into several types, including:\\n\\n1. Single-user, single-tasking operating systems (e.g., MS-DOS)\\n2. Multi-user, multi-tasking operating systems (e.g., Windows XP, macOS)\\n3. Mobile operating systems (e.g., Android, iOS)\\n\\nThe operating system software provides a layer of abstraction between the hardware and user-level applications, allowing users to interact with the computer using high-level commands and programs, rather than directly accessing low-level hardware components.\\n\\nIn summary, an operating system is essential software that manages computer resources, provides a platform for running applications, and enables users to interact with their computers in a convenient and efficient manner.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:39:28,105 - langsmith.client - WARNING - Failed to send compressed multipart ingest: Connection error caused failure to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. Please confirm your internet connection. ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'api.smith.langchain.com\\', port=443): Max retries exceeded with url: /runs/multipart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x3142727d0>: Failed to resolve \\'api.smith.langchain.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))'))\n",
      "Content-Length: 1176\n",
      "API Key: lsv2_********************************************82\n",
      "2025-04-17 13:39:37,707 - langsmith.client - WARNING - Failed to send compressed multipart ingest: Connection error caused failure to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. Please confirm your internet connection. ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'api.smith.langchain.com\\', port=443): Max retries exceeded with url: /runs/multipart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x3190cf310>: Failed to resolve \\'api.smith.langchain.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))'))\n",
      "Content-Length: 6919\n",
      "API Key: lsv2_********************************************82\n"
     ]
    }
   ],
   "source": [
    "question = input(\"Enter your query\")\n",
    "assistant.answer_query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065f4cf3-5efd-4f03-b2c0-08167871569a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
