{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6c9f53c-5fc2-4eee-9000-3cfae300122e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "scrolled": true
   },
   "source": [
    "# Improving DB with Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445904a8-9cd7-48a0-ab9e-093cfb9bd1a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class FoundationalKnowledgeManager:\n",
    "    def __init__(self, vector_db, embedding_model):\n",
    "        self.vector_db = vector_db\n",
    "        self.embedding_model = embedding_model\n",
    "        self.github_sources = {\n",
    "            \"algorithms\": \"TheAlgorithms/Python\",\n",
    "            \"os\": \"tuhdo/os01\",\n",
    "            \"system_design\": \"donnemartin/system-design-primer\",\n",
    "            \"python\": \"jakevdp/PythonDataScienceHandbook\",\n",
    "            \"ml\": \"microsoft/ML-For-Beginners\",\n",
    "            \"database\": \"pingcap/awesome-database-learning\",\n",
    "            \"networks\": \"leandromoreira/linux-network-performance-parameters\"\n",
    "        }\n",
    "        \n",
    "    def load_from_github(self, topic_key):\n",
    "        \"\"\"Load content from predefined GitHub repositories with foundational CS knowledge\"\"\"\n",
    "        if topic_key not in self.github_sources:\n",
    "            return f\"Topic {topic_key} not found in available sources\"\n",
    "            \n",
    "        repo = self.github_sources[topic_key]\n",
    "        # Implement GitHub content fetching logic here\n",
    "        # Use GitHub API to get markdown/text content from README files and other docs\n",
    "        \n",
    "        # Process content and add to vector DB with metadata\n",
    "        # Important: Tag these documents as foundational knowledge\n",
    "        # self.vector_db.add_texts(texts, metadatas=[{\"chunk_type\": \"foundational\", \"topic\": topic_key}])\n",
    "        \n",
    "        return f\"Loaded foundational knowledge for {topic_key} from {repo}\"\n",
    "        \n",
    "    def load_from_file(self, file_path, topic_name):\n",
    "        \"\"\"Load content from local file\"\"\"\n",
    "        # Implement file loading logic\n",
    "        # Process content and add to vector DB with metadata \n",
    "        # self.vector_db.add_texts(texts, metadatas=[{\"chunk_type\": \"foundational\", \"topic\": topic_name}])\n",
    "        \n",
    "    def load_standard_cs_topics(self):\n",
    "        \"\"\"Load all standard CS topics\"\"\"\n",
    "        results = []\n",
    "        for topic in self.github_sources:\n",
    "            results.append(self.load_from_github(topic))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6c0eec-dacb-4b49-81ad-dac04a9beac3",
   "metadata": {},
   "source": [
    "# Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b354f4-0b65-4e2e-8c7e-ad6f57c0271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"false\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"ResearchPro2\" \n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = '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'\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f02e6dc9-ea69-4a86-938e-fbd57f5ae39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Using cached scikit_learn-1.7.1-cp311-cp311-macosx_12_0_arm64.whl (8.7 MB)\n",
      "Installing collected packages: scikit-learn\n",
      "Successfully installed scikit-learn-1.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8f6906f-0a67-4bde-ac97-8d6e3cfeede1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/5n/8m4t00412h97v592vh5y0pgc0000gn/T/ipykernel_79293/2792269843.py\", line 7, in <module>\n",
      "    from sentence_transformers import SentenceTransformer, util\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/__init__.py\", line 14, in <module>\n",
      "    from sentence_transformers.cross_encoder import (\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/__init__.py\", line 3, in <module>\n",
      "    from .CrossEncoder import CrossEncoder\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py\", line 26, in <module>\n",
      "    from sentence_transformers.cross_encoder.fit_mixin import FitMixin\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/fit_mixin.py\", line 19, in <module>\n",
      "    from sentence_transformers.datasets.NoDuplicatesDataLoader import NoDuplicatesDataLoader\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/datasets/__init__.py\", line 13, in <module>\n",
      "    from .ParallelSentencesDataset import ParallelSentencesDataset\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/datasets/ParallelSentencesDataset.py\", line 19, in <module>\n",
      "    from sentence_transformers import SentenceTransformer\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py\", line 34, in <module>\n",
      "    from sentence_transformers.model_card import SentenceTransformerModelCardData, generate_model_card\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/model_card.py\", line 36, in <module>\n",
      "    from datasets import Dataset, DatasetDict, IterableDataset, IterableDatasetDict, Value\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/datasets/__init__.py\", line 17, in <module>\n",
      "    from .arrow_dataset import Dataset\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 56, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/arrow/__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/arrow/array.py\", line 52, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/ops/__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/ops/array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/computation/expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/computation/check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/compat/_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"/opt/anaconda3/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/numexpr/__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/5n/8m4t00412h97v592vh5y0pgc0000gn/T/ipykernel_79293/2792269843.py\", line 7, in <module>\n",
      "    from sentence_transformers import SentenceTransformer, util\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/__init__.py\", line 14, in <module>\n",
      "    from sentence_transformers.cross_encoder import (\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/__init__.py\", line 3, in <module>\n",
      "    from .CrossEncoder import CrossEncoder\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py\", line 26, in <module>\n",
      "    from sentence_transformers.cross_encoder.fit_mixin import FitMixin\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/fit_mixin.py\", line 19, in <module>\n",
      "    from sentence_transformers.datasets.NoDuplicatesDataLoader import NoDuplicatesDataLoader\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/datasets/__init__.py\", line 13, in <module>\n",
      "    from .ParallelSentencesDataset import ParallelSentencesDataset\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/datasets/ParallelSentencesDataset.py\", line 19, in <module>\n",
      "    from sentence_transformers import SentenceTransformer\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py\", line 34, in <module>\n",
      "    from sentence_transformers.model_card import SentenceTransformerModelCardData, generate_model_card\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/model_card.py\", line 36, in <module>\n",
      "    from datasets import Dataset, DatasetDict, IterableDataset, IterableDatasetDict, Value\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/datasets/__init__.py\", line 17, in <module>\n",
      "    from .arrow_dataset import Dataset\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 56, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/arrow/__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/arrow/array.py\", line 66, in <module>\n",
      "    from pandas.core.arrays.masked import BaseMaskedArray\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py\", line 61, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/nanops.py\", line 52, in <module>\n",
      "    bn = import_optional_dependency(\"bottleneck\", errors=\"warn\")\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/compat/_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"/opt/anaconda3/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/bottleneck/__init__.py\", line 7, in <module>\n",
      "    from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, util\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationBufferMemory\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArxivLoader\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/__init__.py:14\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[1;32m     11\u001b[0m     export_optimized_onnx_model,\n\u001b[1;32m     12\u001b[0m     export_static_quantized_openvino_model,\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     CrossEncoder,\n\u001b[1;32m     16\u001b[0m     CrossEncoderModelCardData,\n\u001b[1;32m     17\u001b[0m     CrossEncoderTrainer,\n\u001b[1;32m     18\u001b[0m     CrossEncoderTrainingArguments,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_card\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainer\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfit_mixin\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FitMixin\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_card\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData, generate_model_card\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     cross_encoder_init_args_decorator,\n\u001b[1;32m     30\u001b[0m     cross_encoder_predict_rank_args_decorator,\n\u001b[1;32m     31\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/fit_mixin.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_utils_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BatchEncoding\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining_args\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainingArguments\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mNoDuplicatesDataLoader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceLabelDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceLabelDataset\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceEvaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceEvaluator\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/datasets/__init__.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDenoisingAutoEncoderDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DenoisingAutoEncoderDataset\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mNoDuplicatesDataLoader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mParallelSentencesDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceLabelDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceLabelDataset\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentencesDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentencesDataset\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/datasets/ParallelSentencesDataset.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InputExample\n\u001b[1;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimilarity_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimilarityFunction\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __MODEL_HUB_ORGANIZATION__, __version__\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceEvaluator\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfit_mixin\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FitMixin\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Normalize, Pooling, Transformer\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/evaluation/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mBinaryClassificationEvaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BinaryClassificationEvaluator\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mEmbeddingSimilarityEvaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EmbeddingSimilarityEvaluator\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mInformationRetrievalEvaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InformationRetrievalEvaluator\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/evaluation/BinaryClassificationEvaluator.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Literal\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m average_precision_score, matthews_corrcoef\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m paired_cosine_distances, paired_euclidean_distances, paired_manhattan_distances\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceEvaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceEvaluator\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Install and import necessary packages\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import BaseOutputParser, StrOutputParser\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import time\n",
    "from typing import List, Literal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67465538-9a43-4952-9d20-79b8be8800d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PDF_DIR = \"../Database/arXiv_papers\" \n",
    "DB_DIR =\"../Database/Vector_DB\"\n",
    "# Embedding & Vector DB\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_db = Chroma(persist_directory=DB_DIR, embedding_function=embedding_model)\n",
    "\n",
    "# Load embedding model once for scoring\n",
    "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# LLM\n",
    "llm = OllamaLLM(model=\"llama3.2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc6c3966-ac6d-45b2-8274-130d417c3e94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Initializing SageRAG Research Assistant...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mInitializing SageRAG Research Assistant\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading embedding model<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading embedding model\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading language model<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading language model\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Connecting to vector database<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Connecting to vector database\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Initialization complete!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mInitialization complete!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 14:20:35,506 - SageRAG - INFO - Initializing default embedding model\n",
      "/var/folders/5n/8m4t00412h97v592vh5y0pgc0000gn/T/ipykernel_73751/3369783994.py:99: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  self.memory = ConversationBufferMemory(\n",
      "2025-06-25 14:20:39,483 - SageRAG - INFO - Research Assistant initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24803\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">╭──────────────────────────────────────────────────── Welcome ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span> ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span> ┃                                         <span style=\"font-weight: bold\">SageRAG Research Assistant</span>                                          ┃ <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span> ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span> Ask questions about scientific papers in the database.                                                          <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m╭─\u001b[0m\u001b[36m───────────────────────────────────────────────────\u001b[0m\u001b[36m Welcome \u001b[0m\u001b[36m───────────────────────────────────────────────────\u001b[0m\u001b[36m─╮\u001b[0m\n",
       "\u001b[36m│\u001b[0m ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ \u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m ┃                                         \u001b[1mSageRAG Research Assistant\u001b[0m                                          ┃ \u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ \u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m                                                                                                                 \u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m Ask questions about scientific papers in the database.                                                          \u001b[36m│\u001b[0m\n",
       "\u001b[36m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Options:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mOptions:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Ask a question\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m. Ask a question\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. View conversation history\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m. View conversation history\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Clear conversation history\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m. Clear conversation history\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. Go for Advanced Research\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4\u001b[0m. Go for Advanced Research\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. Exit\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m5\u001b[0m. Exit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your choice (1-5):  1\n",
      "\n",
      "[bold]Enter your query:[/bold]  What is Neural Network?\n",
      "Use conversation memory? (y/n):  n\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Processing your query...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mProcessing your query\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should_use_memory False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Using full query improvement pipeline...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mUsing full query improvement pipeline\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Evaluating your query...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mEvaluating your query\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Query Rating: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">/</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mQuery Rating: \u001b[0m\u001b[1;32m4\u001b[0m\u001b[1;32m/\u001b[0m\u001b[1;32m5\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Explanation: The query <span style=\"color: #008000; text-decoration-color: #008000\">'What is Neural Network?'</span> is rated as <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> out of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> due to its clarity, specificity, and \n",
       "relevance. The term <span style=\"color: #008000; text-decoration-color: #008000\">'Neural Network'</span> is a well-defined technical concept in the field of artificial intelligence \n",
       "and machine learning. However, it lacks specificity, making it potentially broad and open to multiple \n",
       "interpretations. This could lead to less accurate search results. Despite this, the query is still effective as it \n",
       "retrieves relevant scientific content related to neural networks.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Explanation: The query \u001b[32m'What is Neural Network?'\u001b[0m is rated as \u001b[1;36m4\u001b[0m out of \u001b[1;36m5\u001b[0m due to its clarity, specificity, and \n",
       "relevance. The term \u001b[32m'Neural Network'\u001b[0m is a well-defined technical concept in the field of artificial intelligence \n",
       "and machine learning. However, it lacks specificity, making it potentially broad and open to multiple \n",
       "interpretations. This could lead to less accurate search results. Despite this, the query is still effective as it \n",
       "retrieves relevant scientific content related to neural networks.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Generating improved query variations...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mGenerating improved query variations\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Available search queries:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mAvailable search queries:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">.</span> Original: What is Neural Network?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m0\u001b[0m\u001b[1m.\u001b[0m Original: What is Neural Network?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Confidence: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;32mConfidence: \u001b[0m\u001b[1;32m1.00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: **Adding relevant synonyms or related concepts**: <span style=\"color: #008000; text-decoration-color: #008000\">\"Investigate the theoretical foundations</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of Neural Network Models, incorporating concepts from machine learning, cognitive science, and computational </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">neuroscience.\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m1\u001b[0m: Rewrite \u001b[1;36m3\u001b[0m: **Adding relevant synonyms or related concepts**: \u001b[32m\"Investigate the theoretical foundations\u001b[0m\n",
       "\u001b[32mof Neural Network Models, incorporating concepts from machine learning, cognitive science, and computational \u001b[0m\n",
       "\u001b[32mneuroscience.\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.53</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.53\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: **Breaking down complex queries into clearer formulations**: <span style=\"color: #008000; text-decoration-color: #008000\">\"Describe the fundamental </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">principles and mathematical frameworks that govern the behavior of neural networks, including activation functions </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and backpropagation.\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m2\u001b[0m: Rewrite \u001b[1;36m2\u001b[0m: **Breaking down complex queries into clearer formulations**: \u001b[32m\"Describe the fundamental \u001b[0m\n",
       "\u001b[32mprinciples and mathematical frameworks that govern the behavior of neural networks, including activation functions \u001b[0m\n",
       "\u001b[32mand backpropagation.\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.53</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.53\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: **Technical terminology for better vector matching**: <span style=\"color: #008000; text-decoration-color: #008000\">\"Explain the concepts and algorithms</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">underlying Artificial Neural Network Architectures in deep learning applications.\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m3\u001b[0m: Rewrite \u001b[1;36m1\u001b[0m: **Technical terminology for better vector matching**: \u001b[32m\"Explain the concepts and algorithms\u001b[0m\n",
       "\u001b[32munderlying Artificial Neural Network Architectures in deep learning applications.\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.51</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.51\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>: Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>: **Including key entities and relationships from the original query**: <span style=\"color: #008000; text-decoration-color: #008000\">\"Analyze the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">evolution of Artificial Neural Network Models, tracing the development of feedforward networks, backpropagation, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and deep learning architectures in computer vision and natural language processing.\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m4\u001b[0m: Rewrite \u001b[1;36m5\u001b[0m: **Including key entities and relationships from the original query**: \u001b[32m\"Analyze the \u001b[0m\n",
       "\u001b[32mevolution of Artificial Neural Network Models, tracing the development of feedforward networks, backpropagation, \u001b[0m\n",
       "\u001b[32mand deep learning architectures in computer vision and natural language processing.\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.49</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.49\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>: Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>: **Varying syntax while preserving semantic meaning**: <span style=\"color: #008000; text-decoration-color: #008000\">\"What is the relationship between </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">artificial neural networks and adaptive systems? How do recurrent neural networks contribute to modeling dynamic </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">behavior?\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m5\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m5\u001b[0m: Rewrite \u001b[1;36m4\u001b[0m: **Varying syntax while preserving semantic meaning**: \u001b[32m\"What is the relationship between \u001b[0m\n",
       "\u001b[32martificial neural networks and adaptive systems? How do recurrent neural networks contribute to modeling dynamic \u001b[0m\n",
       "\u001b[32mbehavior?\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.41</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.41\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Select queries to use:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mSelect queries to use:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Enter the numbers of the queries you want to use <span style=\"font-weight: bold\">(</span>comma-separated, e.g., <span style=\"color: #008000; text-decoration-color: #008000\">'0,2,3'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Enter the numbers of the queries you want to use \u001b[1m(\u001b[0mcomma-separated, e.g., \u001b[32m'0,2,3'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">  1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Selected </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> queries for retrieval.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mSelected \u001b[0m\u001b[1;32m1\u001b[0m\u001b[1;32m queries for retrieval.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Retrieving relevant documents...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mRetrieving relevant documents\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG123: only this is working\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Retrieved </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> documents</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mRetrieved \u001b[0m\u001b[1;32m5\u001b[0m\u001b[1;32m documents\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Top retrieved documents:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mTop retrieved documents:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.4293</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m. \u001b[1;31mScore: \u001b[0m\u001b[1;31m0.4293\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Preview: term Deep Learning is describing the procedure of performing\n",
       "machine learning tasks with deep artiﬁcial neural networks\n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"font-weight: bold\">]</span>. In reality, the best per<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Preview: term Deep Learning is describing the procedure of performing\n",
       "machine learning tasks with deep artiﬁcial neural networks\n",
       "\u001b[1m[\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1m]\u001b[0m. In reality, the best per\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Source: \n",
       ".<span style=\"color: #800080; text-decoration-color: #800080\">/arxiv_data/deep_learning_fundamentals/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">1803.02129v1_A_Non-Technical_Survey_on_Deep_Convolutional_Neura.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Source: \n",
       ".\u001b[35m/arxiv_data/deep_learning_fundamentals/\u001b[0m\u001b[95m1803.02129v1_A_Non-Technical_Survey_on_Deep_Convolutional_Neura.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3909</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m. \u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3909\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Preview: Kriegeskorte &amp; Golan <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2019</span><span style=\"font-weight: bold\">)</span> Neural network models and deep learning \n",
       " \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> \n",
       " \n",
       "function, and to approximate many natural functions with fewer weights and<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Preview: Kriegeskorte & Golan \u001b[1m(\u001b[0m\u001b[1;36m2019\u001b[0m\u001b[1m)\u001b[0m Neural network models and deep learning \n",
       " \n",
       "\u001b[1;36m4\u001b[0m \n",
       " \n",
       "function, and to approximate many natural functions with fewer weights and\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Source: \n",
       ".<span style=\"color: #800080; text-decoration-color: #800080\">/arxiv_data/deep_learning_fundamentals/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">1902.04704v2_Neural_network_models_and_deep_learning_-_a_primer.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Source: \n",
       ".\u001b[35m/arxiv_data/deep_learning_fundamentals/\u001b[0m\u001b[95m1902.04704v2_Neural_network_models_and_deep_learning_-_a_primer.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3854</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m. \u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3854\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Preview: proposedamethodforlayer-wiseinitialtrainingofneuralnetworks,whichleveraged\n",
       "some of the challenges in training networks with several layers. However, t<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Preview: proposedamethodforlayer-wiseinitialtrainingofneuralnetworks,whichleveraged\n",
       "some of the challenges in training networks with several layers. However, t\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Source: .<span style=\"color: #800080; text-decoration-color: #800080\">/arxiv_data/deep_learning_fundamentals/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">2003.03253v1_Introduction_to_deep_learning.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Source: .\u001b[35m/arxiv_data/deep_learning_fundamentals/\u001b[0m\u001b[95m2003.03253v1_Introduction_to_deep_learning.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3801</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4\u001b[0m. \u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3801\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Preview: improved techniques for pretraining, initialization, regularization, and normalization, along \n",
       "with the introduction of rectified linear units, have a<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Preview: improved techniques for pretraining, initialization, regularization, and normalization, along \n",
       "with the introduction of rectified linear units, have a\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Source: \n",
       ".<span style=\"color: #800080; text-decoration-color: #800080\">/arxiv_data/deep_learning_fundamentals/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">1902.04704v2_Neural_network_models_and_deep_learning_-_a_primer.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Source: \n",
       ".\u001b[35m/arxiv_data/deep_learning_fundamentals/\u001b[0m\u001b[95m1902.04704v2_Neural_network_models_and_deep_learning_-_a_primer.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3778</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m5\u001b[0m. \u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3778\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Preview: Figure <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>: Convolutional layers only face a limited preceptive ﬁeld and all neurons share the\n",
       "same weights <span style=\"font-weight: bold\">(</span>cf. left side of the ﬁgure; adopted from <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Preview: Figure \u001b[1;36m6\u001b[0m: Convolutional layers only face a limited preceptive ﬁeld and all neurons share the\n",
       "same weights \u001b[1m(\u001b[0mcf. left side of the ﬁgure; adopted from \u001b[1m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Source: \n",
       ".<span style=\"color: #800080; text-decoration-color: #800080\">/arxiv_data/deep_learning_fundamentals/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">1810.05401v2_A_Gentle_Introduction_to_Deep_Learning_in_Medical_.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Source: \n",
       ".\u001b[35m/arxiv_data/deep_learning_fundamentals/\u001b[0m\u001b[95m1810.05401v2_A_Gentle_Introduction_to_Deep_Learning_in_Medical_.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Generating answer with document context...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mGenerating answer with document context\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 14:21:40,028 - SageRAG - INFO - Document relevance: Sufficient=False, Confidence=0.27\n",
      "2025-06-25 14:21:40,031 - SageRAG - INFO - Explanation: While the documents mention Neural Network and Deep Learning, they do not specifically define what a Neural Network is. They provide general information about deep learning and its applications, but lack concrete explanations or definitions.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Retrieved documents insufficient. Using LLM fallback...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mRetrieved documents insufficient. Using LLM fallback\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">╭───────────────────────────────────────────── Answer ─────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> A Neural Network is a fundamental concept in machine learning and artificial intelligence. In    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> simple terms, a neural network is a computer system inspired by the structure and function of    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> the human brain.                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> The term \"neural\" refers to the brain's neurons, which are specialized cells that process and    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> transmit information. A neural network consists of layers of interconnected nodes or \"neurons,\"  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> each of which receives one or more inputs, performs a computation, and then sends the output to  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> other neurons. This process is called forward propagation.                                       <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> The key characteristics of a neural network are:                                                 <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span><span style=\"font-weight: bold\">Distributed representation</span>: Neural networks represent data as a collection of interconnected  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>nodes, rather than using a single formula or rule.                                            <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span><span style=\"font-weight: bold\">Parallel processing</span>: Neural networks can handle multiple inputs simultaneously and process    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>them in parallel, making them more efficient than traditional computers.                      <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 3 </span><span style=\"font-weight: bold\">Learning</span>: Neural networks can learn from experience by adjusting the connections between      <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>nodes based on the input data.                                                                <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> There are several types of neural networks, including:                                           <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span><span style=\"font-weight: bold\">Feedforward Networks</span>: The simplest type, where information flows only in one direction from   <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>input to output.                                                                              <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span><span style=\"font-weight: bold\">Recurrent Neural Networks (RNNs)</span>: Used for tasks like natural language processing and speech  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>recognition, RNNs have feedback connections that allow information to flow in a loop.         <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 3 </span><span style=\"font-weight: bold\">Convolutional Neural Networks (CNNs)</span>: Designed specifically for image and video analysis,     <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>CNNs use convolutional and pooling layers to extract features.                                <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> Neural networks are widely used in applications such as:                                         <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span>Image classification and object detection                                                     <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span>Natural language processing and sentiment analysis                                            <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 3 </span>Speech recognition and generation                                                             <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 4 </span>Predictive modeling and regression                                                            <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> While I can provide general information on neural networks, please note that my responses may    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> not be based on specific research papers or citations. If you're looking for in-depth knowledge  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> on a particular topic, I recommend consulting academic sources or scientific literature.         <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m╭─\u001b[0m\u001b[32m────────────────────────────────────────────\u001b[0m\u001b[32m Answer \u001b[0m\u001b[32m────────────────────────────────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
       "\u001b[32m│\u001b[0m A Neural Network is a fundamental concept in machine learning and artificial intelligence. In    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m simple terms, a neural network is a computer system inspired by the structure and function of    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m the human brain.                                                                                 \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m The term \"neural\" refers to the brain's neurons, which are specialized cells that process and    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m transmit information. A neural network consists of layers of interconnected nodes or \"neurons,\"  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m each of which receives one or more inputs, performs a computation, and then sends the output to  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m other neurons. This process is called forward propagation.                                       \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m The key characteristics of a neural network are:                                                 \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m 1 \u001b[0m\u001b[1mDistributed representation\u001b[0m: Neural networks represent data as a collection of interconnected  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m   \u001b[0mnodes, rather than using a single formula or rule.                                            \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m 2 \u001b[0m\u001b[1mParallel processing\u001b[0m: Neural networks can handle multiple inputs simultaneously and process    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m   \u001b[0mthem in parallel, making them more efficient than traditional computers.                      \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m 3 \u001b[0m\u001b[1mLearning\u001b[0m: Neural networks can learn from experience by adjusting the connections between      \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m   \u001b[0mnodes based on the input data.                                                                \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m There are several types of neural networks, including:                                           \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m 1 \u001b[0m\u001b[1mFeedforward Networks\u001b[0m: The simplest type, where information flows only in one direction from   \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m   \u001b[0minput to output.                                                                              \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m 2 \u001b[0m\u001b[1mRecurrent Neural Networks (RNNs)\u001b[0m: Used for tasks like natural language processing and speech  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m   \u001b[0mrecognition, RNNs have feedback connections that allow information to flow in a loop.         \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m 3 \u001b[0m\u001b[1mConvolutional Neural Networks (CNNs)\u001b[0m: Designed specifically for image and video analysis,     \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m   \u001b[0mCNNs use convolutional and pooling layers to extract features.                                \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m Neural networks are widely used in applications such as:                                         \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m 1 \u001b[0mImage classification and object detection                                                     \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m 2 \u001b[0mNatural language processing and sentiment analysis                                            \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m 3 \u001b[0mSpeech recognition and generation                                                             \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[1;33m 4 \u001b[0mPredictive modeling and regression                                                            \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m While I can provide general information on neural networks, please note that my responses may    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m not be based on specific research papers or citations. If you're looking for in-depth knowledge  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m on a particular topic, I recommend consulting academic sources or scientific literature.         \u001b[32m│\u001b[0m\n",
       "\u001b[32m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Would you like to get an llm generated answer?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mWould you like to get an llm generated answer?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(y/n):  y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">╭───────────────────────────────────────────── Answer ─────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> I think there's been a repeat question! You've already asked about what a neural network is, and <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> I provided a detailed explanation. However, if you'd like a brief summary, here it is:           <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> A neural network is a computer system inspired by the structure and function of the human brain. <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> It consists of layers of interconnected nodes or \"neurons\" that process and transmit information <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> through forward propagation. Neural networks are characterized by their distributed              <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> representation, parallel processing capabilities, and ability to learn from experience.          <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> If you have any further questions about neural networks or would like more information on        <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> specific types (e.g., feedforward, recurrent, convolutional), feel free to ask!                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m╭─\u001b[0m\u001b[32m────────────────────────────────────────────\u001b[0m\u001b[32m Answer \u001b[0m\u001b[32m────────────────────────────────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
       "\u001b[32m│\u001b[0m I think there's been a repeat question! You've already asked about what a neural network is, and \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m I provided a detailed explanation. However, if you'd like a brief summary, here it is:           \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m A neural network is a computer system inspired by the structure and function of the human brain. \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m It consists of layers of interconnected nodes or \"neurons\" that process and transmit information \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m through forward propagation. Neural networks are characterized by their distributed              \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m representation, parallel processing capabilities, and ability to learn from experience.          \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m If you have any further questions about neural networks or would like more information on        \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m specific types (e.g., feedforward, recurrent, convolutional), feel free to ask!                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Options:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mOptions:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Ask a question\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m. Ask a question\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. View conversation history\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m. View conversation history\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Clear conversation history\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m. Clear conversation history\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. Go for Advanced Research\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4\u001b[0m. Go for Advanced Research\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. Exit\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m5\u001b[0m. Exit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your choice (1-5):  5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Thank you for using SageRAG Research Assistant. Goodbye!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mThank you for using SageRAG Research Assistant. Goodbye!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import os\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, field\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, BaseOutputParser\n",
    "from langchain.schema import Document, HumanMessage, AIMessage\n",
    "from langchain.vectorstores import VectorStore\n",
    "from langchain.chains import LLMChain\n",
    "# from langchain.llms.base import BaseLLM\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from pydantic import BaseModel, Field\n",
    "import logging\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "# Suppress library-specific logging\n",
    "import logging\n",
    "logging.getLogger('sentence_transformers').setLevel(logging.WARNING)\n",
    "logging.getLogger('transformers').setLevel(logging.WARNING)\n",
    "logging.getLogger('chromadb').setLevel(logging.WARNING)\n",
    "logging.getLogger('LangChainDeprecationWarning').setLevel(logging.WARNING)\n",
    "logging.getLogger('httpx').setLevel(logging.WARNING)\n",
    "\n",
    "# Then set up your own logging as before\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"SageRAG\")\n",
    "\n",
    "# Rich console for prettier output\n",
    "console = Console()\n",
    "\n",
    "# Enhanced output parsers with better error handling\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Parse output that contains a numbered list and return as a list of strings.\"\"\"\n",
    "    \n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        \"\"\"Parse text into a list of strings.\"\"\"\n",
    "        # Updated regex to be more robust with various numbering styles\n",
    "        lines = re.findall(r\"^\\s*\\d+\\.?\\s+(.*?)$\", text, re.MULTILINE)\n",
    "        return [line.strip() for line in lines if line.strip()]\n",
    "    \n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"line_list\"\n",
    "\n",
    "class QueryRating(BaseModel):\n",
    "    \"\"\"Schema for query rating output.\"\"\"\n",
    "    rating: int = Field(description=\"Rating from 1-5\")\n",
    "    explanation: str = Field(description=\"Explanation for the rating\")\n",
    "\n",
    "@dataclass\n",
    "class RetrievedDocument:\n",
    "    \"\"\"Dataclass for tracking retrieved document info.\"\"\"\n",
    "    document: Document\n",
    "    score: float\n",
    "    query: str = \"\"\n",
    "    rank: int = 0\n",
    "\n",
    "class ResearchAssistant:\n",
    "    \"\"\"AI Research Assistant using RAG for scientific paper queries.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        llm: OllamaLLM, \n",
    "        vector_db: VectorStore, \n",
    "        embeddings_model: Optional[SentenceTransformer] = None,\n",
    "        relevance_threshold: float = 0.2,\n",
    "        max_docs_per_query: int = 4\n",
    "    ):\n",
    "        \"\"\"Initialize the Research Assistant with LLM and vector database.\n",
    "        \n",
    "        Args:\n",
    "            llm: The language model to use for generation\n",
    "            vector_db: Vector database for document retrieval\n",
    "            embeddings_model: Optional SentenceTransformer model for semantic similarity\n",
    "            relevance_threshold: Minimum relevance score for documents\n",
    "            max_docs_per_query: Maximum documents to use per query\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.vector_db = vector_db\n",
    "        self.relevance_threshold = relevance_threshold\n",
    "        self.max_docs_per_query = max_docs_per_query\n",
    "        \n",
    "        # Initialize sentence transformer model for semantic similarity if not provided\n",
    "        if embeddings_model is None:\n",
    "            logger.info(\"Initializing default embedding model\")\n",
    "            self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        else:\n",
    "            self.semantic_model = embeddings_model\n",
    "            \n",
    "        # Initialize memory\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        # Create a retriever for direct use\n",
    "        self.retriever = vector_db.as_retriever()\n",
    "        \n",
    "        # Define parsers\n",
    "        self.json_parser = JsonOutputParser(pydantic_object=QueryRating)\n",
    "        self.list_parser = LineListOutputParser()\n",
    "        \n",
    "        logger.info(\"Research Assistant initialized successfully\")\n",
    "    \n",
    "    def rate_query(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Rates the query and gives an explanation.\n",
    "        \n",
    "        Args:\n",
    "            query: The user's query to be evaluated\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing rating and explanation\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an intelligent assistant trained to evaluate search queries for a scientific research database.\n",
    "        Given the following query: \"{query}\"\n",
    "        \n",
    "        1. Rate the query on a scale of 1 (very poor) to 5 (excellent) based on:\n",
    "           - Clarity: Is the query clear and unambiguous?\n",
    "           - Specificity: Does it contain specific technical terms or concepts?\n",
    "           - Relevance: Is it focused on retrieving scientific content?\n",
    "           - Retrievability: Will it work well with vector search?\n",
    "        \n",
    "        2. Provide a short explanation for the rating (what makes it effective or ineffective).\n",
    "        \n",
    "        Respond in JSON format:\n",
    "        {{\n",
    "          \"rating\": <number between 1-5>,\n",
    "          \"explanation\": \"<your explanation>\"\n",
    "        }}\n",
    "        \"\"\")\n",
    "        \n",
    "        chain: Runnable = prompt | self.llm | self.json_parser\n",
    "        \n",
    "        try:\n",
    "            return chain.invoke({\"query\": query})\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error rating query: {e}\")\n",
    "            # Fallback response if parsing fails\n",
    "            return {\n",
    "                \"rating\": 3, \n",
    "                \"explanation\": \"Unable to rate query properly. Consider adding more specific terms.\"\n",
    "            }\n",
    "\n",
    "    \n",
    "    def suggest_rewrites(self, query: str, chat_history: Optional[List] = None) -> List[str]:\n",
    "        \"\"\"Returns rephrased versions of the query optimized for retrieval.\n",
    "        \n",
    "        Args:\n",
    "            query: Original query to rewrite\n",
    "            chat_history: Optional conversation history for context\n",
    "            \n",
    "        Returns:\n",
    "            List of rewritten queries\n",
    "        \"\"\"\n",
    "        history_context = \"\"\n",
    "        if chat_history:\n",
    "            history_context = \"Consider this conversation context when rewriting the query:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use only last 3 messages for brevity\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    history_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"history_context\"],\n",
    "            template=\"\"\"You are an AI assistant specializing in scientific research queries.\n",
    "        \n",
    "        {history_context}\n",
    "        \n",
    "        TASK: Create 5 alternative versions of the user's question that will help improve retrieval from a scientific paper database.\n",
    "        \n",
    "        Original question: {question}\n",
    "        \n",
    "        For each alternative version:\n",
    "        - Write a COMPLETE, EXECUTABLE query (not just a description)\n",
    "        - Make each version meaningfully different while preserving the core information need\n",
    "        - Include the ACTUAL REWRITTEN QUESTION text\n",
    "        \n",
    "        FORMAT YOUR RESPONSE EXACTLY LIKE THIS EXAMPLE:\n",
    "    \n",
    "        1. Rewrite 1: **Technical terminology for better vector matching**: \"What are the impacts of increased ocean acidification and sea surface temperature anomalies on scleractinian coral communities and associated reef ecosystems?\"\n",
    "           Confidence: 0.70\n",
    "        2. Rewrite 2: **Breaking down complex queries into clearer formulations**: \"How do rising ocean temperatures affect coral health? What role does ocean acidification play in coral bleaching events? What are the ecosystem-level consequences of coral reef degradation due to climate change?\"\n",
    "           Confidence: 0.70\n",
    "        3. Rewrite 3: **Adding relevant synonyms or related concepts**: \"Examine the effects of global warming, greenhouse gas emissions, and anthropogenic climate forcing on coral reef ecosystems, coral bleaching, calcification rates, and reef biodiversity.\"\n",
    "           Confidence: 0.70\n",
    "        4. Rewrite 4: **Varying syntax while preserving semantic meaning**: \"In what ways are coral reef systems being modified by climate change variables? How are anthozoans responding to altered oceanic conditions resulting from global climate shifts?\"\n",
    "           Confidence: 0.70\n",
    "        5. Rewrite 5: **Including key entities and relationships from the original query**: \"Analyze the causal relationship between climate change parameters (CO2 levels, temperature increase, ocean pH) and coral reef ecosystem health indicators (species diversity, coral cover percentage, bleaching frequency).\"\n",
    "           Confidence: 0.70\n",
    "        \n",
    "        Now, for the question \"{question}\", provide 5 complete, executable rewritten queries following the exact format above. Each rewrite must include the full rewritten question text, not just a description of how to rewrite it.\n",
    "        \"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            result = (prompt | self.llm | self.list_parser).invoke({\n",
    "                \"question\": query,\n",
    "                \"history_context\": history_context\n",
    "            })\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error suggesting rewrites: {e}\")\n",
    "            # Return a minimal set of rewrites if parsing fails\n",
    "            return [\n",
    "                query,  # Original query\n",
    "                f\"research about {query}\",\n",
    "                f\"papers discussing {query}\"\n",
    "            ]\n",
    "    def compute_confidence(self, original: str, rewritten: str) -> float:\n",
    "        \"\"\"Computes semantic similarity between original and rewritten queries.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            rewritten: Rewritten version\n",
    "            \n",
    "        Returns:\n",
    "            Similarity score between 0-1\n",
    "        \"\"\"\n",
    "        try:\n",
    "            vec_orig = self.semantic_model.encode(original, convert_to_tensor=True)\n",
    "            vec_rewrite = self.semantic_model.encode(rewritten, convert_to_tensor=True)\n",
    "            return float(util.pytorch_cos_sim(vec_orig, vec_rewrite).item())\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error computing similarity: {e}\")\n",
    "            return 0.7  # Default reasonable value\n",
    "    \n",
    "    def score_queries(self, original: str, queries: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Scores and sorts rephrased queries by confidence.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            queries: List of rewritten queries\n",
    "            \n",
    "        Returns:\n",
    "            List of (query, score) tuples sorted by score\n",
    "        \"\"\"\n",
    "        scored = [(q, self.compute_confidence(original, q)) for q in queries]\n",
    "        return sorted(scored, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    def present_query_options(self, original: str, queries: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Presents the original and rewritten queries with confidence scores.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            queries: List of rewritten queries\n",
    "            \n",
    "        Returns:\n",
    "            List of (query, score) tuples including original query\n",
    "        \"\"\"\n",
    "        # Add original query at the top\n",
    "        scored_queries = [(\"Original: \" + original, 1.0)]\n",
    "        \n",
    "        # Add scored rewritten queries\n",
    "        rewritten_scores = self.score_queries(original, queries)\n",
    "        for i, (query, score) in enumerate(rewritten_scores, 1):\n",
    "            if \"Query:\" in query:\n",
    "                parts = query.split(\"Query:\")\n",
    "                # Format with Query on a new line and bold\n",
    "                formatted_query = f\"{parts[0]}\\n[bold]Query:[/bold]{parts[1]}\"\n",
    "                scored_queries.append((f\"Rewrite {i}: {formatted_query}\", score))\n",
    "            else:\n",
    "                scored_queries.append((f\"Rewrite {i}: {query}\", score))\n",
    "            \n",
    "        # Display options in a nice format\n",
    "        console.print(\"\\n[bold cyan]Available search queries:[/bold cyan]\")\n",
    "        for i, (query, score) in enumerate(scored_queries):\n",
    "            confidence_color = \"green\" if score > 0.8 else \"yellow\" if score > 0.6 else \"red\"\n",
    "            console.print(f\"[bold]{i}.[/bold] {query}\")\n",
    "            console.print(f\"   [bold {confidence_color}]Confidence: {score:.2f}[/bold {confidence_color}]\")\n",
    "            \n",
    "        return scored_queries\n",
    "    \n",
    "    \n",
    "    def retrieve_documents(\n",
    "        self, \n",
    "        queries: List[str], \n",
    "        k: int = 5\n",
    "    ) -> Tuple[List[RetrievedDocument], Dict[str, List[RetrievedDocument]]]:\n",
    "        \"\"\"Retrieves documents using multiple queries, preserving query information.\n",
    "        \n",
    "        Args:\n",
    "            queries: List of queries to retrieve documents for\n",
    "            k: Number of documents to retrieve per query\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (all unique documents, query->documents mapping)\n",
    "        \"\"\"\n",
    "        all_docs: List[RetrievedDocument] = []\n",
    "        unique_content = set()\n",
    "        query_docs_map: Dict[str, List[RetrievedDocument]] = {}\n",
    "        \n",
    "        # Retrieve docs for each query\n",
    "        for query in queries:\n",
    "            console.print(f\"\\n[bold blue]Query:[/bold blue] '{query}'\")\n",
    "            \n",
    "            try:\n",
    "                results = self.vector_db.similarity_search_with_relevance_scores(query, k=k)\n",
    "                print(\"LOG123: this is results length\",len(results))\n",
    "                docs_for_query: List[RetrievedDocument] = []\n",
    "                \n",
    "                for i, (doc, score) in enumerate(results, 1):\n",
    "                    # Create retrieved document object\n",
    "                    retrieved_doc = RetrievedDocument(\n",
    "                        document=doc,\n",
    "                        score=score,\n",
    "                        query=query,\n",
    "                        rank=i\n",
    "                    )\n",
    "                    \n",
    "                    # Display result info\n",
    "                    score_color = \"green\" if score > 0.8 else \"yellow\" if score > 0.6 else \"red\"\n",
    "                    console.print(f\"[bold]--- Result {i} ---[/bold]\")\n",
    "                    console.print(f\"[bold {score_color}]Score: {score:.4f}[/bold {score_color}]\")\n",
    "                    \n",
    "                    # Show document preview\n",
    "                    preview = doc.page_content[:150] + \"...\" if len(doc.page_content) > 150 else doc.page_content\n",
    "                    console.print(Panel(preview, title=\"Content Preview\", width=100))\n",
    "                    \n",
    "                    # Show metadata if available\n",
    "                    if hasattr(doc, 'metadata') and doc.metadata:\n",
    "                        source = doc.metadata.get('source', 'Unknown')\n",
    "                        console.print(f\"[dim]Source: {source}[/dim]\")\n",
    "                    \n",
    "                    # Add to results for this query\n",
    "                    docs_for_query.append(retrieved_doc)\n",
    "                    \n",
    "                    # Only add unique documents to overall collection\n",
    "                    if doc.page_content not in unique_content:\n",
    "                        unique_content.add(doc.page_content)\n",
    "                        all_docs.append(retrieved_doc)\n",
    "                query_docs_map[query] = docs_for_query\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error retrieving documents for query '{query}': {e}\")\n",
    "                console.print(f\"[bold red]Error retrieving documents for query: {query}[/bold red]\")\n",
    "        \n",
    "        console.print(f\"\\n[bold green]Total unique documents:[/bold green] {len(all_docs)}\")\n",
    "        return all_docs, query_docs_map\n",
    "\n",
    "    def rerank_docs(self, query: str, docs: List[RetrievedDocument]) -> List[RetrievedDocument]:\n",
    "        \"\"\"Reranks documents based on semantic similarity to the query.\n",
    "        \n",
    "        Args:\n",
    "            query: Query to compare documents against\n",
    "            docs: List of retrieved documents\n",
    "            \n",
    "        Returns:\n",
    "            Reranked list of documents with updated scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            query_embedding = self.semantic_model.encode(query, convert_to_tensor=True)\n",
    "            reranked = []\n",
    "            \n",
    "            for doc in docs:\n",
    "                doc_embedding = self.semantic_model.encode(doc.document.page_content, convert_to_tensor=True)\n",
    "                new_score = float(util.pytorch_cos_sim(query_embedding, doc_embedding).item())\n",
    "                \n",
    "                # Create new RetrievedDocument with updated score\n",
    "                reranked_doc = RetrievedDocument(\n",
    "                    document=doc.document,\n",
    "                    score=new_score,\n",
    "                    query=doc.query,\n",
    "                    rank=0  # Will be updated after sorting\n",
    "                )\n",
    "                reranked.append(reranked_doc)\n",
    "            \n",
    "            # Sort by score and update ranks\n",
    "            reranked.sort(key=lambda x: x.score, reverse=True)\n",
    "            for i, doc in enumerate(reranked, 1):\n",
    "                doc.rank = i\n",
    "                \n",
    "            return reranked\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reranking documents: {e}\")\n",
    "            return docs  # Return original documents if reranking fails\n",
    "    \n",
    "    def can_answer_without_retrieval(self, question: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"Determines if a question can be answered directly from memory without retrieval.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (can_answer, answer_if_available)\n",
    "        \"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        if not chat_history:\n",
    "            return False, None\n",
    "            \n",
    "        # Create the prompt to check if we can answer directly from the conversation\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an AI assistant helping with a scientific research conversation.\n",
    "        Given the following conversation history and a new question, determine if the question:\n",
    "        1. Can be answered directly based ONLY on the conversation history (like \"what was my previous question?\")\n",
    "        2. Does NOT require retrieving new information from scientific papers\n",
    "        \n",
    "        If both conditions are true, provide the answer. Otherwise, respond with \"NEEDS_RETRIEVAL\".\n",
    "        \n",
    "        Conversation History:\n",
    "        {chat_history}\n",
    "        \n",
    "        New Question: {question}\n",
    "        \n",
    "        Your assessment (answer directly or respond with \"NEEDS_RETRIEVAL\"):\n",
    "        \"\"\")\n",
    "        \n",
    "        # Format the chat history for context\n",
    "        history_str = \"\"\n",
    "        for message in chat_history[-5:]:  # Use only last 5 messages for brevity\n",
    "            if hasattr(message, \"content\"):\n",
    "                role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                history_str += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        try:\n",
    "            # Ask the LLM if this can be answered without retrieval\n",
    "            response = self.llm.invoke(\n",
    "                prompt.format(chat_history=history_str, question=question)\n",
    "            )\n",
    "            \n",
    "            # If the response is \"NEEDS_RETRIEVAL\", we need to use retrieval\n",
    "            if \"NEEDS_RETRIEVAL\" in response:\n",
    "                return False, None\n",
    "            else:\n",
    "                logger.info(\"Question can be answered from conversation history\")\n",
    "                return True, response\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error determining if retrieval needed: {e}\")\n",
    "            return False, None  \n",
    "    \n",
    "    def generate_final_answer(\n",
    "        self, \n",
    "        question: str, \n",
    "        docs: List[RetrievedDocument], \n",
    "        max_docs: int = 5\n",
    "    ) -> str:\n",
    "        \"\"\"Generates the final answer from retrieved documents with LLM fallback.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            docs: List of retrieved documents\n",
    "            max_docs: Maximum number of documents to include\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # First, evaluate if the documents are sufficient\n",
    "        is_sufficient, confidence = self.evaluate_document_relevance(question, docs)\n",
    "        \n",
    "        # If documents are insufficient, use LLM fallback\n",
    "        if not is_sufficient or confidence < 0.4:  # Adjust threshold as needed\n",
    "            console.print(\"[yellow]Retrieved documents insufficient. Using LLM fallback...[/yellow]\")\n",
    "            return self.generate_llm_answer(question)\n",
    "        \n",
    "        # Otherwise, continue with RAG as before\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "                \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful scientific research assistant analyzing scientific papers. \n",
    "            Use the following retrieved context chunks to answer the user's question thoroughly.\n",
    "            \n",
    "            Guidelines:\n",
    "            - Focus on providing accurate information from the papers\n",
    "            - Synthesize information across documents when appropriate\n",
    "            - Cite the sources of information in your answer (e.g., \"According to Document 1...\")\n",
    "            - If the information isn't in the context, indicate what's missing rather than making up information\n",
    "            - If the context contains conflicting information, highlight the disagreement and possible reasons\n",
    "            - Maintain scientific accuracy above all else\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Format document content with metadata\n",
    "        context_pieces = []\n",
    "        for i, doc in enumerate(docs[:max_docs]):\n",
    "            chunk = f\"Document {i+1} [Relevance: {doc.score:.2f}]:\\n{doc.document.page_content}\"\n",
    "            \n",
    "            # Add metadata if available\n",
    "            if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                source = doc.document.metadata.get('source', 'Unknown')\n",
    "                chunk += f\"\\nSource: {source}\"\n",
    "                \n",
    "            context_pieces.append(chunk)\n",
    "            \n",
    "        context = \"\\n\\n---\\n\\n\".join(context_pieces)\n",
    "        \n",
    "        try:\n",
    "            # Generate answer\n",
    "            answer = (prompt | self.llm).invoke({\n",
    "                \"question\": question, \n",
    "                \"context\": context,\n",
    "                \"chat_history\": chat_context\n",
    "            })\n",
    "            \n",
    "            # Save the QA pair to memory\n",
    "            self.memory.chat_memory.add_messages([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=answer)\n",
    "            ])\n",
    "            \n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating answer: {e}\")\n",
    "            return \"I'm sorry, I encountered an error while generating your answer. Please try rephrasing your question.\"\n",
    "    \n",
    "    def generate_combined_answer(\n",
    "        self, \n",
    "        question: str, \n",
    "        query_results: Dict[str, List[RetrievedDocument]]\n",
    "    ) -> str:\n",
    "        \"\"\"Generates a combined answer from multiple query results with LLM fallback.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            query_results: Dictionary mapping queries to retrieved documents\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Combine all relevant documents from different queries\n",
    "        all_docs = []\n",
    "        for query, docs in query_results.items():\n",
    "            all_docs.extend(docs[:self.max_docs_per_query])\n",
    "        \n",
    "        # Evaluate if the documents are sufficient\n",
    "        is_sufficient, confidence = self.evaluate_document_relevance(question, all_docs)\n",
    "        \n",
    "        # If documents are insufficient, use LLM fallback\n",
    "        if not is_sufficient or confidence < 0.2:  # Adjust threshold as needed\n",
    "            console.print(\"[yellow]Retrieved documents insufficient. Using LLM fallback...[/yellow]\")\n",
    "            return self.generate_llm_answer(question)\n",
    "        \n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        # Combine all context sections\n",
    "        all_context_sections = []\n",
    "        \n",
    "        for query, docs in query_results.items():\n",
    "            # Use only the top N documents for each query\n",
    "            docs_for_query = docs[:self.max_docs_per_query]\n",
    "            if docs_for_query:\n",
    "                all_context_sections.append(f\"Results for query: '{query}'\")\n",
    "                for i, doc in enumerate(docs_for_query, 1):\n",
    "                    section = f\"Document {i} [Relevance: {doc.score:.2f}]:\\n{doc.document.page_content}\"\n",
    "                    \n",
    "                    # Add metadata if available\n",
    "                    if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                        source = doc.document.metadata.get('source', 'Unknown')\n",
    "                        section += f\"\\nSource: {source}\"\n",
    "                        \n",
    "                    all_context_sections.append(section)\n",
    "        \n",
    "        # Join all context sections\n",
    "        combined_context = \"\\n\\n---\\n\\n\".join(all_context_sections)\n",
    "        \n",
    "        # Create a prompt that emphasizes synthesizing information across different query results\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful scientific research assistant analyzing scientific papers. \n",
    "            The user's question has been reformulated in several ways, and each formulation returned different documents.\n",
    "            \n",
    "            Guidelines:\n",
    "            - Synthesize information across ALL retrieved documents to provide a comprehensive answer\n",
    "            - Compare and contrast findings from different sources\n",
    "            - Highlight the most relevant information from each source\n",
    "            - Cite the specific documents you're referencing (e.g., \"According to the paper in Document 3...\")\n",
    "            - If information conflicts across sources, explain the different perspectives\n",
    "            - If the information isn't in the context, indicate what's missing\n",
    "            - Maintain scientific accuracy above all else\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Context from multiple query formulations:\n",
    "            {context}\n",
    "            \n",
    "            Original Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Generate answer\n",
    "            answer = (prompt | self.llm).invoke({\n",
    "                \"question\": question, \n",
    "                \"context\": combined_context,\n",
    "                \"chat_history\": chat_context\n",
    "            })\n",
    "            \n",
    "            # Save the QA pair to memory\n",
    "            self.memory.chat_memory.add_messages([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=answer)\n",
    "            ])\n",
    "            \n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating combined answer: {e}\")\n",
    "            return \"I'm sorry, I encountered an error while generating your answer. Please try rephrasing your question.\"\n",
    "            \n",
    "    def evaluate_document_relevance(self, question: str, docs: List[RetrievedDocument]) -> Tuple[bool, float]:\n",
    "        \"\"\"Evaluates whether documents are sufficient to answer the question.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            docs: List of retrieved documents\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (is_sufficient, confidence_score)\n",
    "        \"\"\"\n",
    "        if not docs:\n",
    "            return False, 0.0\n",
    "            \n",
    "        # If all docs have very low scores, they're likely not relevant\n",
    "        avg_score = sum(doc.score for doc in docs) / len(docs)\n",
    "        \n",
    "        # Create a prompt to evaluate document relevance\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are evaluating whether a set of retrieved documents contains information \n",
    "        to answer a user's question.\n",
    "        \n",
    "        User Question: {question}\n",
    "        \n",
    "        Retrieved Information:\n",
    "        {doc_summaries}\n",
    "        \n",
    "        Please analyze whether the retrieved documents contain enough information to answer the question.\n",
    "        Respond with a JSON object:\n",
    "        {{\n",
    "          \"is_sufficient\": true/false,\n",
    "          \"confidence\": <number between 0-1>,\n",
    "          \"explanation\": \"<brief explanation>\"\n",
    "        }}\n",
    "        \n",
    "        Only respond with the JSON object.\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create short summaries of each document\n",
    "        doc_summaries = []\n",
    "        for i, doc in enumerate(docs[:3]):  # Use top 3 docs for evaluation\n",
    "            summary = doc.document.page_content[:200] + \"...\" if len(doc.document.page_content) > 200 else doc.document.page_content\n",
    "            doc_summaries.append(f\"Document {i+1} [Score: {doc.score:.2f}]: {summary}\")\n",
    "        \n",
    "        doc_summaries_text = \"\\n\\n\".join(doc_summaries)\n",
    "        \n",
    "        try:\n",
    "            # Parse the LLM response as JSON\n",
    "            response = self.llm.invoke(\n",
    "                prompt.format(question=question, doc_summaries=doc_summaries_text)\n",
    "            )\n",
    "            \n",
    "            # Extract JSON from the response\n",
    "            json_match = re.search(r'\\{.*\\}', response.replace('\\n', ' '), re.DOTALL)\n",
    "            if json_match:\n",
    "                result = json.loads(json_match.group(0))\n",
    "                is_sufficient = result.get(\"is_sufficient\", False)\n",
    "                confidence = result.get(\"confidence\", 0.0)\n",
    "                \n",
    "                logger.info(f\"Document relevance: Sufficient={is_sufficient}, Confidence={confidence}\")\n",
    "                logger.info(f\"Explanation: {result.get('explanation', 'No explanation provided')}\")\n",
    "                \n",
    "                return is_sufficient, confidence\n",
    "            else:\n",
    "                # If JSON parsing fails, use heuristics based on average score\n",
    "                return avg_score > self.relevance_threshold, avg_score\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating document relevance: {e}\")\n",
    "            # Fall back to using avg score\n",
    "            return avg_score > self.relevance_threshold, avg_score\n",
    "\n",
    "    def generate_llm_answer(self, question: str) -> str:\n",
    "        \"\"\"Generates an answer directly from the LLM when documents aren't sufficient.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful scientific research assistant. \n",
    "            \n",
    "            The user has asked a question, but we couldn't find relevant documents in our scientific database.\n",
    "            Since this appears to be a question you can answer from your general knowledge, please provide\n",
    "            a helpful response.\n",
    "            \n",
    "            Guidelines:\n",
    "            - Be clear that you're answering from general knowledge rather than specific papers\n",
    "            - Provide accurate, factual information\n",
    "            - If the question is about a very specialized scientific topic that requires references to papers,\n",
    "              indicate that you lack specific citations but can provide general information\n",
    "            - If it's a basic concept, provide a thorough explanation\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Generate answer\n",
    "            answer = (prompt | self.llm).invoke({\n",
    "                \"question\": question,\n",
    "                \"chat_history\": chat_context\n",
    "            })\n",
    "            \n",
    "            # Save the QA pair to memory\n",
    "            self.memory.chat_memory.add_messages([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=answer)\n",
    "            ])\n",
    "            \n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating LLM answer: {e}\")\n",
    "            return \"I'm sorry, I couldn't find relevant information in my scientific database and encountered an error trying to provide a general answer. Please try rephrasing your question.\"\n",
    "    \n",
    "    def ask_with_memory(self, question: str, num_results: int = 5) -> str:\n",
    "        \"\"\"Uses conversation memory to process follow-up questions.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            num_results: Number of results to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # First, check if this is a question we can answer directly from memory\n",
    "            can_direct_answer, direct_answer = self.can_answer_without_retrieval(question)\n",
    "            if can_direct_answer:\n",
    "                console.print(\"[bold green]Question can be answered directly from conversation history...[/bold green]\")\n",
    "                # Save the QA pair to memory manually\n",
    "                self.memory.chat_memory.add_messages([\n",
    "                    HumanMessage(content=question),\n",
    "                    AIMessage(content=direct_answer)\n",
    "                ])\n",
    "                return direct_answer\n",
    "# Otherwise, use the full query improvement pipeline\n",
    "            \n",
    "            # Step 1: Rate the query\n",
    "            console.print(\"\\n[bold cyan]Evaluating your query (with memory context)...[/bold cyan]\")\n",
    "            rating_result = self.rate_query(question)\n",
    "            \n",
    "            rating_color = \"green\" if rating_result['rating'] >= 4 else \"yellow\" if rating_result['rating'] >= 3 else \"red\"\n",
    "            console.print(f\"[bold {rating_color}]Query Rating: {rating_result['rating']}/5[/bold {rating_color}]\")\n",
    "            console.print(f\"Explanation: {rating_result['explanation']}\")\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 2: Suggest rewrites if the rating is less than perfect\n",
    "            rewritten_queries = []\n",
    "            if rating_result[\"rating\"] < 5:\n",
    "                console.print(\"[bold cyan]Generating improved query variations...[/bold cyan]\")\n",
    "                # Get chat history for context in rewrites\n",
    "                chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "                rewritten_queries = self.suggest_rewrites(question, chat_history)\n",
    "                console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 3: Present options to the user\n",
    "            query_options = self.present_query_options(question, rewritten_queries)\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 4: Get user selection\n",
    "            console.print(\"[bold cyan]Select queries to use:[/bold cyan]\")\n",
    "            console.print(\"Enter the numbers of the queries you want to use (comma-separated, e.g., '0,2,3')\")\n",
    "            selected_indices_input = input(\"> \")\n",
    "            \n",
    "            try:\n",
    "                selected_indices = [int(idx.strip()) for idx in selected_indices_input.split(\",\")]\n",
    "            except ValueError:\n",
    "                console.print(\"[bold red]Invalid input. Using original query only.[/bold red]\")\n",
    "                selected_indices = [0]  # Default to original query\n",
    "            \n",
    "            # Get the selected queries\n",
    "            selected_queries = []\n",
    "            for idx in selected_indices:\n",
    "                if idx == 0:  # Original query\n",
    "                    selected_queries.append(question)\n",
    "                elif 0 < idx <= len(rewritten_queries):  # Rewritten query\n",
    "                    query_text = rewritten_queries[idx-1]\n",
    "                    selected_queries.append(query_text)\n",
    "            \n",
    "            if not selected_queries:\n",
    "                console.print(\"[bold red]No valid queries selected. Using original query.[/bold red]\")\n",
    "                selected_queries = [question]\n",
    "                \n",
    "            console.print(f\"\\n[bold green]Selected {len(selected_queries)} queries for retrieval.[/bold green]\")\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 5: Retrieve documents\n",
    "            console.print(\"[bold cyan]Retrieving relevant documents...[/bold cyan]\")\n",
    "            \n",
    "            if len(selected_queries) > 1:\n",
    "                # If multiple queries were selected, use the multi-query approach\n",
    "                docs, query_docs_map = self.retrieve_documents(selected_queries, k=num_results)\n",
    "                \n",
    "                # Generate a combined answer\n",
    "                console.print(\"\\n[bold cyan]Generating combined answer with conversation+docs context...[/bold cyan]\")\n",
    "                answer = self.generate_combined_answer(question, query_docs_map)\n",
    "            else:\n",
    "                # If only one query was selected, use the standard approach\n",
    "                try:\n",
    "                    retrieved_docs = []\n",
    "                    results = self.vector_db.similarity_search_with_relevance_scores(selected_queries[0], k=num_results)\n",
    "                    \n",
    "                    for i, (doc, score) in enumerate(results, 1):\n",
    "                        retrieved_docs.append(RetrievedDocument(\n",
    "                            document=doc,\n",
    "                            score=score,\n",
    "                            query=selected_queries[0],\n",
    "                            rank=i\n",
    "                        ))\n",
    "                    \n",
    "                    console.print(f\"[bold green]Retrieved {len(retrieved_docs)} documents[/bold green]\")\n",
    "                    \n",
    "                    # Show previews of the retrieved documents\n",
    "                    console.print(\"\\n[bold cyan]Top retrieved documents:[/bold cyan]\")\n",
    "                    for i, doc in enumerate(retrieved_docs[:5], 1):\n",
    "                        score_color = \"green\" if doc.score > 0.8 else \"yellow\" if doc.score > 0.6 else \"red\"\n",
    "                        console.print(f\"{i}. [bold {score_color}]Score: {doc.score:.4f}[/bold {score_color}]\")\n",
    "                        \n",
    "                        preview = doc.document.page_content[:150] + \"...\" if len(doc.document.page_content) > 150 else doc.document.page_content\n",
    "                        console.print(f\"   Preview: {preview}\")\n",
    "                        \n",
    "                        if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                            console.print(f\"   Source: {doc.document.metadata.get('source', 'Unknown')}\")\n",
    "                    \n",
    "                    # Generate answer with memory context\n",
    "                    console.print(\"\\n[bold cyan]Generating answer with conversation + doc context...[/bold cyan]\")\n",
    "                    answer = self.generate_final_answer(question, retrieved_docs)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in retrieval: {e}\")\n",
    "                    return f\"I'm sorry, I encountered an error while retrieving documents: {str(e)}\"\n",
    "            \n",
    "            return answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in ask_with_memory: {e}\")\n",
    "            return f\"I'm sorry, I encountered an error while processing your question: {str(e)}\"\n",
    "\n",
    "    def search_with_query_feedback(self, query: str, num_results: int = 5) -> str:\n",
    "        \"\"\"Main pipeline that processes a query and returns an answer.\"\"\"\n",
    "        # Step 1: Rate the query\n",
    "        console.print(\"\\n[bold cyan]Evaluating your query...[/bold cyan]\")\n",
    "        rating_result = self.rate_query(query)\n",
    "            \n",
    "        rating_color = \"green\" if rating_result['rating'] >= 4 else \"yellow\" if rating_result['rating'] >= 3 else \"red\"\n",
    "        console.print(f\"[bold {rating_color}]Query Rating: {rating_result['rating']}/5[/bold {rating_color}]\")\n",
    "        console.print(f\"Explanation: {rating_result['explanation']}\")\n",
    "        console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "        # Step 2: Suggest rewrites if the rating is less than perfect\n",
    "        rewritten_queries = []\n",
    "        if rating_result[\"rating\"] < 5:\n",
    "            console.print(\"[bold cyan]Generating improved query variations...[/bold cyan]\")\n",
    "            rewritten_queries = self.suggest_rewrites(query)\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 3: Present options to the user\n",
    "        query_options = self.present_query_options(query, rewritten_queries)\n",
    "        console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 4: Get user selection\n",
    "        console.print(\"[bold cyan]Select queries to use:[/bold cyan]\")\n",
    "        console.print(\"Enter the numbers of the queries you want to use (comma-separated, e.g., '0,2,3')\")\n",
    "        selected_indices_input = input(\"> \")\n",
    "            \n",
    "        try:\n",
    "            selected_indices = [int(idx.strip()) for idx in selected_indices_input.split(\",\")]\n",
    "        except ValueError:\n",
    "            console.print(\"[bold red]Invalid input. Using original query only.[/bold red]\")\n",
    "            selected_indices = [0]  # Default to original query\n",
    "        \n",
    "        # Get the selected queries\n",
    "        selected_queries = []\n",
    "        for idx in selected_indices:\n",
    "            if idx == 0:  # Original query\n",
    "                selected_queries.append(query)\n",
    "            elif 0 < idx <= len(rewritten_queries):  # Rewritten query\n",
    "                query_text = rewritten_queries[idx-1]\n",
    "                selected_queries.append(query_text)\n",
    "                \n",
    "        if not selected_queries:\n",
    "            console.print(\"[bold red]No valid queries selected. Using original query.[/bold red]\")\n",
    "            selected_queries = [query]\n",
    "            \n",
    "        console.print(f\"\\n[bold green]Selected {len(selected_queries)} queries for retrieval.[/bold green]\")\n",
    "        console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 5: Retrieve documents\n",
    "        console.print(\"[bold cyan]Retrieving relevant documents...[/bold cyan]\")\n",
    "        \n",
    "        if len(selected_queries) > 1:\n",
    "            # If multiple queries were selected, use the multi-query approach\n",
    "            print(\"LOG123: multiple queries this is working\")\n",
    "            docs, query_docs_map = self.retrieve_documents(selected_queries, k=num_results)\n",
    "            # Generate a combined answer\n",
    "            console.print(\"\\n[bold cyan]Generating combined answer with documents context...[/bold cyan]\")\n",
    "            answer = self.generate_combined_answer(query, query_docs_map)\n",
    "        else:\n",
    "            # If only one query was selected, use the standard approach\n",
    "            print(\"LOG123: only this is working\")\n",
    "            try:\n",
    "                retrieved_docs = []\n",
    "                results = self.vector_db.similarity_search_with_relevance_scores(selected_queries[0], k=num_results)\n",
    "                \n",
    "                for i, (doc, score) in enumerate(results, 1):\n",
    "                    retrieved_docs.append(RetrievedDocument(\n",
    "                        document=doc,\n",
    "                        score=score,\n",
    "                        query=selected_queries[0],\n",
    "                        rank=i\n",
    "                    ))\n",
    "                \n",
    "                console.print(f\"[bold green]Retrieved {len(retrieved_docs)} documents[/bold green]\")\n",
    "                \n",
    "                # Show previews of the retrieved documents\n",
    "                console.print(\"\\n[bold cyan]Top retrieved documents:[/bold cyan]\")\n",
    "                for i, doc in enumerate(retrieved_docs[:5], 1):\n",
    "                    score_color = \"green\" if doc.score > 0.8 else \"yellow\" if doc.score > 0.6 else \"red\"\n",
    "                    console.print(f\"{i}. [bold {score_color}]Score: {doc.score:.4f}[/bold {score_color}]\")\n",
    "                    \n",
    "                    preview = doc.document.page_content[:150] + \"...\" if len(doc.document.page_content) > 150 else doc.document.page_content\n",
    "                    console.print(f\"   Preview: {preview}\")\n",
    "                    \n",
    "                    if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                        console.print(f\"   Source: {doc.document.metadata.get('source', 'Unknown')}\")\n",
    "                \n",
    "                # Generate answer with memory context\n",
    "                console.print(\"\\n[bold cyan]Generating answer with document context...[/bold cyan]\")\n",
    "                answer = self.generate_final_answer(query, retrieved_docs)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in retrieval: {e}\")\n",
    "                return f\"I'm sorry, I encountered an error while retrieving documents: {str(e)}\"\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def process_query(self, query: str, use_memory: bool = False, num_results: int = 5) -> str:\n",
    "        \"\"\"Main entry point that decides whether to use memory or the full pipeline.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            use_memory: Whether to explicitly use memory mode\n",
    "            num_results: Number of results to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Check if this might be a follow-up question that should use memory\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        should_use_memory = use_memory and (chat_history and self._is_likely_followup(query))\n",
    "        print(\"should_use_memory\",should_use_memory)\n",
    "        if should_use_memory:\n",
    "            console.print(\"[bold cyan]Using conversation memory to process this query...[/bold cyan]\")\n",
    "            return self.ask_with_memory(query, num_results)\n",
    "        else:\n",
    "            console.print(\"[bold cyan]Using full query improvement pipeline...[/bold cyan]\")\n",
    "            return self.search_with_query_feedback(query, num_results)\n",
    "    \n",
    "    def _is_likely_followup(self, query: str) -> bool:\n",
    "        \"\"\"Heuristically determines if a query is likely a follow-up question.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating if this is likely a follow-up\n",
    "        \"\"\"\n",
    "        # Look for pronouns, references, and questions that seem to refer to previous context\n",
    "        followup_indicators = [\n",
    "            # Pronouns\n",
    "            \"it\", \"this\", \"that\", \"they\", \"them\", \"these\", \"those\",\n",
    "            # Reference terms\n",
    "            \"previous\", \"earlier\", \"above\", \"mentioned\", \"last\", \"former\",\n",
    "            # Follow-up phrases\n",
    "            \"what about\", \"how about\", \"tell me more\", \"elaborate\", \"clarify\", \"explain further\",\n",
    "            \"can you expand\", \"additionally\", \"furthermore\", \"also\", \"related to that\",\n",
    "            # Short questions that likely need context\n",
    "            \"why\", \"how does\", \"can you explain\", \"what does\", \"what is\", \"who is\"\n",
    "        ]\n",
    "        \n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Check for any of the indicators\n",
    "        if any(indicator in query_lower for indicator in followup_indicators):\n",
    "            return True\n",
    "            \n",
    "        # # Check for very short queries (likely follow-ups)\n",
    "        # if len(query.split()) < 4:\n",
    "        #     return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    def clear_memory(self) -> None:\n",
    "        \"\"\"Clears the conversation memory.\"\"\"\n",
    "        self.memory.clear()\n",
    "        console.print(\"[bold green]Conversation memory cleared.[/bold green]\")\n",
    "        \n",
    "    def get_chat_history(self) -> str:\n",
    "        \"\"\"Returns the formatted chat history.\n",
    "        \n",
    "        Returns:\n",
    "            String representation of chat history\n",
    "        \"\"\"\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        if not chat_history:\n",
    "            return \"No conversation history.\"\n",
    "            \n",
    "        history_str = \"\"\n",
    "        for i, message in enumerate(chat_history):\n",
    "            if hasattr(message, \"content\"):\n",
    "                role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                history_str += f\"{role}: {message.content}\\n\\n\"\n",
    "                \n",
    "        return history_str\n",
    "\n",
    "    def Load_query(self, query):\n",
    "        \"\"\"\n",
    "        Load research papers from arXiv based on the given query and store them in the temporary database.\n",
    "        Returns the top 3 papers information.\n",
    "        \"\"\"\n",
    "        console.print(f\"[bold cyan]Searching arXiv for papers related to:[/bold cyan] {query}\")\n",
    "        \n",
    "        try:\n",
    "            # Use the working ArxivLoader from langchain_community\n",
    "            loader = ArxivLoader(\n",
    "                query=query,\n",
    "                load_max_docs=3,  # Get top 3 papers\n",
    "                load_all_available_meta=True\n",
    "            )\n",
    "            \n",
    "            documents = loader.load()\n",
    "            console.print(f\"[green]Found {len(documents)} relevant papers[/green]\")\n",
    "            \n",
    "            # Download PDFs for more complete content\n",
    "            enhanced_docs = self.download_pdf_content(documents)\n",
    "            \n",
    "            # Store documents in temporary database\n",
    "            paper_ids = self.StoreInDB(enhanced_docs)\n",
    "            \n",
    "            # Display basic information about the loaded papers\n",
    "            self.display_paper_summaries(enhanced_docs)\n",
    "            \n",
    "            return paper_ids\n",
    "        \n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Error loading papers: {str(e)}[/bold red]\")\n",
    "            # # Fall back to simulated papers if needed\n",
    "            # if self.allow_fallback:\n",
    "            #     console.print(\"[yellow]Using fallback data for demonstration purposes.[/yellow]\")\n",
    "            #     return self.create_fallback_papers(query)\n",
    "            return []\n",
    "    \n",
    "    def download_pdf_content(self, documents):\n",
    "        \"\"\"\n",
    "        Enhance documents by downloading the full PDF content when available.\n",
    "        \"\"\"\n",
    "        enhanced_docs = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            try:\n",
    "                # Get metadata\n",
    "                metadata = doc.metadata\n",
    "                title = metadata.get(\"Title\", \"Untitled\")\n",
    "                console.print(f\"[cyan]Processing paper: {title}[/cyan]\")\n",
    "                \n",
    "                # Try to find arXiv ID from page content\n",
    "                arxiv_id = None\n",
    "                match = re.search(r'arXiv:(\\d{4}\\.\\d{5})', doc.page_content)\n",
    "                \n",
    "                if match:\n",
    "                    arxiv_id = match.group(1)\n",
    "                else:\n",
    "                    # Try to extract from Entry ID\n",
    "                    entry_id = metadata.get(\"Entry ID\", \"\")\n",
    "                    id_match = re.search(r'abs/(\\d{4}\\.\\d{5})', entry_id)\n",
    "                    if id_match:\n",
    "                        arxiv_id = id_match.group(1)\n",
    "                \n",
    "                if arxiv_id:\n",
    "                    # Generate PDF URL\n",
    "                    pdf_url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "                    console.print(f\"[cyan]Found PDF URL: {pdf_url}[/cyan]\")\n",
    "                    \n",
    "                    # Create temp directory if it doesn't exist\n",
    "                    temp_dir = \"./temp_pdfs\"\n",
    "                    os.makedirs(temp_dir, exist_ok=True)\n",
    "                    \n",
    "                    # Download PDF\n",
    "                    pdf_filename = f\"{temp_dir}/{arxiv_id}.pdf\"\n",
    "                    \n",
    "                    # Check if file already exists\n",
    "                    if not os.path.exists(pdf_filename):\n",
    "                        console.print(f\"[cyan]Downloading PDF...[/cyan]\")\n",
    "                        response = requests.get(pdf_url, timeout=30)\n",
    "                        with open(pdf_filename, 'wb') as f:\n",
    "                            f.write(response.content)\n",
    "                    \n",
    "                    # Load PDF content\n",
    "                    console.print(f\"[cyan]Extracting content from PDF...[/cyan]\")\n",
    "                    pdf_loader = PyPDFLoader(pdf_filename)\n",
    "                    pdf_docs = pdf_loader.load()\n",
    "                    \n",
    "                    # Create enhanced document with PDF content\n",
    "                    full_content = doc.page_content + \"\\n\\n\" + \"\\n\\n\".join([pdf_doc.page_content for pdf_doc in pdf_docs])\n",
    "                    \n",
    "                    # Update metadata with PDF info\n",
    "                    updated_metadata = metadata.copy()\n",
    "                    updated_metadata[\"pdf_url\"] = pdf_url\n",
    "                    updated_metadata[\"pdf_path\"] = pdf_filename\n",
    "                    updated_metadata[\"pdf_pages\"] = len(pdf_docs)\n",
    "                    \n",
    "                    enhanced_doc = Document(page_content=full_content, metadata=updated_metadata)\n",
    "                    enhanced_docs.append(enhanced_doc)\n",
    "                    \n",
    "                else:\n",
    "                    # If PDF can't be found, use original document\n",
    "                    console.print(f\"[yellow]Could not extract arXiv ID for {title}. Using abstract only.[/yellow]\")\n",
    "                    enhanced_docs.append(doc)\n",
    "            \n",
    "            except Exception as e:\n",
    "                console.print(f\"[yellow]Error enhancing document: {str(e)}. Using abstract only.[/yellow]\")\n",
    "                enhanced_docs.append(doc)\n",
    "        \n",
    "        return enhanced_docs\n",
    "    \n",
    "    def StoreInDB(self, documents):\n",
    "        \"\"\"\n",
    "        Store loaded documents in the temporary vector database.\n",
    "        Returns list of document IDs.\n",
    "        \"\"\"\n",
    "        paper_ids = []\n",
    "        \n",
    "        console.print(\"[bold cyan]Storing papers in temporary database...[/bold cyan]\")\n",
    "        \n",
    "        try:\n",
    "            # Split documents into chunks for better retrieval\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000, \n",
    "                chunk_overlap=200\n",
    "            )\n",
    "            \n",
    "            for doc in documents:\n",
    "                # Create document chunks\n",
    "                chunks = text_splitter.split_documents([doc])\n",
    "                \n",
    "                # Create unique ID for the paper\n",
    "                paper_id = f\"paper_{uuid.uuid4().hex[:8]}\"\n",
    "                paper_ids.append(paper_id)\n",
    "                \n",
    "                # Add metadata to each chunk and sanitize it\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    # Create clean metadata dictionary\n",
    "                    clean_metadata = {\n",
    "                        \"paper_id\": paper_id,\n",
    "                        \"chunk_id\": i,\n",
    "                        \"title\": doc.metadata.get(\"Title\", \"Untitled\"),\n",
    "                        \"authors\": doc.metadata.get(\"Authors\", \"Unknown\"),\n",
    "                        \"published\": doc.metadata.get(\"Published\", \"Unknown\")\n",
    "                    }\n",
    "                    \n",
    "                    # Ensure all values are valid types (str, int, float, bool)\n",
    "                    for key, value in clean_metadata.items():\n",
    "                        if value is None:\n",
    "                            clean_metadata[key] = \"None\"  # Convert None to string\n",
    "                        elif not isinstance(value, (str, int, float, bool)):\n",
    "                            clean_metadata[key] = str(value)  # Convert complex types to string\n",
    "                    \n",
    "                    # Replace existing metadata with clean version\n",
    "                    chunk.metadata = clean_metadata\n",
    "                \n",
    "                # Add to vector database\n",
    "                self.vector_db.add_documents(chunks)\n",
    "            \n",
    "            # Persist the temporary database\n",
    "            self.vector_db.persist()\n",
    "            console.print(f\"[green]Successfully stored {len(documents)} papers in database[/green]\")\n",
    "            # Fixed typo from 'onsole' to 'console' and removed debug line\n",
    "            return paper_ids\n",
    "        \n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Error storing documents: {str(e)}[/bold red]\")\n",
    "            return []\n",
    "    \n",
    "    def display_paper_summaries(self, documents):\n",
    "        \"\"\"Display a summary of each loaded paper.\"\"\"\n",
    "        console.print(\"\\n[bold cyan]Loaded Papers:[/bold cyan]\")\n",
    "        \n",
    "        for i, doc in enumerate(documents):\n",
    "            title = doc.metadata.get(\"Title\", \"Untitled\")\n",
    "            authors = doc.metadata.get(\"Authors\", \"Unknown\")\n",
    "            published = doc.metadata.get(\"Published\", \"Unknown\")\n",
    "            \n",
    "            console.print(f\"\\n[bold]{i+1}. {title}[/bold]\")\n",
    "            console.print(f\"   Authors: {authors}\")\n",
    "            console.print(f\"   Published: {published}\")\n",
    "            \n",
    "            # Generate a brief summary of the paper\n",
    "            summary = self.generate_paper_summary(doc)\n",
    "            console.print(f\"   Summary: {summary}\")\n",
    "    \n",
    "    def generate_paper_summary(self, document):\n",
    "        \"\"\"Generate a concise summary of a paper using the LLM.\"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            Generate a concise 5-sentence summary of the following research paper:\n",
    "            \n",
    "            Title: {document.metadata.get('Title', 'Untitled')}\n",
    "            Abstract: {document.page_content[:500]}...\n",
    "            \n",
    "            Summary:\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.llm.predict(prompt)\n",
    "            return response.strip()\n",
    "        except:\n",
    "            return \"Summary not available.\"\n",
    "    \n",
    "    def query_temp_database(self, query, paper_ids=None, k=5):\n",
    "        \"\"\"\n",
    "        Query the temporary database with the given query.\n",
    "        \n",
    "        Args:\n",
    "            query: The query string\n",
    "            paper_ids: Optional list of paper IDs to restrict the search to\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of relevant document chunks\n",
    "        \"\"\"\n",
    "        console.print(f\"[bold cyan]Searching temporary database for:[/bold cyan] {query}\")\n",
    "        \n",
    "        try:\n",
    "            # Create filter for specific papers if provided\n",
    "            filter_dict = None\n",
    "            if paper_ids:\n",
    "                filter_dict = {\"paper_id\": {\"$in\": paper_ids}}\n",
    "            \n",
    "            # Query the vector database\n",
    "            search_results = self.vector_db.similarity_search(\n",
    "                query=query,\n",
    "                k=k,\n",
    "                filter=filter_dict\n",
    "            )\n",
    "            \n",
    "            console.print(f\"[green]Found {len(search_results)} relevant sections[/green]\")\n",
    "            return search_results\n",
    "        \n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Error querying database: {str(e)}[/bold red]\")\n",
    "            return []\n",
    "    \n",
    "    def answer_research_question(self, query, context_docs=None):\n",
    "        \"\"\"\n",
    "        Answer a research question using context from the temporary database.\n",
    "        \n",
    "        Args:\n",
    "            query: The research question\n",
    "            context_docs: Optional pre-retrieved context documents\n",
    "            \n",
    "        Returns:\n",
    "            Detailed answer to the research question\n",
    "        \"\"\"\n",
    "        # If context not provided, retrieve it\n",
    "        if not context_docs:\n",
    "            context_docs = self.query_temp_database(query, k=5)\n",
    "        \n",
    "        if not context_docs:\n",
    "            return \"I couldn't find relevant information to answer your question.\"\n",
    "        \n",
    "        # Prepare context\n",
    "        context_text = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(context_docs)])\n",
    "        \n",
    "        # Prepare sources\n",
    "        sources = []\n",
    "        for doc in context_docs:\n",
    "            paper_title = doc.metadata.get(\"title\", \"Unknown Title\")\n",
    "            authors = doc.metadata.get(\"authors\", \"Unknown Authors\")\n",
    "            source = f\"- {paper_title} by {authors}\"\n",
    "            if source not in sources:\n",
    "                sources.append(source)\n",
    "        \n",
    "        # Generate the answer\n",
    "        prompt = f\"\"\"\n",
    "        You are a helpful research assistant. Answer the following question based on the provided research paper extracts.\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Here are relevant extracts from research papers:\n",
    "        {context_text}\n",
    "        \n",
    "        Provide a comprehensive answer, citing specific information from the papers.\n",
    "        Make sure to organize your response clearly and highlight key insights.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            answer = self.llm.predict(prompt)\n",
    "            \n",
    "            # Add sources at the end\n",
    "            final_answer = f\"{answer}\\n\\nSources:\\n\" + \"\\n\".join(sources)\n",
    "            return final_answer\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"Error generating answer: {str(e)}\"\n",
    "\n",
    "\n",
    "def main():    \n",
    "    \"\"\"Main function to run the research assistant.\"\"\"\n",
    "    try:\n",
    "        from langchain_community.llms import OpenAI\n",
    "        from langchain_community.vectorstores import Chroma\n",
    "        from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "        \n",
    "        # Check for API key\n",
    "        if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "            console.print(\"[bold red]ERROR: OPENAI_API_KEY environment variable not set.[/bold red]\")\n",
    "            console.print(f\"Please set your API key with: export OPENAI_API_KEY={os.environ['OPENAI_API_KEY']}\")\n",
    "            return\n",
    "            \n",
    "        # Load models and vector database\n",
    "        console.print(\"[bold cyan]Initializing SageRAG Research Assistant...[/bold cyan]\")\n",
    "        \n",
    "        console.print(\"Loading embedding model...\")\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        \n",
    "        console.print(\"Loading language model...\")\n",
    "        llm = OllamaLLM(model=\"llama3.2\")\n",
    "        \n",
    "        console.print(\"Connecting to vector database...\")\n",
    "\n",
    "        vector_db = Chroma(persist_directory= DB_DIR, embedding_function=embeddings)\n",
    "        \n",
    "        console.print(\"[bold green]Initialization complete![/bold green]\")\n",
    "        \n",
    "        # Initialize the research assistant\n",
    "        assistant = ResearchAssistant(llm=llm, vector_db=vector_db)\n",
    "        print(assistant.vector_db._collection.count())\n",
    "        # Display welcome message\n",
    "        console.print(Panel.fit(\n",
    "            Markdown(\"# SageRAG Research Assistant\\n\\nAsk questions about scientific papers in the database.\"),\n",
    "            title=\"Welcome\",\n",
    "            border_style=\"cyan\"\n",
    "        ))\n",
    "        \n",
    "        # Main interaction loop\n",
    "        while True:\n",
    "            console.print(\"\\n[bold cyan]Options:[/bold cyan]\")\n",
    "            console.print(\"1. Ask a question\")\n",
    "            console.print(\"2. View conversation history\")\n",
    "            console.print(\"3. Clear conversation history\")\n",
    "            console.print(\"4. Go for Advanced Research\")\n",
    "            console.print(\"5. Exit\")\n",
    "            \n",
    "            choice = input(\"\\nEnter your choice (1-5): \").strip()\n",
    "            \n",
    "            if choice == \"1\":\n",
    "                query = input(f\"\\n[bold]Enter your query:[/bold] \")\n",
    "                if not query.strip():\n",
    "                    console.print(f\"[yellow]Empty query. Please try again.[/yellow]\")\n",
    "                    continue\n",
    "                    \n",
    "                # Ask the user whether to use memory mode or full pipeline\n",
    "                use_memory_input = input(\"Use conversation memory? (y/n): \").lower()\n",
    "                use_memory = use_memory_input.startswith('y')\n",
    "                \n",
    "                # Process the query\n",
    "                console.print(f\"\\n[bold cyan]Processing your query...[/bold cyan]\")\n",
    "                answer = assistant.process_query(query, use_memory=use_memory)\n",
    "                \n",
    "                # Display the final answer nicely formatted\n",
    "                console.print(Panel(\n",
    "                    Markdown(answer),\n",
    "                    title=\"Answer\",\n",
    "                    border_style=\"green\",\n",
    "                    width=100\n",
    "                ))\n",
    "\n",
    "                console.print(\"\\n[bold cyan]Would you like to get an llm generated answer?[/bold cyan]\")\n",
    "                llm_answer = input(\"(y/n): \").lower()\n",
    "                llm_answer = llm_answer.startswith('y')\n",
    "                if(llm_answer):\n",
    "                    answer = assistant.generate_llm_answer(query)\n",
    "                \n",
    "                # Display the final answer nicely formatted\n",
    "                console.print(Panel(\n",
    "                    Markdown(answer),\n",
    "                    title=\"Answer\",\n",
    "                    border_style=\"green\",\n",
    "                    width=100\n",
    "                ))\n",
    "            # elif choice ==\"6\":\n",
    "                \n",
    "                \n",
    "            elif choice == \"2\":\n",
    "                history = assistant.get_chat_history()\n",
    "                console.print(Panel(\n",
    "                    history if history else \"No conversation history.\",\n",
    "                    title=\"Conversation History\",\n",
    "                    border_style=\"blue\"\n",
    "                ))\n",
    "                \n",
    "            elif choice == \"3\":\n",
    "                assistant.clear_memory()\n",
    "\n",
    "            elif choice == \"4\":\n",
    "                query = input(f\"\\n[bold]Enter your query:[/bold] \")\n",
    "                if not query.strip():\n",
    "                    console.print(f\"[yellow]Empty query. Please try again.[/yellow]\")\n",
    "                    continue\n",
    "                \n",
    "                # Initialize the adv research assistant\n",
    "                vector_db_temp = Chroma(persist_directory= \"./tempDB\", embedding_function=embeddings)\n",
    "                advAssistant = ResearchAssistant(llm=llm, vector_db=vector_db_temp)\n",
    "                # Process the query\n",
    "                console.print(f\"\\n[bold cyan]Processing your query...[/bold cyan]\")\n",
    "                # Step 1: Rate the query\n",
    "                console.print(\"\\n[bold cyan]Evaluating your query (with memory context)...[/bold cyan]\")\n",
    "                rating_result = advAssistant.rate_query(query)\n",
    "                \n",
    "                rating_color = \"green\" if rating_result['rating'] >= 4 else \"yellow\" if rating_result['rating'] >= 3 else \"red\"\n",
    "                console.print(f\"[bold {rating_color}]Query Rating: {rating_result['rating']}/5[/bold {rating_color}]\")\n",
    "                console.print(f\"Explanation: {rating_result['explanation']}\")\n",
    "                console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "                \n",
    "                # Step 2: Suggest rewrites if the rating is less than perfect\n",
    "                rewritten_queries = []\n",
    "                if rating_result[\"rating\"] < 5:\n",
    "                    console.print(\"[bold cyan]Generating improved query variations...[/bold cyan]\")\n",
    "                    rewritten_queries = advAssistant.suggest_rewrites(query)\n",
    "                    console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "                \n",
    "                # Step 3: Present options to the user\n",
    "                query_options = advAssistant.present_query_options(query, rewritten_queries)\n",
    "                console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "                \n",
    "                # Step 4: Get user selection\n",
    "                console.print(\"[bold cyan]Select queries to use:[/bold cyan]\")\n",
    "                console.print(\"Enter the query you want to use (comma-separated, e.g., '0,2,3')\")\n",
    "                selected_index = input(\"> \")\n",
    "                try:\n",
    "                    idx = int(selected_index)\n",
    "                    if idx == 0:  # Original query\n",
    "                        selected_query = query\n",
    "                    elif 0 < idx <= len(rewritten_queries):  # Rewritten query\n",
    "                        selected_query = rewritten_queries[idx-1]\n",
    "                    else:\n",
    "                        console.print(\"[yellow]Invalid selection. Using original query.[/yellow]\")\n",
    "                        selected_query = query\n",
    "                except ValueError:\n",
    "                    console.print(\"[yellow]Invalid input. Using original query.[/yellow]\")\n",
    "                    selected_query = query\n",
    "                \n",
    "                # Step 5: Load papers based on selected query\n",
    "                paper_ids = advAssistant.Load_query(selected_query)\n",
    "                \n",
    "                if paper_ids:\n",
    "                    # Step 6: Interactive research loop\n",
    "                    console.print(advAssistant.vector_db)\n",
    "                    while True:\n",
    "                        research_question = input(\"\\n[bold]Ask a question about these papers (or type 'exit' to return):[/bold] \")\n",
    "                        if research_question.lower() in ['exit', 'quit', 'back']:\n",
    "                            break\n",
    "                        if not research_question.strip():\n",
    "                            console.print(f\"[yellow]Empty query. Please try again.[/yellow]\")\n",
    "                            continue\n",
    "                            \n",
    "                        # Ask the user whether to use memory mode or full pipeline\n",
    "                        use_memory_input = input(\"Use conversation memory? (y/n): \").lower()\n",
    "                        use_memory = use_memory_input.startswith('y')\n",
    "                        \n",
    "                        # Process the query\n",
    "                        console.print(f\"\\n[bold cyan]Processing your query...[/bold cyan]\")\n",
    "                        answer = advAssistant.process_query(research_question, use_memory=use_memory)\n",
    "                        \n",
    "                        # Display the final answer nicely formatted\n",
    "                        console.print(Panel(\n",
    "                            Markdown(answer),\n",
    "                            title=\"Answer\",\n",
    "                            border_style=\"green\",\n",
    "                            width=100\n",
    "                        ))\n",
    "                else:\n",
    "                    console.print(\"[yellow]No papers were loaded. Please try a different query.[/yellow]\")\n",
    "                            \n",
    "                print(advAssistant.vector_db._collection.count())\n",
    "            elif choice == \"5\":\n",
    "                console.print(\"[bold green]Thank you for using SageRAG Research Assistant. Goodbye![/bold green]\")\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                console.print(\"[yellow]Invalid choice. Please enter a number from 1-4.[/yellow]\")\n",
    "                \n",
    "    except ImportError as e:\n",
    "        console.print(f\"[bold red]Error: Missing required packages: {e}[/bold red]\")\n",
    "        console.print(\"Please install the required packages with pip:\")\n",
    "        console.print(\"pip install langchain langchain_community sentence-transformers rich openai chromadb\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]An unexpected error occurred: {e}[/bold red]\")\n",
    "        import traceback\n",
    "        console.print(traceback.format_exc())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e891c5b9-5059-4502-90a4-b09026f31836",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
