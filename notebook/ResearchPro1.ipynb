{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6c9f53c-5fc2-4eee-9000-3cfae300122e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Improving DB with Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "445904a8-9cd7-48a0-ab9e-093cfb9bd1a5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class FoundationalKnowledgeManager:\n",
    "    def __init__(self, vector_db, embedding_model):\n",
    "        self.vector_db = vector_db\n",
    "        self.embedding_model = embedding_model\n",
    "        self.github_sources = {\n",
    "            \"algorithms\": \"TheAlgorithms/Python\",\n",
    "            \"os\": \"tuhdo/os01\",\n",
    "            \"system_design\": \"donnemartin/system-design-primer\",\n",
    "            \"python\": \"jakevdp/PythonDataScienceHandbook\",\n",
    "            \"ml\": \"microsoft/ML-For-Beginners\",\n",
    "            \"database\": \"pingcap/awesome-database-learning\",\n",
    "            \"networks\": \"leandromoreira/linux-network-performance-parameters\"\n",
    "        }\n",
    "        \n",
    "    def load_from_github(self, topic_key):\n",
    "        \"\"\"Load content from predefined GitHub repositories with foundational CS knowledge\"\"\"\n",
    "        if topic_key not in self.github_sources:\n",
    "            return f\"Topic {topic_key} not found in available sources\"\n",
    "            \n",
    "        repo = self.github_sources[topic_key]\n",
    "        # Implement GitHub content fetching logic here\n",
    "        # Use GitHub API to get markdown/text content from README files and other docs\n",
    "        \n",
    "        # Process content and add to vector DB with metadata\n",
    "        # Important: Tag these documents as foundational knowledge\n",
    "        # self.vector_db.add_texts(texts, metadatas=[{\"chunk_type\": \"foundational\", \"topic\": topic_key}])\n",
    "        \n",
    "        return f\"Loaded foundational knowledge for {topic_key} from {repo}\"\n",
    "        \n",
    "    def load_from_file(self, file_path, topic_name):\n",
    "        \"\"\"Load content from local file\"\"\"\n",
    "        # Implement file loading logic\n",
    "        # Process content and add to vector DB with metadata \n",
    "        # self.vector_db.add_texts(texts, metadatas=[{\"chunk_type\": \"foundational\", \"topic\": topic_name}])\n",
    "        \n",
    "    def load_standard_cs_topics(self):\n",
    "        \"\"\"Load all standard CS topics\"\"\"\n",
    "        results = []\n",
    "        for topic in self.github_sources:\n",
    "            results.append(self.load_from_github(topic))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6c0eec-dacb-4b49-81ad-dac04a9beac3",
   "metadata": {},
   "source": [
    "# Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b354f4-0b65-4e2e-8c7e-ad6f57c0271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"false\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"ResearchPro2\" \n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = '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'\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8f6906f-0a67-4bde-ac97-8d6e3cfeede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import necessary packages\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import BaseOutputParser, StrOutputParser\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "from typing import List, Literal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67465538-9a43-4952-9d20-79b8be8800d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5n/8m4t00412h97v592vh5y0pgc0000gn/T/ipykernel_65219/4293527524.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "/var/folders/5n/8m4t00412h97v592vh5y0pgc0000gn/T/ipykernel_65219/4293527524.py:6: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_db = Chroma(persist_directory=DB_DIR, embedding_function=embedding_model)\n"
     ]
    }
   ],
   "source": [
    "PDF_DIR = \"../../arxiv_data\"  # Go up two levels, then into arxiv_data\n",
    "# DB_DIR = \"../../arxiv_vector1_db\"  # Go up two levels, then into arxiv_vector1_db\n",
    "DB_DIR =\"./tempDB\"\n",
    "# Embedding & Vector DB\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_db = Chroma(persist_directory=DB_DIR, embedding_function=embedding_model)\n",
    "\n",
    "# Load embedding model once for scoring\n",
    "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# LLM\n",
    "llm = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "# print(os.getcwd())\n",
    "\n",
    "# import os\n",
    "\n",
    "# # Get current working directory\n",
    "# cwd = os.getcwd()\n",
    "\n",
    "# # Move up two levels\n",
    "# target_path = os.path.abspath(os.path.join(cwd, \"../../arxiv_vector1_db\"))\n",
    "\n",
    "# print(\"Current Directory:\", cwd)\n",
    "# print(\"Resolved ../../arxiv_vector1_db :\", target_path)\n",
    "# # List files/directories at that location\n",
    "# files = os.listdir(target_path)\n",
    "\n",
    "# print(f\"Contents of {target_path}:\")\n",
    "# for f in files:\n",
    "#     print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3627ae4b-5313-4aad-8204-30ff36b2cc14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Initializing SageRAG Research Assistant...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mInitializing SageRAG Research Assistant\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading embedding model<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading embedding model\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading language model<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading language model\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Connecting to vector database<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Connecting to vector database\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Initialization complete!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mInitialization complete!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 15:07:07,738 - SageRAG - INFO - Initializing default embedding model\n",
      "2025-04-21 15:07:11,430 - SageRAG - INFO - Research Assistant initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">╭──────────────────────────────────────────────────── Welcome ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span> ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span> ┃                                         <span style=\"font-weight: bold\">SageRAG Research Assistant</span>                                          ┃ <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span> ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span> Ask questions about scientific papers in the database.                                                          <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m╭─\u001b[0m\u001b[36m───────────────────────────────────────────────────\u001b[0m\u001b[36m Welcome \u001b[0m\u001b[36m───────────────────────────────────────────────────\u001b[0m\u001b[36m─╮\u001b[0m\n",
       "\u001b[36m│\u001b[0m ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ \u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m ┃                                         \u001b[1mSageRAG Research Assistant\u001b[0m                                          ┃ \u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ \u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m                                                                                                                 \u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m Ask questions about scientific papers in the database.                                                          \u001b[36m│\u001b[0m\n",
       "\u001b[36m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Options:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mOptions:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Ask a question\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m. Ask a question\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. View conversation history\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m. View conversation history\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Clear conversation history\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m. Clear conversation history\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. Go for Advanced Research\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4\u001b[0m. Go for Advanced Research\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. Exit\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m5\u001b[0m. Exit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your choice (1-5):  1\n",
      "\n",
      "[bold]Enter your query:[/bold]  What is FairSearch API ?\n",
      "Use conversation memory? (y/n):  n\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Processing your query...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mProcessing your query\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should_use_memory []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Using full query improvement pipeline...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mUsing full query improvement pipeline\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Evaluating your query...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mEvaluating your query\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 15:07:37,377 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Query Rating: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">/</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mQuery Rating: \u001b[0m\u001b[1;32m4\u001b[0m\u001b[1;32m/\u001b[0m\u001b[1;32m5\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Explanation: The query <span style=\"color: #008000; text-decoration-color: #008000\">'What is FairSearch API?'</span> is rated as <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> out of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> because it is clear and concise. The term \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'FairSearch'</span> suggests a specific search algorithm or technology, which indicates a good level of specificity. \n",
       "However, the lack of technical context or domain information makes the query slightly less relevant to scientific \n",
       "research. Nevertheless, it will still retrieve some relevant results from a general knowledge database, but may not\n",
       "yield precise or specialized content.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Explanation: The query \u001b[32m'What is FairSearch API?'\u001b[0m is rated as \u001b[1;36m4\u001b[0m out of \u001b[1;36m5\u001b[0m because it is clear and concise. The term \n",
       "\u001b[32m'FairSearch'\u001b[0m suggests a specific search algorithm or technology, which indicates a good level of specificity. \n",
       "However, the lack of technical context or domain information makes the query slightly less relevant to scientific \n",
       "research. Nevertheless, it will still retrieve some relevant results from a general knowledge database, but may not\n",
       "yield precise or specialized content.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Generating improved query variations...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mGenerating improved query variations\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 15:07:40,683 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Available search queries:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mAvailable search queries:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">.</span> Original: What is FairSearch API ?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m0\u001b[0m\u001b[1m.\u001b[0m Original: What is FairSearch API ?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Confidence: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;32mConfidence: \u001b[0m\u001b[1;32m1.00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: Original: What is FairSearch API ?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m1\u001b[0m: Original: What is FairSearch API ?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Confidence: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.95</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;32mConfidence: \u001b[0m\u001b[1;32m0.95\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: **Breaking down complex queries into clearer formulations**: <span style=\"color: #008000; text-decoration-color: #008000\">\"How does the FairSearch API </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">facilitate information retrieval from academic databases? What features enable it to optimize search results for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">researchers?\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m2\u001b[0m: Rewrite \u001b[1;36m2\u001b[0m: **Breaking down complex queries into clearer formulations**: \u001b[32m\"How does the FairSearch API \u001b[0m\n",
       "\u001b[32mfacilitate information retrieval from academic databases? What features enable it to optimize search results for \u001b[0m\n",
       "\u001b[32mresearchers?\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.64</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.64\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>: **Including key entities and relationships from the original query**: <span style=\"color: #008000; text-decoration-color: #008000\">\"Analyze the role of</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">natural language processing, information retrieval techniques, and machine learning algorithms in the FairSearch </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">API's functionality.\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m3\u001b[0m: Rewrite \u001b[1;36m5\u001b[0m: **Including key entities and relationships from the original query**: \u001b[32m\"Analyze the role of\u001b[0m\n",
       "\u001b[32mnatural language processing, information retrieval techniques, and machine learning algorithms in the FairSearch \u001b[0m\n",
       "\u001b[32mAPI's functionality.\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.59</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.59\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>: Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: **Technical terminology for better vector matching**: <span style=\"color: #008000; text-decoration-color: #008000\">\"What search algorithms and data </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">structures are employed in the FairSearch API to efficiently query large datasets?\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m4\u001b[0m: Rewrite \u001b[1;36m1\u001b[0m: **Technical terminology for better vector matching**: \u001b[32m\"What search algorithms and data \u001b[0m\n",
       "\u001b[32mstructures are employed in the FairSearch API to efficiently query large datasets?\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.58</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.58\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>: Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>: **Varying syntax while preserving semantic meaning**: <span style=\"color: #008000; text-decoration-color: #008000\">\"In what ways does the FairSearch </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">API facilitate fast and accurate search results for academic publications? What are its advantages over traditional</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">search engines?\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m5\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m5\u001b[0m: Rewrite \u001b[1;36m4\u001b[0m: **Varying syntax while preserving semantic meaning**: \u001b[32m\"In what ways does the FairSearch \u001b[0m\n",
       "\u001b[32mAPI facilitate fast and accurate search results for academic publications? What are its advantages over traditional\u001b[0m\n",
       "\u001b[32msearch engines?\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.57</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.57\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>: Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: **Adding relevant synonyms or related concepts**: <span style=\"color: #008000; text-decoration-color: #008000\">\"Examine the FairSearch API's </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">application in bibliographic data management, document similarity analysis, and scholarly communication systems.\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m6\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m6\u001b[0m: Rewrite \u001b[1;36m3\u001b[0m: **Adding relevant synonyms or related concepts**: \u001b[32m\"Examine the FairSearch API's \u001b[0m\n",
       "\u001b[32mapplication in bibliographic data management, document similarity analysis, and scholarly communication systems.\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.53</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.53\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Select queries to use:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mSelect queries to use:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Enter the numbers of the queries you want to use <span style=\"font-weight: bold\">(</span>comma-separated, e.g., <span style=\"color: #008000; text-decoration-color: #008000\">'0,2,3'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Enter the numbers of the queries you want to use \u001b[1m(\u001b[0mcomma-separated, e.g., \u001b[32m'0,2,3'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Selected </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> queries for retrieval.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mSelected \u001b[0m\u001b[1;32m1\u001b[0m\u001b[1;32m queries for retrieval.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Retrieving relevant documents...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mRetrieving relevant documents\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG123: only this is working\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Retrieved </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> documents</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mRetrieved \u001b[0m\u001b[1;32m5\u001b[0m\u001b[1;32m documents\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Top retrieved documents:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mTop retrieved documents:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3467</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m. \u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3467\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Preview: E. FairSearch\n",
       "FairSearch is the fair open-source search API to apply\n",
       "fairness in ranked search results <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">33</span><span style=\"font-weight: bold\">]</span>. Its ranked group fairness\n",
       "deﬁnition exten<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Preview: E. FairSearch\n",
       "FairSearch is the fair open-source search API to apply\n",
       "fairness in ranked search results \u001b[1m[\u001b[0m\u001b[1;36m33\u001b[0m\u001b[1m]\u001b[0m. Its ranked group fairness\n",
       "deﬁnition exten\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Source: Unknown\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Source: Unknown\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3467</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m. \u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3467\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Preview: E. FairSearch\n",
       "FairSearch is the fair open-source search API to apply\n",
       "fairness in ranked search results <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">33</span><span style=\"font-weight: bold\">]</span>. Its ranked group fairness\n",
       "deﬁnition exten<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Preview: E. FairSearch\n",
       "FairSearch is the fair open-source search API to apply\n",
       "fairness in ranked search results \u001b[1m[\u001b[0m\u001b[1;36m33\u001b[0m\u001b[1m]\u001b[0m. Its ranked group fairness\n",
       "deﬁnition exten\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Source: Unknown\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Source: Unknown\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.2909</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m. \u001b[1;31mScore: \u001b[0m\u001b[1;31m0.2909\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Preview: ical display. Each data point is displayed in the graph are\n",
       "editable and can be set to different values to observe counter-\n",
       "factual points in the data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Preview: ical display. Each data point is displayed in the graph are\n",
       "editable and can be set to different values to observe counter-\n",
       "factual points in the data\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Source: Unknown\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Source: Unknown\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.2909</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4\u001b[0m. \u001b[1;31mScore: \u001b[0m\u001b[1;31m0.2909\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Preview: ical display. Each data point is displayed in the graph are\n",
       "editable and can be set to different values to observe counter-\n",
       "factual points in the data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Preview: ical display. Each data point is displayed in the graph are\n",
       "editable and can be set to different values to observe counter-\n",
       "factual points in the data\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Source: Unknown\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Source: Unknown\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.2827</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m5\u001b[0m. \u001b[1;31mScore: \u001b[0m\u001b[1;31m0.2827\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Preview: using linear classiﬁer and neural network classiﬁer. Model s\n",
       "of classiﬁcation can be assessed against any input features . A\n",
       "ground truth feature can <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Preview: using linear classiﬁer and neural network classiﬁer. Model s\n",
       "of classiﬁcation can be assessed against any input features . A\n",
       "ground truth feature can \u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Source: Unknown\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Source: Unknown\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Generating answer with document context...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mGenerating answer with document context\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 15:07:55,907 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-04-21 15:07:56,789 - SageRAG - INFO - Document relevance: Sufficient=True, Confidence=0.9\n",
      "2025-04-21 15:07:56,791 - SageRAG - INFO - Explanation: The retrieved documents provide a clear and concise definition of FairSearch API, its purpose, and its benefits.\n",
      "2025-04-21 15:07:59,734 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">╭───────────────────────────────────────────── Answer ─────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> According to the provided context, FairSearch is an open-source search API that applies fairness <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> in ranked search results. Its ranked group fairness definition extends group fairness using the  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> standard notion of protected groups, based on ensuring that the proportion of protected          <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> candidates in every prefix of the top-k ranking remains statistically above or indistinguishable <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> from a given minimum.                                                                            <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> According to Document 1 [Relevance: 0.35], FairSearch is described as \"the fair open-source      <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> search API to apply fairness in ranked search results\".                                          <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> Additionally, in Document 3 and Document 4, there is a mention of E. FairSearch, but the         <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> description provided in these documents seems to be incomplete or identical to Document 1.       <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> There appears to be some discrepancy in the information across different documents, with         <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> Document 5 stating \"E. F airSearch\" instead of the correct spelling \"E. FairSearch\".             <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m╭─\u001b[0m\u001b[32m────────────────────────────────────────────\u001b[0m\u001b[32m Answer \u001b[0m\u001b[32m────────────────────────────────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
       "\u001b[32m│\u001b[0m According to the provided context, FairSearch is an open-source search API that applies fairness \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m in ranked search results. Its ranked group fairness definition extends group fairness using the  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m standard notion of protected groups, based on ensuring that the proportion of protected          \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m candidates in every prefix of the top-k ranking remains statistically above or indistinguishable \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m from a given minimum.                                                                            \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m According to Document 1 [Relevance: 0.35], FairSearch is described as \"the fair open-source      \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m search API to apply fairness in ranked search results\".                                          \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m Additionally, in Document 3 and Document 4, there is a mention of E. FairSearch, but the         \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m description provided in these documents seems to be incomplete or identical to Document 1.       \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m There appears to be some discrepancy in the information across different documents, with         \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m Document 5 stating \"E. F airSearch\" instead of the correct spelling \"E. FairSearch\".             \u001b[32m│\u001b[0m\n",
       "\u001b[32m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Would you like to get an llm generated answer?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mWould you like to get an llm generated answer?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(y/n):  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 15:56:37,480 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">╭───────────────────────────────────────────── Answer ─────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> I'm happy to help you with your question about FairSearch API! However, I want to clarify that   <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> my response will be based on general knowledge and may not reflect the most up-to-date or        <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> accurate information from specific papers.                                                       <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> From what I've gathered, FairSearch is an open-source search API that aims to promote fairness   <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> in ranked search results. It uses a unique approach called \"ranked group fairness\" to ensure     <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> that the proportion of protected candidates in every prefix of the top-k ranking remains         <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> statistically above or indistinguishable from a given minimum.                                   <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> In essence, FairSearch applies a fairness metric to search results, aiming to prevent biases and <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> discriminatory outcomes. This is achieved by analyzing the representation of underrepresented    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> groups in the top-ranked results and adjusting the ranking accordingly.                          <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> While I couldn't find any specific citations to support this information, it's clear that        <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> FairSearch is designed to address issues of fairness and bias in search engines. If you're       <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> looking for more detailed information or references to specific papers, I'd be happy to try and  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> help you with that!                                                                              <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m╭─\u001b[0m\u001b[32m────────────────────────────────────────────\u001b[0m\u001b[32m Answer \u001b[0m\u001b[32m────────────────────────────────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
       "\u001b[32m│\u001b[0m I'm happy to help you with your question about FairSearch API! However, I want to clarify that   \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m my response will be based on general knowledge and may not reflect the most up-to-date or        \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m accurate information from specific papers.                                                       \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m From what I've gathered, FairSearch is an open-source search API that aims to promote fairness   \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m in ranked search results. It uses a unique approach called \"ranked group fairness\" to ensure     \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m that the proportion of protected candidates in every prefix of the top-k ranking remains         \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m statistically above or indistinguishable from a given minimum.                                   \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m In essence, FairSearch applies a fairness metric to search results, aiming to prevent biases and \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m discriminatory outcomes. This is achieved by analyzing the representation of underrepresented    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m groups in the top-ranked results and adjusting the ranking accordingly.                          \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m While I couldn't find any specific citations to support this information, it's clear that        \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m FairSearch is designed to address issues of fairness and bias in search engines. If you're       \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m looking for more detailed information or references to specific papers, I'd be happy to try and  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m help you with that!                                                                              \u001b[32m│\u001b[0m\n",
       "\u001b[32m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Options:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mOptions:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Ask a question\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m. Ask a question\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. View conversation history\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m. View conversation history\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Clear conversation history\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m. Clear conversation history\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. Go for Advanced Research\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4\u001b[0m. Go for Advanced Research\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. Exit\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m5\u001b[0m. Exit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your choice (1-5):  5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Thank you for using SageRAG Research Assistant. Goodbye!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mThank you for using SageRAG Research Assistant. Goodbye!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### import os\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, BaseOutputParser\n",
    "from langchain.schema import Document, HumanMessage, AIMessage\n",
    "from langchain.vectorstores import VectorStore\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms.base import BaseLLM\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from pydantic import BaseModel, Field\n",
    "import logging\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "# Suppress library-specific logging\n",
    "import logging\n",
    "logging.getLogger('sentence_transformers').setLevel(logging.WARNING)\n",
    "logging.getLogger('transformers').setLevel(logging.WARNING)\n",
    "logging.getLogger('chromadb').setLevel(logging.WARNING)\n",
    "\n",
    "# Then set up your own logging as before\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"SageRAG\")\n",
    "\n",
    "# Rich console for prettier output\n",
    "console = Console()\n",
    "\n",
    "# Enhanced output parsers with better error handling\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Parse output that contains a numbered list and return as a list of strings.\"\"\"\n",
    "    \n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        \"\"\"Parse text into a list of strings.\"\"\"\n",
    "        # Updated regex to be more robust with various numbering styles\n",
    "        lines = re.findall(r\"^\\s*\\d+\\.?\\s+(.*?)$\", text, re.MULTILINE)\n",
    "        return [line.strip() for line in lines if line.strip()]\n",
    "    \n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"line_list\"\n",
    "\n",
    "class QueryRating(BaseModel):\n",
    "    \"\"\"Schema for query rating output.\"\"\"\n",
    "    rating: int = Field(description=\"Rating from 1-5\")\n",
    "    explanation: str = Field(description=\"Explanation for the rating\")\n",
    "\n",
    "@dataclass\n",
    "class RetrievedDocument:\n",
    "    \"\"\"Dataclass for tracking retrieved document info.\"\"\"\n",
    "    document: Document\n",
    "    score: float\n",
    "    query: str = \"\"\n",
    "    rank: int = 0\n",
    "\n",
    "class ResearchAssistant:\n",
    "    \"\"\"AI Research Assistant using RAG for scientific paper queries.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        llm: BaseLLM, \n",
    "        vector_db: VectorStore, \n",
    "        embeddings_model: Optional[SentenceTransformer] = None,\n",
    "        relevance_threshold: float = 0.2,\n",
    "        max_docs_per_query: int = 3\n",
    "    ):\n",
    "        \"\"\"Initialize the Research Assistant with LLM and vector database.\n",
    "        \n",
    "        Args:\n",
    "            llm: The language model to use for generation\n",
    "            vector_db: Vector database for document retrieval\n",
    "            embeddings_model: Optional SentenceTransformer model for semantic similarity\n",
    "            relevance_threshold: Minimum relevance score for documents\n",
    "            max_docs_per_query: Maximum documents to use per query\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.vector_db = vector_db\n",
    "        self.relevance_threshold = relevance_threshold\n",
    "        self.max_docs_per_query = max_docs_per_query\n",
    "        \n",
    "        # Initialize sentence transformer model for semantic similarity if not provided\n",
    "        if embeddings_model is None:\n",
    "            logger.info(\"Initializing default embedding model\")\n",
    "            self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        else:\n",
    "            self.semantic_model = embeddings_model\n",
    "            \n",
    "        # Initialize memory\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        # Create a retriever for direct use\n",
    "        self.retriever = vector_db.as_retriever()\n",
    "        \n",
    "        # Define parsers\n",
    "        self.json_parser = JsonOutputParser(pydantic_object=QueryRating)\n",
    "        self.list_parser = LineListOutputParser()\n",
    "        \n",
    "        logger.info(\"Research Assistant initialized successfully\")\n",
    "    \n",
    "    def rate_query(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Rates the query and gives an explanation.\n",
    "        \n",
    "        Args:\n",
    "            query: The user's query to be evaluated\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing rating and explanation\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an intelligent assistant trained to evaluate search queries for a scientific research database.\n",
    "        Given the following query: \"{query}\"\n",
    "        \n",
    "        1. Rate the query on a scale of 1 (very poor) to 5 (excellent) based on:\n",
    "           - Clarity: Is the query clear and unambiguous?\n",
    "           - Specificity: Does it contain specific technical terms or concepts?\n",
    "           - Relevance: Is it focused on retrieving scientific content?\n",
    "           - Retrievability: Will it work well with vector search?\n",
    "        \n",
    "        2. Provide a short explanation for the rating (what makes it effective or ineffective).\n",
    "        \n",
    "        Respond in JSON format:\n",
    "        {{\n",
    "          \"rating\": <number between 1-5>,\n",
    "          \"explanation\": \"<your explanation>\"\n",
    "        }}\n",
    "        \"\"\")\n",
    "        \n",
    "        chain: Runnable = prompt | self.llm | self.json_parser\n",
    "        \n",
    "        try:\n",
    "            return chain.invoke({\"query\": query})\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error rating query: {e}\")\n",
    "            # Fallback response if parsing fails\n",
    "            return {\n",
    "                \"rating\": 3, \n",
    "                \"explanation\": \"Unable to rate query properly. Consider adding more specific terms.\"\n",
    "            }\n",
    "    \n",
    "    # def suggest_rewrites(self, query: str, chat_history: Optional[List] = None) -> List[str]:\n",
    "    #     \"\"\"Returns rephrased versions of the query optimized for retrieval.\n",
    "        \n",
    "    #     Args:\n",
    "    #         query: Original query to rewrite\n",
    "    #         chat_history: Optional conversation history for context\n",
    "            \n",
    "    #     Returns:\n",
    "    #         List of rewritten queries\n",
    "    #     \"\"\"\n",
    "    #     history_context = \"\"\n",
    "    #     if chat_history:\n",
    "    #         history_context = \"Consider this conversation context when rewriting the query:\\n\"\n",
    "    #         for message in chat_history[-3:]:  # Use only last 3 messages for brevity\n",
    "    #             if hasattr(message, \"content\"):\n",
    "    #                 role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "    #                 history_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        # prompt = PromptTemplate(\n",
    "        #     input_variables=[\"question\", \"history_context\"],\n",
    "        #     template=\"\"\"You are an AI assistant specializing in scientific research queries.\n",
    "            \n",
    "        #     {history_context}\n",
    "            \n",
    "        #     Rewrite the following question in 5 different ways to improve retrieval from a scientific paper database.\n",
    "        #     Each rewrite should be a complete, standalone query that could be used directly for search.\n",
    "            \n",
    "        #     For each rewrite, focus on a different strategy and provide a brief explanation:\n",
    "        #     1. Technical terminology for better vector matching\n",
    "        #     2. Breaking down the query into a clearer, more detailed formulation\n",
    "        #     3. Including relevant synonyms or related concepts that might appear in papers\n",
    "        #     4. Using different syntax while preserving the original meaning\n",
    "        #     5. Explicitly mentioning key entities and relationships from the original query\n",
    "            \n",
    "        #     Format your response as a numbered list with the rewritten query and its explanation.\n",
    "        \n",
    "        #     Original Question: {question}\n",
    "            \n",
    "        #     Rewritten Queries:\n",
    "        #     1. Rewrite 1: **Technical terminology for better vector matching**: [your rewrite]\n",
    "        #     2. Rewrite 2: **Breaking down complex queries into clearer formulations**: [your rewrite]\n",
    "        #     ...\"\"\"\n",
    "        # )\n",
    "        \n",
    "    #     try:\n",
    "    #         # Make sure we're handling the response type correctly\n",
    "    #         result = (prompt | self.llm | self.list_parser).invoke({\n",
    "    #             \"question\": query,\n",
    "    #             \"history_context\": history_context\n",
    "    #         })\n",
    "    #         return result\n",
    "            \n",
    "    #     except Exception as e:\n",
    "    #         logger.error(f\"Error suggesting rewrites with explanations: {e}\")\n",
    "    #         # Return minimal set with explanations if parsing fails\n",
    "    #         return [\n",
    "    #             {\"query\": query, \"explanation\": \"Original query\", \"confidence\": 1.0},\n",
    "    #             {\"query\": f\"Research about {query}\", \"explanation\": \"Technical terminology\", \"confidence\": 0.7},\n",
    "    #             {\"query\": f\"Papers discussing {query} methodology and applications\", \"explanation\": \"Breaking down query\", \"confidence\": 0.7},\n",
    "    #             {\"query\": f\"Literature review on {query} techniques and algorithms\", \"explanation\": \"Including synonyms\", \"confidence\": 0.7},\n",
    "    #             {\"query\": f\"Recent advances in {query} research\", \"explanation\": \"Different syntax\", \"confidence\": 0.7},\n",
    "    #             {\"query\": f\"Theoretical foundations of {query}\", \"explanation\": \"Key entities and relationships\", \"confidence\": 0.7}\n",
    "    #         ]\n",
    "    \n",
    "    def suggest_rewrites(self, query: str, chat_history: Optional[List] = None) -> List[str]:\n",
    "        \"\"\"Returns rephrased versions of the query optimized for retrieval.\n",
    "        \n",
    "        Args:\n",
    "            query: Original query to rewrite\n",
    "            chat_history: Optional conversation history for context\n",
    "            \n",
    "        Returns:\n",
    "            List of rewritten queries\n",
    "        \"\"\"\n",
    "        history_context = \"\"\n",
    "        if chat_history:\n",
    "            history_context = \"Consider this conversation context when rewriting the query:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use only last 3 messages for brevity\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    history_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"history_context\"],\n",
    "            template=\"\"\"You are an AI assistant specializing in scientific research queries.\n",
    "        \n",
    "        {history_context}\n",
    "        \n",
    "        TASK: Create 5 alternative versions of the user's question that will help improve retrieval from a scientific paper database.\n",
    "        \n",
    "        Original question: {question}\n",
    "        \n",
    "        For each alternative version:\n",
    "        - Write a COMPLETE, EXECUTABLE query (not just a description)\n",
    "        - Make each version meaningfully different while preserving the core information need\n",
    "        - Include the ACTUAL REWRITTEN QUESTION text\n",
    "        \n",
    "        FORMAT YOUR RESPONSE EXACTLY LIKE THIS EXAMPLE:\n",
    "        \n",
    "        0. Original: What are the effects of climate change on coral reefs?\n",
    "           Confidence: 1.00\n",
    "        1. Rewrite 1: **Technical terminology for better vector matching**: \"What are the impacts of increased ocean acidification and sea surface temperature anomalies on scleractinian coral communities and associated reef ecosystems?\"\n",
    "           Confidence: 0.70\n",
    "        2. Rewrite 2: **Breaking down complex queries into clearer formulations**: \"How do rising ocean temperatures affect coral health? What role does ocean acidification play in coral bleaching events? What are the ecosystem-level consequences of coral reef degradation due to climate change?\"\n",
    "           Confidence: 0.70\n",
    "        3. Rewrite 3: **Adding relevant synonyms or related concepts**: \"Examine the effects of global warming, greenhouse gas emissions, and anthropogenic climate forcing on coral reef ecosystems, coral bleaching, calcification rates, and reef biodiversity.\"\n",
    "           Confidence: 0.70\n",
    "        4. Rewrite 4: **Varying syntax while preserving semantic meaning**: \"In what ways are coral reef systems being modified by climate change variables? How are anthozoans responding to altered oceanic conditions resulting from global climate shifts?\"\n",
    "           Confidence: 0.70\n",
    "        5. Rewrite 5: **Including key entities and relationships from the original query**: \"Analyze the causal relationship between climate change parameters (CO2 levels, temperature increase, ocean pH) and coral reef ecosystem health indicators (species diversity, coral cover percentage, bleaching frequency).\"\n",
    "           Confidence: 0.70\n",
    "        \n",
    "        Now, for the question \"{question}\", provide 5 complete, executable rewritten queries following the exact format above. Each rewrite must include the full rewritten question text, not just a description of how to rewrite it.\n",
    "        \"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            result = (prompt | self.llm | self.list_parser).invoke({\n",
    "                \"question\": query,\n",
    "                \"history_context\": history_context\n",
    "            })\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error suggesting rewrites: {e}\")\n",
    "            # Return a minimal set of rewrites if parsing fails\n",
    "            return [\n",
    "                query,  # Original query\n",
    "                f\"research about {query}\",\n",
    "                f\"papers discussing {query}\"\n",
    "            ]\n",
    "    def compute_confidence(self, original: str, rewritten: str) -> float:\n",
    "        \"\"\"Computes semantic similarity between original and rewritten queries.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            rewritten: Rewritten version\n",
    "            \n",
    "        Returns:\n",
    "            Similarity score between 0-1\n",
    "        \"\"\"\n",
    "        try:\n",
    "            vec_orig = self.semantic_model.encode(original, convert_to_tensor=True)\n",
    "            vec_rewrite = self.semantic_model.encode(rewritten, convert_to_tensor=True)\n",
    "            return float(util.pytorch_cos_sim(vec_orig, vec_rewrite).item())\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error computing similarity: {e}\")\n",
    "            return 0.7  # Default reasonable value\n",
    "    \n",
    "    def score_queries(self, original: str, queries: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Scores and sorts rephrased queries by confidence.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            queries: List of rewritten queries\n",
    "            \n",
    "        Returns:\n",
    "            List of (query, score) tuples sorted by score\n",
    "        \"\"\"\n",
    "        scored = [(q, self.compute_confidence(original, q)) for q in queries]\n",
    "        return sorted(scored, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    def present_query_options(self, original: str, queries: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Presents the original and rewritten queries with confidence scores.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            queries: List of rewritten queries\n",
    "            \n",
    "        Returns:\n",
    "            List of (query, score) tuples including original query\n",
    "        \"\"\"\n",
    "        # Add original query at the top\n",
    "        scored_queries = [(\"Original: \" + original, 1.0)]\n",
    "        \n",
    "        # Add scored rewritten queries\n",
    "        rewritten_scores = self.score_queries(original, queries)\n",
    "        for i, (query, score) in enumerate(rewritten_scores, 1):\n",
    "            if \"Query:\" in query:\n",
    "                parts = query.split(\"Query:\")\n",
    "                # Format with Query on a new line and bold\n",
    "                formatted_query = f\"{parts[0]}\\n[bold]Query:[/bold]{parts[1]}\"\n",
    "                scored_queries.append((f\"Rewrite {i}: {formatted_query}\", score))\n",
    "            else:\n",
    "                scored_queries.append((f\"Rewrite {i}: {query}\", score))\n",
    "            \n",
    "        # Display options in a nice format\n",
    "        console.print(\"\\n[bold cyan]Available search queries:[/bold cyan]\")\n",
    "        for i, (query, score) in enumerate(scored_queries):\n",
    "            confidence_color = \"green\" if score > 0.8 else \"yellow\" if score > 0.6 else \"red\"\n",
    "            console.print(f\"[bold]{i}.[/bold] {query}\")\n",
    "            console.print(f\"   [bold {confidence_color}]Confidence: {score:.2f}[/bold {confidence_color}]\")\n",
    "            \n",
    "        return scored_queries\n",
    "    \n",
    "    \n",
    "    def retrieve_documents(\n",
    "        self, \n",
    "        queries: List[str], \n",
    "        k: int = 5\n",
    "    ) -> Tuple[List[RetrievedDocument], Dict[str, List[RetrievedDocument]]]:\n",
    "        \"\"\"Retrieves documents using multiple queries, preserving query information.\n",
    "        \n",
    "        Args:\n",
    "            queries: List of queries to retrieve documents for\n",
    "            k: Number of documents to retrieve per query\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (all unique documents, query->documents mapping)\n",
    "        \"\"\"\n",
    "        all_docs: List[RetrievedDocument] = []\n",
    "        unique_content = set()\n",
    "        query_docs_map: Dict[str, List[RetrievedDocument]] = {}\n",
    "        \n",
    "        # Retrieve docs for each query\n",
    "        for query in queries:\n",
    "            console.print(f\"\\n[bold blue]Query:[/bold blue] '{query}'\")\n",
    "            \n",
    "            try:\n",
    "                results = self.vector_db.similarity_search_with_relevance_scores(query, k=k)\n",
    "                print(\"LOG123: this is results length\",len(results))\n",
    "                docs_for_query: List[RetrievedDocument] = []\n",
    "                \n",
    "                for i, (doc, score) in enumerate(results, 1):\n",
    "                    # Create retrieved document object\n",
    "                    retrieved_doc = RetrievedDocument(\n",
    "                        document=doc,\n",
    "                        score=score,\n",
    "                        query=query,\n",
    "                        rank=i\n",
    "                    )\n",
    "                    \n",
    "                    # Display result info\n",
    "                    score_color = \"green\" if score > 0.8 else \"yellow\" if score > 0.6 else \"red\"\n",
    "                    console.print(f\"[bold]--- Result {i} ---[/bold]\")\n",
    "                    console.print(f\"[bold {score_color}]Score: {score:.4f}[/bold {score_color}]\")\n",
    "                    \n",
    "                    # Show document preview\n",
    "                    preview = doc.page_content[:150] + \"...\" if len(doc.page_content) > 150 else doc.page_content\n",
    "                    console.print(Panel(preview, title=\"Content Preview\", width=100))\n",
    "                    \n",
    "                    # Show metadata if available\n",
    "                    if hasattr(doc, 'metadata') and doc.metadata:\n",
    "                        source = doc.metadata.get('source', 'Unknown')\n",
    "                        console.print(f\"[dim]Source: {source}[/dim]\")\n",
    "                    \n",
    "                    # Add to results for this query\n",
    "                    docs_for_query.append(retrieved_doc)\n",
    "                    \n",
    "                    # Only add unique documents to overall collection\n",
    "                    if doc.page_content not in unique_content:\n",
    "                        unique_content.add(doc.page_content)\n",
    "                        all_docs.append(retrieved_doc)\n",
    "                    for val in unique_content:\n",
    "                        console.print(val)\n",
    "                query_docs_map[query] = docs_for_query\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error retrieving documents for query '{query}': {e}\")\n",
    "                console.print(f\"[bold red]Error retrieving documents for query: {query}[/bold red]\")\n",
    "        \n",
    "        console.print(f\"\\n[bold green]Total unique documents:[/bold green] {len(all_docs)}\")\n",
    "        return all_docs, query_docs_map\n",
    "\n",
    "    def rerank_docs(self, query: str, docs: List[RetrievedDocument]) -> List[RetrievedDocument]:\n",
    "        \"\"\"Reranks documents based on semantic similarity to the query.\n",
    "        \n",
    "        Args:\n",
    "            query: Query to compare documents against\n",
    "            docs: List of retrieved documents\n",
    "            \n",
    "        Returns:\n",
    "            Reranked list of documents with updated scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            query_embedding = self.semantic_model.encode(query, convert_to_tensor=True)\n",
    "            reranked = []\n",
    "            \n",
    "            for doc in docs:\n",
    "                doc_embedding = self.semantic_model.encode(doc.document.page_content, convert_to_tensor=True)\n",
    "                new_score = float(util.pytorch_cos_sim(query_embedding, doc_embedding).item())\n",
    "                \n",
    "                # Create new RetrievedDocument with updated score\n",
    "                reranked_doc = RetrievedDocument(\n",
    "                    document=doc.document,\n",
    "                    score=new_score,\n",
    "                    query=doc.query,\n",
    "                    rank=0  # Will be updated after sorting\n",
    "                )\n",
    "                reranked.append(reranked_doc)\n",
    "            \n",
    "            # Sort by score and update ranks\n",
    "            reranked.sort(key=lambda x: x.score, reverse=True)\n",
    "            for i, doc in enumerate(reranked, 1):\n",
    "                doc.rank = i\n",
    "                \n",
    "            return reranked\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reranking documents: {e}\")\n",
    "            return docs  # Return original documents if reranking fails\n",
    "    \n",
    "    def can_answer_without_retrieval(self, question: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"Determines if a question can be answered directly from memory without retrieval.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (can_answer, answer_if_available)\n",
    "        \"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        if not chat_history:\n",
    "            return False, None\n",
    "            \n",
    "        # Create the prompt to check if we can answer directly from the conversation\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an AI assistant helping with a scientific research conversation.\n",
    "        Given the following conversation history and a new question, determine if the question:\n",
    "        1. Can be answered directly based ONLY on the conversation history (like \"what was my previous question?\")\n",
    "        2. Does NOT require retrieving new information from scientific papers\n",
    "        \n",
    "        If both conditions are true, provide the answer. Otherwise, respond with \"NEEDS_RETRIEVAL\".\n",
    "        \n",
    "        Conversation History:\n",
    "        {chat_history}\n",
    "        \n",
    "        New Question: {question}\n",
    "        \n",
    "        Your assessment (answer directly or respond with \"NEEDS_RETRIEVAL\"):\n",
    "        \"\"\")\n",
    "        \n",
    "        # Format the chat history for context\n",
    "        history_str = \"\"\n",
    "        for message in chat_history[-5:]:  # Use only last 5 messages for brevity\n",
    "            if hasattr(message, \"content\"):\n",
    "                role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                history_str += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        try:\n",
    "            # Ask the LLM if this can be answered without retrieval\n",
    "            response = self.llm.invoke(\n",
    "                prompt.format(chat_history=history_str, question=question)\n",
    "            )\n",
    "            \n",
    "            # If the response is \"NEEDS_RETRIEVAL\", we need to use retrieval\n",
    "            if \"NEEDS_RETRIEVAL\" in response:\n",
    "                return False, None\n",
    "            else:\n",
    "                logger.info(\"Question can be answered from conversation history\")\n",
    "                return True, response\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error determining if retrieval needed: {e}\")\n",
    "            return False, None  \n",
    "    \n",
    "    def generate_final_answer(\n",
    "        self, \n",
    "        question: str, \n",
    "        docs: List[RetrievedDocument], \n",
    "        max_docs: int = 5\n",
    "    ) -> str:\n",
    "        \"\"\"Generates the final answer from retrieved documents with LLM fallback.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            docs: List of retrieved documents\n",
    "            max_docs: Maximum number of documents to include\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # First, evaluate if the documents are sufficient\n",
    "        is_sufficient, confidence = self.evaluate_document_relevance(question, docs)\n",
    "        \n",
    "        # If documents are insufficient, use LLM fallback\n",
    "        if not is_sufficient or confidence < 0.4:  # Adjust threshold as needed\n",
    "            console.print(\"[yellow]Retrieved documents insufficient. Using LLM fallback...[/yellow]\")\n",
    "            return self.generate_llm_answer(question)\n",
    "        \n",
    "        # Otherwise, continue with RAG as before\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "                \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful scientific research assistant analyzing scientific papers. \n",
    "            Use the following retrieved context chunks to answer the user's question thoroughly.\n",
    "            \n",
    "            Guidelines:\n",
    "            - Focus on providing accurate information from the papers\n",
    "            - Synthesize information across documents when appropriate\n",
    "            - Cite the sources of information in your answer (e.g., \"According to Document 1...\")\n",
    "            - If the information isn't in the context, indicate what's missing rather than making up information\n",
    "            - If the context contains conflicting information, highlight the disagreement and possible reasons\n",
    "            - Maintain scientific accuracy above all else\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Format document content with metadata\n",
    "        context_pieces = []\n",
    "        for i, doc in enumerate(docs[:max_docs]):\n",
    "            chunk = f\"Document {i+1} [Relevance: {doc.score:.2f}]:\\n{doc.document.page_content}\"\n",
    "            \n",
    "            # Add metadata if available\n",
    "            if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                source = doc.document.metadata.get('source', 'Unknown')\n",
    "                chunk += f\"\\nSource: {source}\"\n",
    "                \n",
    "            context_pieces.append(chunk)\n",
    "            \n",
    "        context = \"\\n\\n---\\n\\n\".join(context_pieces)\n",
    "        \n",
    "        try:\n",
    "            # Generate answer\n",
    "            answer = (prompt | self.llm).invoke({\n",
    "                \"question\": question, \n",
    "                \"context\": context,\n",
    "                \"chat_history\": chat_context\n",
    "            })\n",
    "            \n",
    "            # Save the QA pair to memory\n",
    "            self.memory.chat_memory.add_messages([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=answer)\n",
    "            ])\n",
    "            \n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating answer: {e}\")\n",
    "            return \"I'm sorry, I encountered an error while generating your answer. Please try rephrasing your question.\"\n",
    "    \n",
    "    def generate_combined_answer(\n",
    "        self, \n",
    "        question: str, \n",
    "        query_results: Dict[str, List[RetrievedDocument]]\n",
    "    ) -> str:\n",
    "        \"\"\"Generates a combined answer from multiple query results with LLM fallback.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            query_results: Dictionary mapping queries to retrieved documents\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Combine all relevant documents from different queries\n",
    "        all_docs = []\n",
    "        for query, docs in query_results.items():\n",
    "            all_docs.extend(docs[:self.max_docs_per_query])\n",
    "        \n",
    "        # Evaluate if the documents are sufficient\n",
    "        is_sufficient, confidence = self.evaluate_document_relevance(question, all_docs)\n",
    "        \n",
    "        # If documents are insufficient, use LLM fallback\n",
    "        if not is_sufficient or confidence < 0.2:  # Adjust threshold as needed\n",
    "            console.print(\"[yellow]Retrieved documents insufficient. Using LLM fallback...[/yellow]\")\n",
    "            return self.generate_llm_answer(question)\n",
    "        \n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        # Combine all context sections\n",
    "        all_context_sections = []\n",
    "        \n",
    "        for query, docs in query_results.items():\n",
    "            # Use only the top N documents for each query\n",
    "            docs_for_query = docs[:self.max_docs_per_query]\n",
    "            if docs_for_query:\n",
    "                all_context_sections.append(f\"Results for query: '{query}'\")\n",
    "                for i, doc in enumerate(docs_for_query, 1):\n",
    "                    section = f\"Document {i} [Relevance: {doc.score:.2f}]:\\n{doc.document.page_content}\"\n",
    "                    \n",
    "                    # Add metadata if available\n",
    "                    if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                        source = doc.document.metadata.get('source', 'Unknown')\n",
    "                        section += f\"\\nSource: {source}\"\n",
    "                        \n",
    "                    all_context_sections.append(section)\n",
    "        \n",
    "        # Join all context sections\n",
    "        combined_context = \"\\n\\n---\\n\\n\".join(all_context_sections)\n",
    "        \n",
    "        # Create a prompt that emphasizes synthesizing information across different query results\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful scientific research assistant analyzing scientific papers. \n",
    "            The user's question has been reformulated in several ways, and each formulation returned different documents.\n",
    "            \n",
    "            Guidelines:\n",
    "            - Synthesize information across ALL retrieved documents to provide a comprehensive answer\n",
    "            - Compare and contrast findings from different sources\n",
    "            - Highlight the most relevant information from each source\n",
    "            - Cite the specific documents you're referencing (e.g., \"According to the paper in Document 3...\")\n",
    "            - If information conflicts across sources, explain the different perspectives\n",
    "            - If the information isn't in the context, indicate what's missing\n",
    "            - Maintain scientific accuracy above all else\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Context from multiple query formulations:\n",
    "            {context}\n",
    "            \n",
    "            Original Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Generate answer\n",
    "            answer = (prompt | self.llm).invoke({\n",
    "                \"question\": question, \n",
    "                \"context\": combined_context,\n",
    "                \"chat_history\": chat_context\n",
    "            })\n",
    "            \n",
    "            # Save the QA pair to memory\n",
    "            self.memory.chat_memory.add_messages([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=answer)\n",
    "            ])\n",
    "            \n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating combined answer: {e}\")\n",
    "            return \"I'm sorry, I encountered an error while generating your answer. Please try rephrasing your question.\"\n",
    "            \n",
    "    def evaluate_document_relevance(self, question: str, docs: List[RetrievedDocument]) -> Tuple[bool, float]:\n",
    "        \"\"\"Evaluates whether documents are sufficient to answer the question.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            docs: List of retrieved documents\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (is_sufficient, confidence_score)\n",
    "        \"\"\"\n",
    "        if not docs:\n",
    "            return False, 0.0\n",
    "            \n",
    "        # If all docs have very low scores, they're likely not relevant\n",
    "        avg_score = sum(doc.score for doc in docs) / len(docs)\n",
    "        \n",
    "        # Create a prompt to evaluate document relevance\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are evaluating whether a set of retrieved documents contains information \n",
    "        to answer a user's question.\n",
    "        \n",
    "        User Question: {question}\n",
    "        \n",
    "        Retrieved Information:\n",
    "        {doc_summaries}\n",
    "        \n",
    "        Please analyze whether the retrieved documents contain enough information to answer the question.\n",
    "        Respond with a JSON object:\n",
    "        {{\n",
    "          \"is_sufficient\": true/false,\n",
    "          \"confidence\": <number between 0-1>,\n",
    "          \"explanation\": \"<brief explanation>\"\n",
    "        }}\n",
    "        \n",
    "        Only respond with the JSON object.\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create short summaries of each document\n",
    "        doc_summaries = []\n",
    "        for i, doc in enumerate(docs[:3]):  # Use top 3 docs for evaluation\n",
    "            summary = doc.document.page_content[:200] + \"...\" if len(doc.document.page_content) > 200 else doc.document.page_content\n",
    "            doc_summaries.append(f\"Document {i+1} [Score: {doc.score:.2f}]: {summary}\")\n",
    "        \n",
    "        doc_summaries_text = \"\\n\\n\".join(doc_summaries)\n",
    "        \n",
    "        try:\n",
    "            # Parse the LLM response as JSON\n",
    "            response = self.llm.invoke(\n",
    "                prompt.format(question=question, doc_summaries=doc_summaries_text)\n",
    "            )\n",
    "            \n",
    "            # Extract JSON from the response\n",
    "            json_match = re.search(r'\\{.*\\}', response.replace('\\n', ' '), re.DOTALL)\n",
    "            if json_match:\n",
    "                result = json.loads(json_match.group(0))\n",
    "                is_sufficient = result.get(\"is_sufficient\", False)\n",
    "                confidence = result.get(\"confidence\", 0.0)\n",
    "                \n",
    "                logger.info(f\"Document relevance: Sufficient={is_sufficient}, Confidence={confidence}\")\n",
    "                logger.info(f\"Explanation: {result.get('explanation', 'No explanation provided')}\")\n",
    "                \n",
    "                return is_sufficient, confidence\n",
    "            else:\n",
    "                # If JSON parsing fails, use heuristics based on average score\n",
    "                return avg_score > self.relevance_threshold, avg_score\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating document relevance: {e}\")\n",
    "            # Fall back to using avg score\n",
    "            return avg_score > self.relevance_threshold, avg_score\n",
    "\n",
    "    def generate_llm_answer(self, question: str) -> str:\n",
    "        \"\"\"Generates an answer directly from the LLM when documents aren't sufficient.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful scientific research assistant. \n",
    "            \n",
    "            The user has asked a question, but we couldn't find relevant documents in our scientific database.\n",
    "            Since this appears to be a question you can answer from your general knowledge, please provide\n",
    "            a helpful response.\n",
    "            \n",
    "            Guidelines:\n",
    "            - Be clear that you're answering from general knowledge rather than specific papers\n",
    "            - Provide accurate, factual information\n",
    "            - If the question is about a very specialized scientific topic that requires references to papers,\n",
    "              indicate that you lack specific citations but can provide general information\n",
    "            - If it's a basic concept, provide a thorough explanation\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Generate answer\n",
    "            answer = (prompt | self.llm).invoke({\n",
    "                \"question\": question,\n",
    "                \"chat_history\": chat_context\n",
    "            })\n",
    "            \n",
    "            # Save the QA pair to memory\n",
    "            self.memory.chat_memory.add_messages([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=answer)\n",
    "            ])\n",
    "            \n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating LLM answer: {e}\")\n",
    "            return \"I'm sorry, I couldn't find relevant information in my scientific database and encountered an error trying to provide a general answer. Please try rephrasing your question.\"\n",
    "    \n",
    "    def ask_with_memory(self, question: str, num_results: int = 5) -> str:\n",
    "        \"\"\"Uses conversation memory to process follow-up questions.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            num_results: Number of results to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # First, check if this is a question we can answer directly from memory\n",
    "            can_direct_answer, direct_answer = self.can_answer_without_retrieval(question)\n",
    "            if can_direct_answer:\n",
    "                console.print(\"[bold green]Question can be answered directly from conversation history...[/bold green]\")\n",
    "                # Save the QA pair to memory manually\n",
    "                self.memory.chat_memory.add_messages([\n",
    "                    HumanMessage(content=question),\n",
    "                    AIMessage(content=direct_answer)\n",
    "                ])\n",
    "                return direct_answer\n",
    "# Otherwise, use the full query improvement pipeline\n",
    "            \n",
    "            # Step 1: Rate the query\n",
    "            console.print(\"\\n[bold cyan]Evaluating your query (with memory context)...[/bold cyan]\")\n",
    "            rating_result = self.rate_query(question)\n",
    "            \n",
    "            rating_color = \"green\" if rating_result['rating'] >= 4 else \"yellow\" if rating_result['rating'] >= 3 else \"red\"\n",
    "            console.print(f\"[bold {rating_color}]Query Rating: {rating_result['rating']}/5[/bold {rating_color}]\")\n",
    "            console.print(f\"Explanation: {rating_result['explanation']}\")\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 2: Suggest rewrites if the rating is less than perfect\n",
    "            rewritten_queries = []\n",
    "            if rating_result[\"rating\"] < 5:\n",
    "                console.print(\"[bold cyan]Generating improved query variations...[/bold cyan]\")\n",
    "                # Get chat history for context in rewrites\n",
    "                chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "                rewritten_queries = self.suggest_rewrites(question, chat_history)\n",
    "                console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 3: Present options to the user\n",
    "            query_options = self.present_query_options(question, rewritten_queries)\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 4: Get user selection\n",
    "            console.print(\"[bold cyan]Select queries to use:[/bold cyan]\")\n",
    "            console.print(\"Enter the numbers of the queries you want to use (comma-separated, e.g., '0,2,3')\")\n",
    "            selected_indices_input = input(\"> \")\n",
    "            \n",
    "            try:\n",
    "                selected_indices = [int(idx.strip()) for idx in selected_indices_input.split(\",\")]\n",
    "            except ValueError:\n",
    "                console.print(\"[bold red]Invalid input. Using original query only.[/bold red]\")\n",
    "                selected_indices = [0]  # Default to original query\n",
    "            \n",
    "            # Get the selected queries\n",
    "            selected_queries = []\n",
    "            for idx in selected_indices:\n",
    "                if idx == 0:  # Original query\n",
    "                    selected_queries.append(question)\n",
    "                elif 0 < idx <= len(rewritten_queries):  # Rewritten query\n",
    "                    query_text = rewritten_queries[idx-1]\n",
    "                    selected_queries.append(query_text)\n",
    "            \n",
    "            if not selected_queries:\n",
    "                console.print(\"[bold red]No valid queries selected. Using original query.[/bold red]\")\n",
    "                selected_queries = [question]\n",
    "                \n",
    "            console.print(f\"\\n[bold green]Selected {len(selected_queries)} queries for retrieval.[/bold green]\")\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 5: Retrieve documents\n",
    "            console.print(\"[bold cyan]Retrieving relevant documents...[/bold cyan]\")\n",
    "            \n",
    "            if len(selected_queries) > 1:\n",
    "                # If multiple queries were selected, use the multi-query approach\n",
    "                docs, query_docs_map = self.retrieve_documents(selected_queries, k=num_results)\n",
    "                \n",
    "                # Generate a combined answer\n",
    "                console.print(\"\\n[bold cyan]Generating combined answer with conversation+docs context...[/bold cyan]\")\n",
    "                answer = self.generate_combined_answer(question, query_docs_map)\n",
    "            else:\n",
    "                # If only one query was selected, use the standard approach\n",
    "                try:\n",
    "                    retrieved_docs = []\n",
    "                    results = self.vector_db.similarity_search_with_relevance_scores(selected_queries[0], k=num_results)\n",
    "                    \n",
    "                    for i, (doc, score) in enumerate(results, 1):\n",
    "                        retrieved_docs.append(RetrievedDocument(\n",
    "                            document=doc,\n",
    "                            score=score,\n",
    "                            query=selected_queries[0],\n",
    "                            rank=i\n",
    "                        ))\n",
    "                    \n",
    "                    console.print(f\"[bold green]Retrieved {len(retrieved_docs)} documents[/bold green]\")\n",
    "                    \n",
    "                    # Show previews of the retrieved documents\n",
    "                    console.print(\"\\n[bold cyan]Top retrieved documents:[/bold cyan]\")\n",
    "                    for i, doc in enumerate(retrieved_docs[:5], 1):\n",
    "                        score_color = \"green\" if doc.score > 0.8 else \"yellow\" if doc.score > 0.6 else \"red\"\n",
    "                        console.print(f\"{i}. [bold {score_color}]Score: {doc.score:.4f}[/bold {score_color}]\")\n",
    "                        \n",
    "                        preview = doc.document.page_content[:150] + \"...\" if len(doc.document.page_content) > 150 else doc.document.page_content\n",
    "                        console.print(f\"   Preview: {preview}\")\n",
    "                        \n",
    "                        if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                            console.print(f\"   Source: {doc.document.metadata.get('source', 'Unknown')}\")\n",
    "                    \n",
    "                    # Generate answer with memory context\n",
    "                    console.print(\"\\n[bold cyan]Generating answer with conversation + doc context...[/bold cyan]\")\n",
    "                    answer = self.generate_final_answer(question, retrieved_docs)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in retrieval: {e}\")\n",
    "                    return f\"I'm sorry, I encountered an error while retrieving documents: {str(e)}\"\n",
    "            \n",
    "            return answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in ask_with_memory: {e}\")\n",
    "            return f\"I'm sorry, I encountered an error while processing your question: {str(e)}\"\n",
    "\n",
    "    def search_with_query_feedback(self, query: str, num_results: int = 5) -> str:\n",
    "        \"\"\"Main pipeline that processes a query and returns an answer.\"\"\"\n",
    "        # Step 1: Rate the query\n",
    "        console.print(\"\\n[bold cyan]Evaluating your query...[/bold cyan]\")\n",
    "        rating_result = self.rate_query(query)\n",
    "            \n",
    "        rating_color = \"green\" if rating_result['rating'] >= 4 else \"yellow\" if rating_result['rating'] >= 3 else \"red\"\n",
    "        console.print(f\"[bold {rating_color}]Query Rating: {rating_result['rating']}/5[/bold {rating_color}]\")\n",
    "        console.print(f\"Explanation: {rating_result['explanation']}\")\n",
    "        console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "        # Step 2: Suggest rewrites if the rating is less than perfect\n",
    "        rewritten_queries = []\n",
    "        if rating_result[\"rating\"] < 5:\n",
    "            console.print(\"[bold cyan]Generating improved query variations...[/bold cyan]\")\n",
    "            rewritten_queries = self.suggest_rewrites(query)\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 3: Present options to the user\n",
    "        query_options = self.present_query_options(query, rewritten_queries)\n",
    "        console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 4: Get user selection\n",
    "        console.print(\"[bold cyan]Select queries to use:[/bold cyan]\")\n",
    "        console.print(\"Enter the numbers of the queries you want to use (comma-separated, e.g., '0,2,3')\")\n",
    "        selected_indices_input = input(\"> \")\n",
    "            \n",
    "        try:\n",
    "            selected_indices = [int(idx.strip()) for idx in selected_indices_input.split(\",\")]\n",
    "        except ValueError:\n",
    "            console.print(\"[bold red]Invalid input. Using original query only.[/bold red]\")\n",
    "            selected_indices = [0]  # Default to original query\n",
    "        \n",
    "        # Get the selected queries\n",
    "        selected_queries = []\n",
    "        for idx in selected_indices:\n",
    "            if idx == 0:  # Original query\n",
    "                selected_queries.append(query)\n",
    "            elif 0 < idx <= len(rewritten_queries):  # Rewritten query\n",
    "                query_text = rewritten_queries[idx-1]\n",
    "                selected_queries.append(query_text)\n",
    "                \n",
    "        if not selected_queries:\n",
    "            console.print(\"[bold red]No valid queries selected. Using original query.[/bold red]\")\n",
    "            selected_queries = [query]\n",
    "            \n",
    "        console.print(f\"\\n[bold green]Selected {len(selected_queries)} queries for retrieval.[/bold green]\")\n",
    "        console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Step 5: Retrieve documents\n",
    "        console.print(\"[bold cyan]Retrieving relevant documents...[/bold cyan]\")\n",
    "        \n",
    "        if len(selected_queries) > 1:\n",
    "            # If multiple queries were selected, use the multi-query approach\n",
    "            print(\"LOG123: multiple queries this is working\")\n",
    "            docs, query_docs_map = self.retrieve_documents(selected_queries, k=num_results)\n",
    "            # Generate a combined answer\n",
    "            console.print(\"\\n[bold cyan]Generating combined answer with documents context...[/bold cyan]\")\n",
    "            answer = self.generate_combined_answer(query, query_docs_map)\n",
    "        else:\n",
    "            # If only one query was selected, use the standard approach\n",
    "            print(\"LOG123: only this is working\")\n",
    "            try:\n",
    "                retrieved_docs = []\n",
    "                results = self.vector_db.similarity_search_with_relevance_scores(selected_queries[0], k=num_results)\n",
    "                \n",
    "                for i, (doc, score) in enumerate(results, 1):\n",
    "                    retrieved_docs.append(RetrievedDocument(\n",
    "                        document=doc,\n",
    "                        score=score,\n",
    "                        query=selected_queries[0],\n",
    "                        rank=i\n",
    "                    ))\n",
    "                \n",
    "                console.print(f\"[bold green]Retrieved {len(retrieved_docs)} documents[/bold green]\")\n",
    "                \n",
    "                # Show previews of the retrieved documents\n",
    "                console.print(\"\\n[bold cyan]Top retrieved documents:[/bold cyan]\")\n",
    "                for i, doc in enumerate(retrieved_docs[:5], 1):\n",
    "                    score_color = \"green\" if doc.score > 0.8 else \"yellow\" if doc.score > 0.6 else \"red\"\n",
    "                    console.print(f\"{i}. [bold {score_color}]Score: {doc.score:.4f}[/bold {score_color}]\")\n",
    "                    \n",
    "                    preview = doc.document.page_content[:150] + \"...\" if len(doc.document.page_content) > 150 else doc.document.page_content\n",
    "                    console.print(f\"   Preview: {preview}\")\n",
    "                    \n",
    "                    if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                        console.print(f\"   Source: {doc.document.metadata.get('source', 'Unknown')}\")\n",
    "                \n",
    "                # Generate answer with memory context\n",
    "                console.print(\"\\n[bold cyan]Generating answer with document context...[/bold cyan]\")\n",
    "                answer = self.generate_final_answer(query, retrieved_docs)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in retrieval: {e}\")\n",
    "                return f\"I'm sorry, I encountered an error while retrieving documents: {str(e)}\"\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def process_query(self, query: str, use_memory: bool = False, num_results: int = 5) -> str:\n",
    "        \"\"\"Main entry point that decides whether to use memory or the full pipeline.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            use_memory: Whether to explicitly use memory mode\n",
    "            num_results: Number of results to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Check if this might be a follow-up question that should use memory\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        should_use_memory = use_memory or (chat_history and self._is_likely_followup(query))\n",
    "        print(\"should_use_memory\",should_use_memory)\n",
    "        if should_use_memory:\n",
    "            console.print(\"[bold cyan]Using conversation memory to process this query...[/bold cyan]\")\n",
    "            return self.ask_with_memory(query, num_results)\n",
    "        else:\n",
    "            console.print(\"[bold cyan]Using full query improvement pipeline...[/bold cyan]\")\n",
    "            return self.search_with_query_feedback(query, num_results)\n",
    "    \n",
    "    def _is_likely_followup(self, query: str) -> bool:\n",
    "        \"\"\"Heuristically determines if a query is likely a follow-up question.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating if this is likely a follow-up\n",
    "        \"\"\"\n",
    "        # Look for pronouns, references, and questions that seem to refer to previous context\n",
    "        followup_indicators = [\n",
    "            # Pronouns\n",
    "            \"it\", \"this\", \"that\", \"they\", \"them\", \"these\", \"those\",\n",
    "            # Reference terms\n",
    "            \"previous\", \"earlier\", \"above\", \"mentioned\", \"last\", \"former\",\n",
    "            # Follow-up phrases\n",
    "            \"what about\", \"how about\", \"tell me more\", \"elaborate\", \"clarify\", \"explain further\",\n",
    "            \"can you expand\", \"additionally\", \"furthermore\", \"also\", \"related to that\",\n",
    "            # Short questions that likely need context\n",
    "            \"why\", \"how does\", \"can you explain\", \"what does\", \"what is\", \"who is\"\n",
    "        ]\n",
    "        \n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Check for any of the indicators\n",
    "        if any(indicator in query_lower for indicator in followup_indicators):\n",
    "            return True\n",
    "            \n",
    "        # # Check for very short queries (likely follow-ups)\n",
    "        # if len(query.split()) < 4:\n",
    "        #     return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    def clear_memory(self) -> None:\n",
    "        \"\"\"Clears the conversation memory.\"\"\"\n",
    "        self.memory.clear()\n",
    "        console.print(\"[bold green]Conversation memory cleared.[/bold green]\")\n",
    "        \n",
    "    def get_chat_history(self) -> str:\n",
    "        \"\"\"Returns the formatted chat history.\n",
    "        \n",
    "        Returns:\n",
    "            String representation of chat history\n",
    "        \"\"\"\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        if not chat_history:\n",
    "            return \"No conversation history.\"\n",
    "            \n",
    "        history_str = \"\"\n",
    "        for i, message in enumerate(chat_history):\n",
    "            if hasattr(message, \"content\"):\n",
    "                role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                history_str += f\"{role}: {message.content}\\n\\n\"\n",
    "                \n",
    "        return history_str\n",
    "\n",
    "    def Load_query(self, query):\n",
    "        \"\"\"\n",
    "        Load research papers from arXiv based on the given query and store them in the temporary database.\n",
    "        Returns the top 3 papers information.\n",
    "        \"\"\"\n",
    "        console.print(f\"[bold cyan]Searching arXiv for papers related to:[/bold cyan] {query}\")\n",
    "        \n",
    "        try:\n",
    "            # Use the working ArxivLoader from langchain_community\n",
    "            loader = ArxivLoader(\n",
    "                query=query,\n",
    "                load_max_docs=3,  # Get top 3 papers\n",
    "                load_all_available_meta=True\n",
    "            )\n",
    "            \n",
    "            documents = loader.load()\n",
    "            console.print(f\"[green]Found {len(documents)} relevant papers[/green]\")\n",
    "            \n",
    "            # Download PDFs for more complete content\n",
    "            enhanced_docs = self.download_pdf_content(documents)\n",
    "            \n",
    "            # Store documents in temporary database\n",
    "            paper_ids = self.StoreInDB(enhanced_docs)\n",
    "            \n",
    "            # Display basic information about the loaded papers\n",
    "            self.display_paper_summaries(enhanced_docs)\n",
    "            \n",
    "            return paper_ids\n",
    "        \n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Error loading papers: {str(e)}[/bold red]\")\n",
    "            # # Fall back to simulated papers if needed\n",
    "            # if self.allow_fallback:\n",
    "            #     console.print(\"[yellow]Using fallback data for demonstration purposes.[/yellow]\")\n",
    "            #     return self.create_fallback_papers(query)\n",
    "            return []\n",
    "    \n",
    "    def download_pdf_content(self, documents):\n",
    "        \"\"\"\n",
    "        Enhance documents by downloading the full PDF content when available.\n",
    "        \"\"\"\n",
    "        enhanced_docs = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            try:\n",
    "                # Get metadata\n",
    "                metadata = doc.metadata\n",
    "                title = metadata.get(\"Title\", \"Untitled\")\n",
    "                console.print(f\"[cyan]Processing paper: {title}[/cyan]\")\n",
    "                \n",
    "                # Try to find arXiv ID from page content\n",
    "                arxiv_id = None\n",
    "                match = re.search(r'arXiv:(\\d{4}\\.\\d{5})', doc.page_content)\n",
    "                \n",
    "                if match:\n",
    "                    arxiv_id = match.group(1)\n",
    "                else:\n",
    "                    # Try to extract from Entry ID\n",
    "                    entry_id = metadata.get(\"Entry ID\", \"\")\n",
    "                    id_match = re.search(r'abs/(\\d{4}\\.\\d{5})', entry_id)\n",
    "                    if id_match:\n",
    "                        arxiv_id = id_match.group(1)\n",
    "                \n",
    "                if arxiv_id:\n",
    "                    # Generate PDF URL\n",
    "                    pdf_url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "                    console.print(f\"[cyan]Found PDF URL: {pdf_url}[/cyan]\")\n",
    "                    \n",
    "                    # Create temp directory if it doesn't exist\n",
    "                    temp_dir = \"./temp_pdfs\"\n",
    "                    os.makedirs(temp_dir, exist_ok=True)\n",
    "                    \n",
    "                    # Download PDF\n",
    "                    pdf_filename = f\"{temp_dir}/{arxiv_id}.pdf\"\n",
    "                    \n",
    "                    # Check if file already exists\n",
    "                    if not os.path.exists(pdf_filename):\n",
    "                        console.print(f\"[cyan]Downloading PDF...[/cyan]\")\n",
    "                        response = requests.get(pdf_url, timeout=30)\n",
    "                        with open(pdf_filename, 'wb') as f:\n",
    "                            f.write(response.content)\n",
    "                    \n",
    "                    # Load PDF content\n",
    "                    console.print(f\"[cyan]Extracting content from PDF...[/cyan]\")\n",
    "                    pdf_loader = PyPDFLoader(pdf_filename)\n",
    "                    pdf_docs = pdf_loader.load()\n",
    "                    \n",
    "                    # Create enhanced document with PDF content\n",
    "                    full_content = doc.page_content + \"\\n\\n\" + \"\\n\\n\".join([pdf_doc.page_content for pdf_doc in pdf_docs])\n",
    "                    \n",
    "                    # Update metadata with PDF info\n",
    "                    updated_metadata = metadata.copy()\n",
    "                    updated_metadata[\"pdf_url\"] = pdf_url\n",
    "                    updated_metadata[\"pdf_path\"] = pdf_filename\n",
    "                    updated_metadata[\"pdf_pages\"] = len(pdf_docs)\n",
    "                    \n",
    "                    enhanced_doc = Document(page_content=full_content, metadata=updated_metadata)\n",
    "                    enhanced_docs.append(enhanced_doc)\n",
    "                    \n",
    "                else:\n",
    "                    # If PDF can't be found, use original document\n",
    "                    console.print(f\"[yellow]Could not extract arXiv ID for {title}. Using abstract only.[/yellow]\")\n",
    "                    enhanced_docs.append(doc)\n",
    "            \n",
    "            except Exception as e:\n",
    "                console.print(f\"[yellow]Error enhancing document: {str(e)}. Using abstract only.[/yellow]\")\n",
    "                enhanced_docs.append(doc)\n",
    "        \n",
    "        return enhanced_docs\n",
    "    \n",
    "    def StoreInDB(self, documents):\n",
    "        \"\"\"\n",
    "        Store loaded documents in the temporary vector database.\n",
    "        Returns list of document IDs.\n",
    "        \"\"\"\n",
    "        paper_ids = []\n",
    "        \n",
    "        console.print(\"[bold cyan]Storing papers in temporary database...[/bold cyan]\")\n",
    "        \n",
    "        try:\n",
    "            # Split documents into chunks for better retrieval\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000, \n",
    "                chunk_overlap=200\n",
    "            )\n",
    "            \n",
    "            for doc in documents:\n",
    "                # Create document chunks\n",
    "                chunks = text_splitter.split_documents([doc])\n",
    "                \n",
    "                # Create unique ID for the paper\n",
    "                paper_id = f\"paper_{uuid.uuid4().hex[:8]}\"\n",
    "                paper_ids.append(paper_id)\n",
    "                \n",
    "                # Add metadata to each chunk and sanitize it\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    # Create clean metadata dictionary\n",
    "                    clean_metadata = {\n",
    "                        \"paper_id\": paper_id,\n",
    "                        \"chunk_id\": i,\n",
    "                        \"title\": doc.metadata.get(\"Title\", \"Untitled\"),\n",
    "                        \"authors\": doc.metadata.get(\"Authors\", \"Unknown\"),\n",
    "                        \"published\": doc.metadata.get(\"Published\", \"Unknown\")\n",
    "                    }\n",
    "                    \n",
    "                    # Ensure all values are valid types (str, int, float, bool)\n",
    "                    for key, value in clean_metadata.items():\n",
    "                        if value is None:\n",
    "                            clean_metadata[key] = \"None\"  # Convert None to string\n",
    "                        elif not isinstance(value, (str, int, float, bool)):\n",
    "                            clean_metadata[key] = str(value)  # Convert complex types to string\n",
    "                    \n",
    "                    # Replace existing metadata with clean version\n",
    "                    chunk.metadata = clean_metadata\n",
    "                \n",
    "                # Add to vector database\n",
    "                self.vector_db.add_documents(chunks)\n",
    "            \n",
    "            # Persist the temporary database\n",
    "            self.vector_db.persist()\n",
    "            console.print(f\"[green]Successfully stored {len(documents)} papers in database[/green]\")\n",
    "            # Fixed typo from 'onsole' to 'console' and removed debug line\n",
    "            return paper_ids\n",
    "        \n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Error storing documents: {str(e)}[/bold red]\")\n",
    "            return []\n",
    "    \n",
    "    def display_paper_summaries(self, documents):\n",
    "        \"\"\"Display a summary of each loaded paper.\"\"\"\n",
    "        console.print(\"\\n[bold cyan]Loaded Papers:[/bold cyan]\")\n",
    "        \n",
    "        for i, doc in enumerate(documents):\n",
    "            title = doc.metadata.get(\"Title\", \"Untitled\")\n",
    "            authors = doc.metadata.get(\"Authors\", \"Unknown\")\n",
    "            published = doc.metadata.get(\"Published\", \"Unknown\")\n",
    "            \n",
    "            console.print(f\"\\n[bold]{i+1}. {title}[/bold]\")\n",
    "            console.print(f\"   Authors: {authors}\")\n",
    "            console.print(f\"   Published: {published}\")\n",
    "            \n",
    "            # Generate a brief summary of the paper\n",
    "            summary = self.generate_paper_summary(doc)\n",
    "            console.print(f\"   Summary: {summary}\")\n",
    "    \n",
    "    def generate_paper_summary(self, document):\n",
    "        \"\"\"Generate a concise summary of a paper using the LLM.\"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            Generate a concise 5-sentence summary of the following research paper:\n",
    "            \n",
    "            Title: {document.metadata.get('Title', 'Untitled')}\n",
    "            Abstract: {document.page_content[:500]}...\n",
    "            \n",
    "            Summary:\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.llm.predict(prompt)\n",
    "            return response.strip()\n",
    "        except:\n",
    "            return \"Summary not available.\"\n",
    "    \n",
    "    def query_temp_database(self, query, paper_ids=None, k=5):\n",
    "        \"\"\"\n",
    "        Query the temporary database with the given query.\n",
    "        \n",
    "        Args:\n",
    "            query: The query string\n",
    "            paper_ids: Optional list of paper IDs to restrict the search to\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of relevant document chunks\n",
    "        \"\"\"\n",
    "        console.print(f\"[bold cyan]Searching temporary database for:[/bold cyan] {query}\")\n",
    "        \n",
    "        try:\n",
    "            # Create filter for specific papers if provided\n",
    "            filter_dict = None\n",
    "            if paper_ids:\n",
    "                filter_dict = {\"paper_id\": {\"$in\": paper_ids}}\n",
    "            \n",
    "            # Query the vector database\n",
    "            search_results = self.vector_db.similarity_search(\n",
    "                query=query,\n",
    "                k=k,\n",
    "                filter=filter_dict\n",
    "            )\n",
    "            \n",
    "            console.print(f\"[green]Found {len(search_results)} relevant sections[/green]\")\n",
    "            return search_results\n",
    "        \n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Error querying database: {str(e)}[/bold red]\")\n",
    "            return []\n",
    "    \n",
    "    def answer_research_question(self, query, context_docs=None):\n",
    "        \"\"\"\n",
    "        Answer a research question using context from the temporary database.\n",
    "        \n",
    "        Args:\n",
    "            query: The research question\n",
    "            context_docs: Optional pre-retrieved context documents\n",
    "            \n",
    "        Returns:\n",
    "            Detailed answer to the research question\n",
    "        \"\"\"\n",
    "        # If context not provided, retrieve it\n",
    "        if not context_docs:\n",
    "            context_docs = self.query_temp_database(query, k=5)\n",
    "        \n",
    "        if not context_docs:\n",
    "            return \"I couldn't find relevant information to answer your question.\"\n",
    "        \n",
    "        # Prepare context\n",
    "        context_text = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(context_docs)])\n",
    "        \n",
    "        # Prepare sources\n",
    "        sources = []\n",
    "        for doc in context_docs:\n",
    "            paper_title = doc.metadata.get(\"title\", \"Unknown Title\")\n",
    "            authors = doc.metadata.get(\"authors\", \"Unknown Authors\")\n",
    "            source = f\"- {paper_title} by {authors}\"\n",
    "            if source not in sources:\n",
    "                sources.append(source)\n",
    "        \n",
    "        # Generate the answer\n",
    "        prompt = f\"\"\"\n",
    "        You are a helpful research assistant. Answer the following question based on the provided research paper extracts.\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Here are relevant extracts from research papers:\n",
    "        {context_text}\n",
    "        \n",
    "        Provide a comprehensive answer, citing specific information from the papers.\n",
    "        Make sure to organize your response clearly and highlight key insights.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            answer = self.llm.predict(prompt)\n",
    "            \n",
    "            # Add sources at the end\n",
    "            final_answer = f\"{answer}\\n\\nSources:\\n\" + \"\\n\".join(sources)\n",
    "            return final_answer\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"Error generating answer: {str(e)}\"\n",
    "\n",
    "\n",
    "def main():    \n",
    "    \"\"\"Main function to run the research assistant.\"\"\"\n",
    "    try:\n",
    "        from langchain_community.llms import OpenAI\n",
    "        from langchain_community.vectorstores import Chroma\n",
    "        from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "        \n",
    "        # Check for API key\n",
    "        if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "            console.print(\"[bold red]ERROR: OPENAI_API_KEY environment variable not set.[/bold red]\")\n",
    "            console.print(f\"Please set your API key with: export OPENAI_API_KEY={os.environ['OPENAI_API_KEY']}\")\n",
    "            return\n",
    "            \n",
    "        # Load models and vector database\n",
    "        console.print(\"[bold cyan]Initializing SageRAG Research Assistant...[/bold cyan]\")\n",
    "        \n",
    "        console.print(\"Loading embedding model...\")\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        \n",
    "        console.print(\"Loading language model...\")\n",
    "        llm = OllamaLLM(model=\"llama3.2\")\n",
    "        \n",
    "        console.print(\"Connecting to vector database...\")\n",
    "\n",
    "        vector_db = Chroma(persist_directory= DB_DIR, embedding_function=embeddings)\n",
    "        \n",
    "        console.print(\"[bold green]Initialization complete![/bold green]\")\n",
    "        \n",
    "        # Initialize the research assistant\n",
    "        assistant = ResearchAssistant(llm=llm, vector_db=vector_db)\n",
    "        print(assistant.vector_db._collection.count())\n",
    "        # Display welcome message\n",
    "        console.print(Panel.fit(\n",
    "            Markdown(\"# SageRAG Research Assistant\\n\\nAsk questions about scientific papers in the database.\"),\n",
    "            title=\"Welcome\",\n",
    "            border_style=\"cyan\"\n",
    "        ))\n",
    "        \n",
    "        # Main interaction loop\n",
    "        while True:\n",
    "            console.print(\"\\n[bold cyan]Options:[/bold cyan]\")\n",
    "            console.print(\"1. Ask a question\")\n",
    "            console.print(\"2. View conversation history\")\n",
    "            console.print(\"3. Clear conversation history\")\n",
    "            console.print(\"4. Go for Advanced Research\")\n",
    "            console.print(\"5. Exit\")\n",
    "            \n",
    "            choice = input(\"\\nEnter your choice (1-5): \").strip()\n",
    "            \n",
    "            if choice == \"1\":\n",
    "                query = input(f\"\\n[bold]Enter your query:[/bold] \")\n",
    "                if not query.strip():\n",
    "                    console.print(f\"[yellow]Empty query. Please try again.[/yellow]\")\n",
    "                    continue\n",
    "                    \n",
    "                # Ask the user whether to use memory mode or full pipeline\n",
    "                use_memory_input = input(\"Use conversation memory? (y/n): \").lower()\n",
    "                use_memory = use_memory_input.startswith('y')\n",
    "                \n",
    "                # Process the query\n",
    "                console.print(f\"\\n[bold cyan]Processing your query...[/bold cyan]\")\n",
    "                answer = assistant.process_query(query, use_memory=use_memory)\n",
    "                \n",
    "                # Display the final answer nicely formatted\n",
    "                console.print(Panel(\n",
    "                    Markdown(answer),\n",
    "                    title=\"Answer\",\n",
    "                    border_style=\"green\",\n",
    "                    width=100\n",
    "                ))\n",
    "\n",
    "                console.print(\"\\n[bold cyan]Would you like to get an llm generated answer?[/bold cyan]\")\n",
    "                llm_answer = input(\"(y/n): \").lower()\n",
    "                llm_answer = llm_answer.startswith('y')\n",
    "                if(llm_answer):\n",
    "                    answer = assistant.generate_llm_answer(query)\n",
    "                \n",
    "                # Display the final answer nicely formatted\n",
    "                console.print(Panel(\n",
    "                    Markdown(answer),\n",
    "                    title=\"Answer\",\n",
    "                    border_style=\"green\",\n",
    "                    width=100\n",
    "                ))\n",
    "            \n",
    "                \n",
    "            elif choice == \"2\":\n",
    "                history = assistant.get_chat_history()\n",
    "                console.print(Panel(\n",
    "                    history if history else \"No conversation history.\",\n",
    "                    title=\"Conversation History\",\n",
    "                    border_style=\"blue\"\n",
    "                ))\n",
    "                \n",
    "            elif choice == \"3\":\n",
    "                assistant.clear_memory()\n",
    "\n",
    "            elif choice == \"4\":\n",
    "                query = input(f\"\\n[bold]Enter your query:[/bold] \")\n",
    "                if not query.strip():\n",
    "                    console.print(f\"[yellow]Empty query. Please try again.[/yellow]\")\n",
    "                    continue\n",
    "                \n",
    "                # Initialize the adv research assistant\n",
    "                vector_db_temp = Chroma(persist_directory= \"./tempDB\", embedding_function=embeddings)\n",
    "                advAssistant = ResearchAssistant(llm=llm, vector_db=vector_db_temp)\n",
    "                # Process the query\n",
    "                console.print(f\"\\n[bold cyan]Processing your query...[/bold cyan]\")\n",
    "                # Step 1: Rate the query\n",
    "                console.print(\"\\n[bold cyan]Evaluating your query (with memory context)...[/bold cyan]\")\n",
    "                rating_result = advAssistant.rate_query(query)\n",
    "                \n",
    "                rating_color = \"green\" if rating_result['rating'] >= 4 else \"yellow\" if rating_result['rating'] >= 3 else \"red\"\n",
    "                console.print(f\"[bold {rating_color}]Query Rating: {rating_result['rating']}/5[/bold {rating_color}]\")\n",
    "                console.print(f\"Explanation: {rating_result['explanation']}\")\n",
    "                console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "                \n",
    "                # Step 2: Suggest rewrites if the rating is less than perfect\n",
    "                rewritten_queries = []\n",
    "                if rating_result[\"rating\"] < 5:\n",
    "                    console.print(\"[bold cyan]Generating improved query variations...[/bold cyan]\")\n",
    "                    rewritten_queries = advAssistant.suggest_rewrites(query)\n",
    "                    console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "                \n",
    "                # Step 3: Present options to the user\n",
    "                query_options = advAssistant.present_query_options(query, rewritten_queries)\n",
    "                console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "                \n",
    "                # Step 4: Get user selection\n",
    "                console.print(\"[bold cyan]Select queries to use:[/bold cyan]\")\n",
    "                console.print(\"Enter the query you want to use (comma-separated, e.g., '0,2,3')\")\n",
    "                selected_index = input(\"> \")\n",
    "                try:\n",
    "                    idx = int(selected_index)\n",
    "                    if idx == 0:  # Original query\n",
    "                        selected_query = query\n",
    "                    elif 0 < idx <= len(rewritten_queries):  # Rewritten query\n",
    "                        selected_query = rewritten_queries[idx-1]\n",
    "                    else:\n",
    "                        console.print(\"[yellow]Invalid selection. Using original query.[/yellow]\")\n",
    "                        selected_query = query\n",
    "                except ValueError:\n",
    "                    console.print(\"[yellow]Invalid input. Using original query.[/yellow]\")\n",
    "                    selected_query = query\n",
    "                \n",
    "                # Step 5: Load papers based on selected query\n",
    "                paper_ids = advAssistant.Load_query(selected_query)\n",
    "                \n",
    "                if paper_ids:\n",
    "                    # Step 6: Interactive research loop\n",
    "                    console.print(advAssistant.vector_db)\n",
    "                    while True:\n",
    "                        research_question = input(\"\\n[bold]Ask a question about these papers (or type 'exit' to return):[/bold] \")\n",
    "                        if research_question.lower() in ['exit', 'quit', 'back']:\n",
    "                            break\n",
    "                        if not research_question.strip():\n",
    "                            console.print(f\"[yellow]Empty query. Please try again.[/yellow]\")\n",
    "                            continue\n",
    "                            \n",
    "                        # Ask the user whether to use memory mode or full pipeline\n",
    "                        use_memory_input = input(\"Use conversation memory? (y/n): \").lower()\n",
    "                        use_memory = use_memory_input.startswith('y')\n",
    "                        \n",
    "                        # Process the query\n",
    "                        console.print(f\"\\n[bold cyan]Processing your query...[/bold cyan]\")\n",
    "                        answer = advAssistant.process_query(research_question, use_memory=use_memory)\n",
    "                        \n",
    "                        # Display the final answer nicely formatted\n",
    "                        console.print(Panel(\n",
    "                            Markdown(answer),\n",
    "                            title=\"Answer\",\n",
    "                            border_style=\"green\",\n",
    "                            width=100\n",
    "                        ))\n",
    "                else:\n",
    "                    console.print(\"[yellow]No papers were loaded. Please try a different query.[/yellow]\")\n",
    "                            \n",
    "                print(advAssistant.vector_db._collection.count())\n",
    "            elif choice == \"5\":\n",
    "                console.print(\"[bold green]Thank you for using SageRAG Research Assistant. Goodbye![/bold green]\")\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                console.print(\"[yellow]Invalid choice. Please enter a number from 1-4.[/yellow]\")\n",
    "                \n",
    "    except ImportError as e:\n",
    "        console.print(f\"[bold red]Error: Missing required packages: {e}[/bold red]\")\n",
    "        console.print(\"Please install the required packages with pip:\")\n",
    "        console.print(\"pip install langchain langchain_community sentence-transformers rich openai chromadb\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]An unexpected error occurred: {e}[/bold red]\")\n",
    "        import traceback\n",
    "        console.print(traceback.format_exc())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cece5b-ab47-424d-98a2-38ac6c3f1b08",
   "metadata": {},
   "source": [
    "# Extra (ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d63484da-ba54-4e16-b709-3fe5a698a20d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'assistant' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(assistant\u001b[38;5;241m.\u001b[39mvector_db\u001b[38;5;241m.\u001b[39m_collection\u001b[38;5;241m.\u001b[39mcount())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'assistant' is not defined"
     ]
    }
   ],
   "source": [
    "print(assistant.vector_db._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c210793-86df-4964-83f0-6bfdcd142b4e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 20:47:27,133 - SageRAG - INFO - Initializing default embedding model\n",
      "2025-04-19 20:47:31,195 - SageRAG - INFO - Research Assistant initialized successfully\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter What is LLM in NLP?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Evaluating your query...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mEvaluating your query\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 20:47:41,148 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Query Rating: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">/</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mQuery Rating: \u001b[0m\u001b[1;32m4\u001b[0m\u001b[1;32m/\u001b[0m\u001b[1;32m5\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Explanation: The query is clear and concise, making it easy to understand the question. It contains specific \n",
       "technical terms <span style=\"font-weight: bold\">(</span>LLM<span style=\"font-weight: bold\">)</span> that are relevant to NLP, which should help retrieve accurate scientific content. However, it\n",
       "lacks specificity in terms of the context or application of LLM in NLP, which might lead to a broad search result \n",
       "set. Additionally, while it's not too vague, the query could be more effective with additional keywords to narrow \n",
       "down the search results.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Explanation: The query is clear and concise, making it easy to understand the question. It contains specific \n",
       "technical terms \u001b[1m(\u001b[0mLLM\u001b[1m)\u001b[0m that are relevant to NLP, which should help retrieve accurate scientific content. However, it\n",
       "lacks specificity in terms of the context or application of LLM in NLP, which might lead to a broad search result \n",
       "set. Additionally, while it's not too vague, the query could be more effective with additional keywords to narrow \n",
       "down the search results.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Generating improved query variations...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mGenerating improved query variations\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 20:47:43,686 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Available search queries:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mAvailable search queries:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">.</span> Original: What is LLM in NLP?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m0\u001b[0m\u001b[1m.\u001b[0m Original: What is LLM in NLP?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Confidence: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;32mConfidence: \u001b[0m\u001b[1;32m1.00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: **Using technical terminology for better vector matching**: Rephrase as <span style=\"color: #008000; text-decoration-color: #008000\">\"What is a Large Language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Model (LLM) in Natural Language Processing?\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m1\u001b[0m: **Using technical terminology for better vector matching**: Rephrase as \u001b[32m\"What is a Large Language \u001b[0m\n",
       "\u001b[32mModel \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in Natural Language Processing?\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.64</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.64\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: **Including key entities and relationships from the original query**: Rephrase as <span style=\"color: #008000; text-decoration-color: #008000\">\"What is the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">relationship between Large Language Models (LLMs), transformer architectures, and their applications in natural </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language generation and text classification?\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m2\u001b[0m: **Including key entities and relationships from the original query**: Rephrase as \u001b[32m\"What is the \u001b[0m\n",
       "\u001b[32mrelationship between Large Language Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, transformer architectures, and their applications in natural \u001b[0m\n",
       "\u001b[32mlanguage generation and text classification?\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.57</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.57\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: **Adding relevant synonyms or related concepts**: Rephrase as <span style=\"color: #008000; text-decoration-color: #008000\">\"What is a transformer-based neural </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">network architecture used for natural language generation and text classification, known colloquially as a Large </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Language Model (LLM)?\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m3\u001b[0m: **Adding relevant synonyms or related concepts**: Rephrase as \u001b[32m\"What is a transformer-based neural \u001b[0m\n",
       "\u001b[32mnetwork architecture used for natural language generation and text classification, known colloquially as a Large \u001b[0m\n",
       "\u001b[32mLanguage Model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m?\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.56</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.56\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>: **Varying syntax while preserving semantic meaning**: Rephrase as <span style=\"color: #008000; text-decoration-color: #008000\">\"Can you explain the theoretical </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">foundations behind Large Language Models (LLMs) in natural language processing?\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m4\u001b[0m: **Varying syntax while preserving semantic meaning**: Rephrase as \u001b[32m\"Can you explain the theoretical \u001b[0m\n",
       "\u001b[32mfoundations behind Large Language Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in natural language processing?\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.56</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.56\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>: **Breaking down complex queries into clearer formulations**: Rephrase as <span style=\"color: #008000; text-decoration-color: #008000\">\"Describe the application of</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Large Language Models (LLMs) in Natural Language Understanding\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m5\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m5\u001b[0m: **Breaking down complex queries into clearer formulations**: Rephrase as \u001b[32m\"Describe the application of\u001b[0m\n",
       "\u001b[32mLarge Language Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in Natural Language Understanding\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Confidence: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.50</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;31mConfidence: \u001b[0m\u001b[1;31m0.50\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant = ResearchAssistant(llm=llm, vector_db=vector_db)\n",
    "query=input(\"enter\")\n",
    "# Step 1: Rate the query\n",
    "console.print(\"\\n[bold cyan]Evaluating your query...[/bold cyan]\")\n",
    "rating_result = assistant.rate_query(query)\n",
    "    \n",
    "rating_color = \"green\" if rating_result['rating'] >= 4 else \"yellow\" if rating_result['rating'] >= 3 else \"red\"\n",
    "console.print(f\"[bold {rating_color}]Query Rating: {rating_result['rating']}/5[/bold {rating_color}]\")\n",
    "console.print(f\"Explanation: {rating_result['explanation']}\")\n",
    "console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "# Step 2: Suggest rewrites if the rating is less than perfect\n",
    "rewritten_queries = []\n",
    "if rating_result[\"rating\"] < 5:\n",
    "    console.print(\"[bold cyan]Generating improved query variations...[/bold cyan]\")\n",
    "    rewritten_queries = assistant.suggest_rewrites(query)\n",
    "    console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "# Step 3: Present options to the user\n",
    "    query_options = assistant.present_query_options(query, rewritten_queries)\n",
    "    console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a417a99-a5cc-49e1-95b0-bb92e086b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "0,1,4\n",
    "What are various tools used in Machine Transltion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc9d2221-b31a-41f0-90ed-90f51994a0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import this module to use the ResearchAssistant class\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, BaseOutputParser\n",
    "from langchain.schema import Document, HumanMessage, AIMessage\n",
    "from langchain.vectorstores import VectorStore\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms.base import BaseLLM\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from pydantic import BaseModel, Field\n",
    "import logging\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"SageRAG\")\n",
    "\n",
    "# Rich console for prettier output\n",
    "console = Console()\n",
    "\n",
    "# Enhanced output parsers with better error handling\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Parse output that contains a numbered list and return as a list of strings.\"\"\"\n",
    "    \n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        \"\"\"Parse text into a list of strings.\"\"\"\n",
    "        # Updated regex to be more robust with various numbering styles\n",
    "        lines = re.findall(r\"^\\s*\\d+\\.?\\s+(.*?)$\", text, re.MULTILINE)\n",
    "        return [line.strip() for line in lines if line.strip()]\n",
    "    \n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"line_list\"\n",
    "\n",
    "class QueryRating(BaseModel):\n",
    "    \"\"\"Schema for query rating output.\"\"\"\n",
    "    rating: int = Field(description=\"Rating from 1-5\")\n",
    "    explanation: str = Field(description=\"Explanation for the rating\")\n",
    "\n",
    "@dataclass\n",
    "class RetrievedDocument:\n",
    "    \"\"\"Dataclass for tracking retrieved document info.\"\"\"\n",
    "    document: Document\n",
    "    score: float\n",
    "    query: str = \"\"\n",
    "    rank: int = 0\n",
    "\n",
    "class ResearchAssistant:\n",
    "    \"\"\"AI Research Assistant using RAG for scientific paper queries.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        llm: BaseLLM, \n",
    "        vector_db: VectorStore, \n",
    "        embeddings_model: Optional[SentenceTransformer] = None,\n",
    "        relevance_threshold: float = 0.6,\n",
    "        max_docs_per_query: int = 3,\n",
    "        always_use_fallback: bool = True  # New parameter to control fallback behavior\n",
    "    ):\n",
    "        \"\"\"Initialize the Research Assistant with LLM and vector database.\n",
    "        \n",
    "        Args:\n",
    "            llm: The language model to use for generation\n",
    "            vector_db: Vector database for document retrieval\n",
    "            embeddings_model: Optional SentenceTransformer model for semantic similarity\n",
    "            relevance_threshold: Minimum relevance score for documents\n",
    "            max_docs_per_query: Maximum documents to use per query\n",
    "            always_use_fallback: Whether to always use LLM fallback for general knowledge questions\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.vector_db = vector_db\n",
    "        self.relevance_threshold = relevance_threshold\n",
    "        self.max_docs_per_query = max_docs_per_query\n",
    "        self.always_use_fallback = always_use_fallback\n",
    "        \n",
    "        # Initialize sentence transformer model for semantic similarity if not provided\n",
    "        if embeddings_model is None:\n",
    "            logger.info(\"Initializing default embedding model\")\n",
    "            self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        else:\n",
    "            self.semantic_model = embeddings_model\n",
    "            \n",
    "        # Initialize memory\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        # Create a retriever for direct use\n",
    "        self.retriever = vector_db.as_retriever()\n",
    "        \n",
    "        # Define parsers\n",
    "        self.json_parser = JsonOutputParser(pydantic_object=QueryRating)\n",
    "        self.list_parser = LineListOutputParser()\n",
    "        \n",
    "        logger.info(\"Research Assistant initialized successfully\")\n",
    "    \n",
    "    def rate_query(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Rates the query and gives an explanation.\n",
    "        \n",
    "        Args:\n",
    "            query: The user's query to be evaluated\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing rating and explanation\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an intelligent assistant trained to evaluate search queries for a scientific research database.\n",
    "        Given the following query: \"{query}\"\n",
    "        \n",
    "        1. Rate the query on a scale of 1 (very poor) to 5 (excellent) based on:\n",
    "           - Clarity: Is the query clear and unambiguous?\n",
    "           - Specificity: Does it contain specific technical terms or concepts?\n",
    "           - Relevance: Is it focused on retrieving scientific content?\n",
    "           - Retrievability: Will it work well with vector search?\n",
    "        \n",
    "        2. Provide a short explanation for the rating (what makes it effective or ineffective).\n",
    "        \n",
    "        Respond in JSON format:\n",
    "        {{\n",
    "          \"rating\": <number between 1-5>,\n",
    "          \"explanation\": \"<your explanation>\"\n",
    "        }}\n",
    "        \"\"\")\n",
    "        \n",
    "        chain: Runnable = prompt | self.llm | self.json_parser\n",
    "        \n",
    "        try:\n",
    "            return chain.invoke({\"query\": query})\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error rating query: {e}\")\n",
    "            # Fallback response if parsing fails\n",
    "            return {\n",
    "                \"rating\": 3, \n",
    "                \"explanation\": \"Unable to rate query properly. Consider adding more specific terms.\"\n",
    "            }\n",
    "    \n",
    "    def is_general_knowledge_question(self, query: str) -> bool:\n",
    "        \"\"\"Determines if a question is likely a general knowledge question rather than research-specific.\n",
    "        \n",
    "        Args:\n",
    "            query: The user's query to evaluate\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating if this is likely a general knowledge question\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an intelligent assistant analyzing a question. Determine if this is a general knowledge question\n",
    "        that doesn't require specific scientific papers to answer properly.\n",
    "        \n",
    "        Question: \"{query}\"\n",
    "        \n",
    "        Examples of general knowledge questions:\n",
    "        - What is an operating system?\n",
    "        - Who invented the telephone?\n",
    "        - How does gravity work?\n",
    "        - What is the difference between RAM and ROM?\n",
    "        \n",
    "        Examples of research-specific questions:\n",
    "        - What are the latest developments in CRISPR gene editing?\n",
    "        - How does the transformer architecture improve machine translation accuracy?\n",
    "        - What methodologies are used to measure quantum entanglement?\n",
    "        - What were the findings of the 2023 paper on climate change impact on coral reefs?\n",
    "        \n",
    "        Is this a general knowledge question that could be answered without specific research papers?\n",
    "        Answer only YES or NO.\n",
    "        \"\"\")\n",
    "        \n",
    "        try:\n",
    "            result = self.llm.invoke(prompt.format(query=query))\n",
    "            return \"YES\" in result.upper()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error determining if general knowledge question: {e}\")\n",
    "            return False  # Default to assuming it's not general knowledge\n",
    "    \n",
    "    def suggest_rewrites(self, query: str, chat_history: Optional[List] = None) -> List[str]:\n",
    "        \"\"\"Returns rephrased versions of the query optimized for retrieval.\n",
    "        \n",
    "        Args:\n",
    "            query: Original query to rewrite\n",
    "            chat_history: Optional conversation history for context\n",
    "            \n",
    "        Returns:\n",
    "            List of rewritten queries\n",
    "        \"\"\"\n",
    "        history_context = \"\"\n",
    "        if chat_history:\n",
    "            history_context = \"Consider this conversation context when rewriting the query:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use only last 3 messages for brevity\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    history_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"history_context\"],\n",
    "            template=\"\"\"You are an AI assistant specializing in scientific research queries.\n",
    "            \n",
    "            {history_context}\n",
    "            \n",
    "            Rephrase the question in 5 different ways to improve retrieval from a scientific paper database. \n",
    "            Focus on:\n",
    "            1. Using technical terminology for better vector matching\n",
    "            2. Breaking down complex queries into clearer formulations\n",
    "            3. Adding relevant synonyms or related concepts\n",
    "            4. Varying syntax while preserving semantic meaning\n",
    "            5. Including key entities and relationships from the original query\n",
    "            \n",
    "            Number each version starting with 1.\n",
    "            \n",
    "            Question: {question}\"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            result = (prompt | self.llm | self.list_parser).invoke({\n",
    "                \"question\": query,\n",
    "                \"history_context\": history_context\n",
    "            })\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error suggesting rewrites: {e}\")\n",
    "            # Return a minimal set of rewrites if parsing fails\n",
    "            return [\n",
    "                query,  # Original query\n",
    "                f\"research about {query}\",\n",
    "                f\"papers discussing {query}\"\n",
    "            ]\n",
    "    \n",
    "    def compute_confidence(self, original: str, rewritten: str) -> float:\n",
    "        \"\"\"Computes semantic similarity between original and rewritten queries.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            rewritten: Rewritten version\n",
    "            \n",
    "        Returns:\n",
    "            Similarity score between 0-1\n",
    "        \"\"\"\n",
    "        try:\n",
    "            vec_orig = self.semantic_model.encode(original, convert_to_tensor=True)\n",
    "            vec_rewrite = self.semantic_model.encode(rewritten, convert_to_tensor=True)\n",
    "            return float(util.pytorch_cos_sim(vec_orig, vec_rewrite).item())\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error computing similarity: {e}\")\n",
    "            return 0.7  # Default reasonable value\n",
    "    \n",
    "    def score_queries(self, original: str, queries: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Scores and sorts rephrased queries by confidence.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            queries: List of rewritten queries\n",
    "            \n",
    "        Returns:\n",
    "            List of (query, score) tuples sorted by score\n",
    "        \"\"\"\n",
    "        scored = [(q, self.compute_confidence(original, q)) for q in queries]\n",
    "        return sorted(scored, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    def present_query_options(self, original: str, queries: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Presents the original and rewritten queries with confidence scores.\n",
    "        \n",
    "        Args:\n",
    "            original: Original query\n",
    "            queries: List of rewritten queries\n",
    "            \n",
    "        Returns:\n",
    "            List of (query, score) tuples including original query\n",
    "        \"\"\"\n",
    "        # Add original query at the top\n",
    "        scored_queries = [(\"Original: \" + original, 1.0)]\n",
    "        \n",
    "        # Add scored rewritten queries\n",
    "        rewritten_scores = self.score_queries(original, queries)\n",
    "        for i, (query, score) in enumerate(rewritten_scores, 1):\n",
    "            scored_queries.append((f\"Rewrite {i}: {query}\", score))\n",
    "            \n",
    "        # Display options in a nice format\n",
    "        console.print(\"\\n[bold cyan]Available search queries:[/bold cyan]\")\n",
    "        for i, (query, score) in enumerate(scored_queries):\n",
    "            confidence_color = \"green\" if score > 0.8 else \"yellow\" if score > 0.6 else \"red\"\n",
    "            console.print(f\"[bold]{i}.[/bold] {query}\")\n",
    "            console.print(f\"   [bold {confidence_color}]Confidence: {score:.2f}[/bold {confidence_color}]\")\n",
    "            \n",
    "        return scored_queries\n",
    "    \n",
    "    def retrieve_documents(\n",
    "        self, \n",
    "        queries: List[str], \n",
    "        k: int = 5\n",
    "    ) -> Tuple[List[RetrievedDocument], Dict[str, List[RetrievedDocument]]]:\n",
    "        \"\"\"Retrieves documents using multiple queries, preserving query information.\n",
    "        \n",
    "        Args:\n",
    "            queries: List of queries to retrieve documents for\n",
    "            k: Number of documents to retrieve per query\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (all unique documents, query->documents mapping)\n",
    "        \"\"\"\n",
    "        all_docs: List[RetrievedDocument] = []\n",
    "        unique_content = set()\n",
    "        query_docs_map: Dict[str, List[RetrievedDocument]] = {}\n",
    "        \n",
    "        # Retrieve docs for each query\n",
    "        for query in queries:\n",
    "            console.print(f\"\\n[bold blue]Query:[/bold blue] '{query}'\")\n",
    "            \n",
    "            try:\n",
    "                results = self.vector_db.similarity_search_with_relevance_scores(query, k=k)\n",
    "                docs_for_query: List[RetrievedDocument] = []\n",
    "                \n",
    "                for i, (doc, score) in enumerate(results, 1):\n",
    "                    # Create retrieved document object\n",
    "                    retrieved_doc = RetrievedDocument(\n",
    "                        document=doc,\n",
    "                        score=score,\n",
    "                        query=query,\n",
    "                        rank=i\n",
    "                    )\n",
    "                    \n",
    "                    # Display result info\n",
    "                    score_color = \"green\" if score > 0.8 else \"yellow\" if score > 0.6 else \"red\"\n",
    "                    console.print(f\"[bold]--- Result {i} ---[/bold]\")\n",
    "                    console.print(f\"[bold {score_color}]Score: {score:.4f}[/bold {score_color}]\")\n",
    "                    \n",
    "                    # Show document preview\n",
    "                    preview = doc.page_content[:150] + \"...\" if len(doc.page_content) > 150 else doc.page_content\n",
    "                    console.print(Panel(preview, title=\"Content Preview\", width=100))\n",
    "                    \n",
    "                    # Show metadata if available\n",
    "                    if hasattr(doc, 'metadata') and doc.metadata:\n",
    "                        source = doc.metadata.get('source', 'Unknown')\n",
    "                        console.print(f\"[dim]Source: {source}[/dim]\")\n",
    "                    \n",
    "                    # Add to results for this query\n",
    "                    docs_for_query.append(retrieved_doc)\n",
    "                    \n",
    "                    # Only add unique documents to overall collection\n",
    "                    if doc.page_content not in unique_content:\n",
    "                        unique_content.add(doc.page_content)\n",
    "                        all_docs.append(retrieved_doc)\n",
    "                \n",
    "                query_docs_map[query] = docs_for_query\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error retrieving documents for query '{query}': {e}\")\n",
    "                console.print(f\"[bold red]Error retrieving documents for query: {query}[/bold red]\")\n",
    "        \n",
    "        console.print(f\"\\n[bold green]Total unique documents:[/bold green] {len(all_docs)}\")\n",
    "        return all_docs, query_docs_map\n",
    "\n",
    "    def rerank_docs(self, query: str, docs: List[RetrievedDocument]) -> List[RetrievedDocument]:\n",
    "        \"\"\"Reranks documents based on semantic similarity to the query.\n",
    "        \n",
    "        Args:\n",
    "            query: Query to compare documents against\n",
    "            docs: List of retrieved documents\n",
    "            \n",
    "        Returns:\n",
    "            Reranked list of documents with updated scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            query_embedding = self.semantic_model.encode(query, convert_to_tensor=True)\n",
    "            reranked = []\n",
    "            \n",
    "            for doc in docs:\n",
    "                doc_embedding = self.semantic_model.encode(doc.document.page_content, convert_to_tensor=True)\n",
    "                new_score = float(util.pytorch_cos_sim(query_embedding, doc_embedding).item())\n",
    "                \n",
    "                # Create new RetrievedDocument with updated score\n",
    "                reranked_doc = RetrievedDocument(\n",
    "                    document=doc.document,\n",
    "                    score=new_score,\n",
    "                    query=doc.query,\n",
    "                    rank=0  # Will be updated after sorting\n",
    "                )\n",
    "                reranked.append(reranked_doc)\n",
    "            \n",
    "            # Sort by score and update ranks\n",
    "            reranked.sort(key=lambda x: x.score, reverse=True)\n",
    "            for i, doc in enumerate(reranked, 1):\n",
    "                doc.rank = i\n",
    "                \n",
    "            return reranked\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reranking documents: {e}\")\n",
    "            return docs  # Return original documents if reranking fails\n",
    "    \n",
    "    def can_answer_without_retrieval(self, question: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"Determines if a question can be answered directly from memory without retrieval.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (can_answer, answer_if_available)\n",
    "        \"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        if not chat_history:\n",
    "            return False, None\n",
    "            \n",
    "        # Create the prompt to check if we can answer directly from the conversation\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an AI assistant helping with a scientific research conversation.\n",
    "        Given the following conversation history and a new question, determine if the question:\n",
    "        1. Can be answered directly based ONLY on the conversation history (like \"what was my previous question?\")\n",
    "        2. Does NOT require retrieving new information from scientific papers\n",
    "        \n",
    "        If both conditions are true, provide the answer. Otherwise, respond with \"NEEDS_RETRIEVAL\".\n",
    "        \n",
    "        Conversation History:\n",
    "        {chat_history}\n",
    "        \n",
    "        New Question: {question}\n",
    "        \n",
    "        Your assessment (answer directly or respond with \"NEEDS_RETRIEVAL\"):\n",
    "        \"\"\")\n",
    "        \n",
    "        # Format the chat history for context\n",
    "        history_str = \"\"\n",
    "        for message in chat_history[-5:]:  # Use only last 5 messages for brevity\n",
    "            if hasattr(message, \"content\"):\n",
    "                role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                history_str += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        try:\n",
    "            # Ask the LLM if this can be answered without retrieval\n",
    "            response = self.llm.invoke(\n",
    "                prompt.format(chat_history=history_str, question=question)\n",
    "            )\n",
    "            \n",
    "            # If the response is \"NEEDS_RETRIEVAL\", we need to use retrieval\n",
    "            if \"NEEDS_RETRIEVAL\" in response:\n",
    "                return False, None\n",
    "            else:\n",
    "                logger.info(\"Question can be answered from conversation history\")\n",
    "                return True, response\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error determining if retrieval needed: {e}\")\n",
    "            return False, None  # Default to retrieval if there's an error\n",
    "    \n",
    "    def generate_final_answer(\n",
    "        self, \n",
    "        question: str, \n",
    "        docs: List[RetrievedDocument], \n",
    "        max_docs: int = 5,\n",
    "        fallback_to_llm: bool = True\n",
    "    ) -> str:\n",
    "        \"\"\"Generates the final answer from retrieved documents with fallback to LLM.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            docs: List of retrieved documents\n",
    "            max_docs: Maximum number of documents to include\n",
    "            fallback_to_llm: Whether to fall back to the LLM when docs are insufficient\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        # First, check if this might be a general knowledge question\n",
    "        is_general = self.is_general_knowledge_question(question)\n",
    "        \n",
    "        # First, check if we have any relevant documents\n",
    "        has_relevant_docs = any(doc.score >= self.relevance_threshold for doc in docs)\n",
    "        \n",
    "        # If no relevant docs were found and fallback is enabled, use LLM directly\n",
    "        if (not has_relevant_docs and fallback_to_llm) or (is_general and self.always_use_fallback):\n",
    "            if is_general:\n",
    "                console.print(\"[yellow]This appears to be a general knowledge question. Using LLM knowledge.[/yellow]\")\n",
    "            else:\n",
    "                console.print(\"[yellow]No relevant documents found. Falling back to LLM knowledge.[/yellow]\")\n",
    "            \n",
    "            # Create a direct LLM response prompt\n",
    "            fallback_prompt = PromptTemplate(\n",
    "                input_variables=[\"question\", \"chat_history\"],\n",
    "                template=\"\"\"You are a helpful scientific research assistant.\n",
    "                \n",
    "                The user has asked a question that either:\n",
    "                1. Could not be answered using our research paper database, or\n",
    "                2. Is a general knowledge question that doesn't require specific papers\n",
    "                \n",
    "                Please answer based on your general knowledge.\n",
    "                \n",
    "                {chat_history}\n",
    "                \n",
    "                Question: {question}\n",
    "                \n",
    "                Provide a helpful, informative answer. If this is a scientific or technical question,\n",
    "                explain concepts clearly and accurately. Don't apologize for not using specific papers \n",
    "                - just provide your best answer directly.\n",
    "                \n",
    "                Answer:\"\"\"\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                answer = (fallback_prompt | self.llm).invoke({\n",
    "                    \"question\": question,\n",
    "                    \"chat_history\": chat_context\n",
    "                })\n",
    "                \n",
    "                # Save the QA pair to memory\n",
    "                self.memory.chat_memory.add_messages([\n",
    "                    HumanMessage(content=question),\n",
    "                    AIMessage(content=answer)\n",
    "                ])\n",
    "                \n",
    "                return answer\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating fallback answer: {e}\")\n",
    "                return \"I'm sorry, I don't have enough information to answer your question accurately.\"\n",
    "        \n",
    "        # Format document content with metadata\n",
    "        context_pieces = []\n",
    "        for i, doc in enumerate(docs[:max_docs]):\n",
    "            chunk = f\"Document {i+1} [Relevance: {doc.score:.2f}]:\\n{doc.document.page_content}\"\n",
    "            \n",
    "            # Add metadata if available\n",
    "            if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                source = doc.document.metadata.get('source', 'Unknown')\n",
    "                chunk += f\"\\nSource: {source}\"\n",
    "                \n",
    "            context_pieces.append(chunk)\n",
    "            \n",
    "        context = \"\\n\\n---\\n\\n\".join(context_pieces)\n",
    "        \n",
    "        # If we have context but it might not be sufficient, use a prompt that can fall back to general knowledge\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful scientific research assistant analyzing scientific papers. \n",
    "            Use the following retrieved context chunks to answer the user's question thoroughly.\n",
    "            \n",
    "            Guidelines:\n",
    "            - Focus on providing accurate information from the papers\n",
    "            - Synthesize information across documents when appropriate\n",
    "            - Cite the sources of information in your answer (e.g., \"According to Document 1...\")\n",
    "            - If the information isn't sufficient in the context, supplement with your general knowledge,\n",
    "              clearly indicating which parts of your answer come from the papers and which parts are from your general knowledge\n",
    "            - If the context contains conflicting information, highlight the disagreement and possible reasons\n",
    "            - Maintain scientific accuracy above all else\n",
    "            - Don't apologize for using general knowledge - just be clear about what comes from papers vs. your knowledge\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Generate answer\n",
    "            answer = (prompt | self.llm).invoke({\n",
    "                \"question\": question, \n",
    "                \"context\": context,\n",
    "                \"chat_history\": chat_context\n",
    "            })\n",
    "            \n",
    "            # Save the QA pair to memory\n",
    "            self.memory.chat_memory.add_messages([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=answer)\n",
    "            ])\n",
    "            \n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating answer: {e}\")\n",
    "            return \"I'm sorry, I encountered an error while generating your answer. Please try rephrasing your question.\"\n",
    "    \n",
    "    def generate_combined_answer(\n",
    "        self, \n",
    "        question: str, \n",
    "        query_results: Dict[str, List[RetrievedDocument]],\n",
    "        fallback_to_llm: bool = True\n",
    "    ) -> str:\n",
    "        \"\"\"Generates a combined answer from multiple query results with fallback.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            query_results: Dictionary mapping queries to retrieved documents\n",
    "            fallback_to_llm: Whether to fall back to the LLM when docs are insufficient\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Get chat history from memory if available\n",
    "        chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Create chat history string for context if available\n",
    "        chat_context = \"\"\n",
    "        if chat_history:\n",
    "            chat_context = \"Previous conversation:\\n\"\n",
    "            for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                if hasattr(message, \"content\"):\n",
    "                    role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                    chat_context += f\"{role}: {message.content}\\n\"\n",
    "        \n",
    "        # Check if this might be a general knowledge question\n",
    "        is_general = self.is_general_knowledge_question(question)\n",
    "        \n",
    "        # Check if we have any relevant documents at all\n",
    "        all_docs = []\n",
    "        for docs_list in query_results.values():\n",
    "            all_docs.extend(docs_list)\n",
    "            \n",
    "        has_relevant_docs = any(doc.score >= self.relevance_threshold for doc in all_docs)\n",
    "        \n",
    "        # If no relevant documents and fallback enabled, use LLM directly\n",
    "        if (not has_relevant_docs and fallback_to_llm) or (is_general and self.always_use_fallback):\n",
    "            if is_general:\n",
    "                console.print(\"[yellow]This appears to be a general knowledge question. Using LLM knowledge.[/yellow]\")\n",
    "            else:\n",
    "                console.print(\"[yellow]No relevant documents found across all queries. Falling back to LLM knowledge.[/yellow]\")\n",
    "            \n",
    "            # Create a direct LLM response prompt\n",
    "            fallback_prompt = PromptTemplate(\n",
    "                input_variables=[\"question\", \"chat_history\"],\n",
    "                template=\"\"\"You are a helpful scientific research assistant.\n",
    "                \n",
    "                The user has asked a question that either:\n",
    "                1. Could not be answered using our research paper database, or\n",
    "                2. Is a general knowledge question that doesn't require specific papers\n",
    "                \n",
    "                Please answer based on your general knowledge.\n",
    "                \n",
    "                {chat_history}\n",
    "                \n",
    "                Question: {question}\n",
    "                \n",
    "                Provide a helpful, informative answer. If this is a scientific or technical question,\n",
    "                explain concepts clearly and accurately. Don't apologize for not using specific papers \n",
    "                - just provide your best answer directly.\n",
    "                \n",
    "                Answer:\"\"\"\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                answer = (fallback_prompt | self.llm).invoke({\n",
    "                    \"question\": question,\n",
    "                    \"chat_history\": chat_context\n",
    "                })\n",
    "                \n",
    "                # Save the QA pair to memory\n",
    "                self.memory.chat_memory.add_messages([\n",
    "                    HumanMessage(content=question),\n",
    "                    AIMessage(content=answer)\n",
    "                ])\n",
    "                \n",
    "                return answer\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating fallback answer: {e}\")\n",
    "                return \"I'm sorry, I don't have enough information to answer your question accurately.\"\n",
    "        \n",
    "        # Combine all relevant documents from different queries\n",
    "        all_context_sections = []\n",
    "        \n",
    "        for query, docs in query_results.items():\n",
    "            # Use only the top N documents for each query\n",
    "            docs_for_query = docs[:self.max_docs_per_query]\n",
    "            if docs_for_query:\n",
    "                all_context_sections.append(f\"Results for query: '{query}'\")\n",
    "                for i, doc in enumerate(docs_for_query, 1):\n",
    "                    section = f\"Document {i} [Relevance: {doc.score:.2f}]:\\n{doc.document.page_content}\"\n",
    "                    \n",
    "                    # Add metadata if available\n",
    "                    if hasattr(doc.document, 'metadata') and doc.document.metadata:\n",
    "                        source = doc.document.metadata.get('source', 'Unknown')\n",
    "                        section += f\"\\nSource: {source}\"\n",
    "                        \n",
    "                    all_context_sections.append(section)\n",
    "        \n",
    "        # Join all context sections\n",
    "        combined_context = \"\\n\\n---\\n\\n\".join(all_context_sections)\n",
    "        \n",
    "        # Create a prompt that emphasizes synthesizing information and can fall back to general knowledge\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\", \"chat_history\"],\n",
    "            template=\"\"\"You are a helpful scientific research assistant analyzing scientific papers. \n",
    "            The user's question has been reformulated in several ways, and each formulation returned different documents.\n",
    "            \n",
    "            Guidelines:\n",
    "            - Synthesize information across ALL retrieved documents to provide a comprehensive answer\n",
    "            - Compare and contrast findings from different sources\n",
    "            - Highlight the most relevant information from each source\n",
    "            - Cite the specific documents you're referencing (e.g., \"According to the paper in Document 3...\")\n",
    "            - If information conflicts across sources, explain the different perspectives\n",
    "            - If the information is insufficient, supplement with your general knowledge,\n",
    "              clearly indicating which parts of your answer come from the papers and which are from your general knowledge\n",
    "            - Maintain scientific accuracy above all else\n",
    "            - Don't apologize for using general knowledge - just be clear about what information comes from papers vs. your knowledge\n",
    "            \n",
    "            {chat_history}\n",
    "            \n",
    "            Context from multiple query formulations:\n",
    "            {context}\n",
    "            \n",
    "            Original Question: {question}\n",
    "            \n",
    "            Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Generate answer\n",
    "            answer = (prompt | self.llm).invoke({\n",
    "                \"question\": question, \n",
    "                \"context\": combined_context,\n",
    "                \"chat_history\": chat_context\n",
    "            })\n",
    "            \n",
    "            # Save the QA pair to memory\n",
    "            self.memory.chat_memory.add_messages([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=answer)\n",
    "            ])\n",
    "            \n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating combined answer: {e}\")\n",
    "            return \"I'm sorry, I encountered an error while generating your answer. Please try rephrasing your question.\"\n",
    "    \n",
    "    def answer_query(self, query: str, use_retrieval: bool = True) -> str:\n",
    "        \"\"\"Direct answer method that either does retrieval or uses LLM fallback.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            use_retrieval: Whether to use retrieval process or direct LLM\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # First check if we can answer directly from conversation history\n",
    "        can_answer, direct_answer = self.can_answer_without_retrieval(query)\n",
    "        if can_answer:\n",
    "            console.print(\"[green]Answering from conversation history[/green]\")\n",
    "            return direct_answer\n",
    "        \n",
    "        # Check if this is a general knowledge question\n",
    "        is_general = self.is_general_knowledge_question(query)\n",
    "        \n",
    "        # If it's general knowledge and we're set to always use fallback for those\n",
    "        if is_general and self.always_use_fallback and not use_retrieval:\n",
    "            console.print(\"[yellow]General knowledge question detected. Using LLM directly.[/yellow]\")\n",
    "            # Create a direct LLM response prompt\n",
    "            chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "            chat_context = \"\"\n",
    "            if chat_history:\n",
    "                chat_context = \"Previous conversation:\\n\"\n",
    "                for message in chat_history[-3:]:  # Use last 3 messages for context\n",
    "                    if hasattr(message, \"content\"):\n",
    "                        role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                        chat_context += f\"{role}: {message.content}\\n\"\n",
    "            \n",
    "            fallback_prompt = PromptTemplate(\n",
    "                input_variables=[\"question\", \"chat_history\"],\n",
    "                template=\"\"\"You are a helpful scientific research assistant.\n",
    "                \n",
    "                The user has asked a general knowledge question that doesn't require specific papers.\n",
    "                Please answer based on your general knowledge.\n",
    "                \n",
    "                {chat_history}\n",
    "                \n",
    "                Question: {question}\n",
    "                \n",
    "                Provide a helpful, informative answer. If this is a scientific or technical question,\n",
    "                explain concepts clearly and accurately.\n",
    "                \n",
    "                Answer:\"\"\"\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                answer = (fallback_prompt | self.llm).invoke({\n",
    "                    \"question\": query,\n",
    "                    \"chat_history\": chat_context\n",
    "                })\n",
    "                \n",
    "                # Save the QA pair to memory\n",
    "                self.memory.chat_memory.add_messages([\n",
    "                    HumanMessage(content=query),\n",
    "                    AIMessage(content=answer)\n",
    "                ])\n",
    "                \n",
    "                return answer\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating direct answer: {e}\")\n",
    "                return \"I'm sorry, I encountered an error while generating your answer.\"\n",
    "        \n",
    "        # Otherwise, proceed with full retrieval pipeline\n",
    "        return self.search_with_query_feedback(query)\n",
    "    \n",
    "    def search_with_query_feedback(self, query: str, num_results: int = 5) -> str:\n",
    "        \"\"\"Main pipeline that processes a query and returns an answer.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            num_results: Number of results to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Rate the original query\n",
    "            console.print(\"\\n[bold cyan]Evaluating your query...[/bold cyan]\")\n",
    "            rating_result = self.rate_query(query)\n",
    "            \n",
    "            rating_color = \"green\" if rating_result['rating'] >= 4 else \"yellow\" if rating_result['rating'] >= 3 else \"red\"\n",
    "            console.print(f\"[bold {rating_color}]Query Rating: {rating_result['rating']}/5[/bold {rating_color}]\")\n",
    "            console.print(f\"Explanation: {rating_result['explanation']}\")\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Check if this is a general knowledge question\n",
    "            is_general = self.is_general_knowledge_question(query)\n",
    "            if is_general and self.always_use_fallback:\n",
    "                console.print(\"[yellow]This appears to be a general knowledge question. Proceeding with simplified search.[/yellow]\")\n",
    "                # For general knowledge questions, we might still want to check the database briefly\n",
    "                # but we'll rely more on the LLM fallback\n",
    "            \n",
    "            # Step 2: Suggest rewrites if the rating is less than perfect\n",
    "            rewritten_queries = []\n",
    "            if rating_result[\"rating\"] < 5:\n",
    "                console.print(\"[bold cyan]Generating improved query variations...[/bold cyan]\")\n",
    "                # Get chat history for context-aware rewrites\n",
    "                chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "                rewritten_queries = self.suggest_rewrites(query, chat_history)\n",
    "                console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 3: Present options to the user\n",
    "            query_options = self.present_query_options(query, rewritten_queries)\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 4: Get user selection\n",
    "            console.print(\"[bold cyan]Select queries to use:[/bold cyan]\")\n",
    "            console.print(\"Enter the numbers of the queries you want to use (comma-separated, e.g., '0,2,3')\")\n",
    "            console.print(\"Or press Enter to use all queries\")\n",
    "            selected_indices_input = input(\"> \")\n",
    "            \n",
    "            if selected_indices_input.strip() == \"\":\n",
    "                # Use all queries if no selection\n",
    "                selected_indices = list(range(len(query_options)))\n",
    "            else:\n",
    "                try:\n",
    "                    selected_indices = [int(idx.strip()) for idx in selected_indices_input.split(\",\")]\n",
    "                except ValueError:\n",
    "                    console.print(\"[bold red]Invalid input. Using original query only.[/bold red]\")\n",
    "                    selected_indices = [0]  # Default to original query\n",
    "            \n",
    "            # Get the selected queries (without the prefix and score)\n",
    "            selected_queries = []\n",
    "            for idx in selected_indices:\n",
    "                if 0 <= idx < len(query_options):\n",
    "                    query_text = query_options[idx][0]\n",
    "                    # Remove the \"Original: \" or \"Rewrite N: \" prefix\n",
    "                    if \"Original: \" in query_text:\n",
    "                        query_text = query_text.replace(\"Original: \", \"\")\n",
    "                    elif \"Rewrite \" in query_text:\n",
    "                        query_text = query_text.split(\": \", 1)[1] if \": \" in query_text else query_text\n",
    "                    selected_queries.append(query_text)\n",
    "            \n",
    "            if not selected_queries:\n",
    "                console.print(\"[bold red]No valid queries selected. Using original query.[/bold red]\")\n",
    "                selected_queries = [query]\n",
    "                \n",
    "            console.print(f\"\\n[bold green]Selected {len(selected_queries)} queries for retrieval.[/bold green]\")\n",
    "            console.print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Step 5: Retrieve documents\n",
    "            console.print(\"[bold cyan]Retrieving relevant documents...[/bold cyan]\")\n",
    "            docs, query_docs_map = self.retrieve_documents(selected_queries, k=num_results)\n",
    "            \n",
    "            # Step 6: Generate answer\n",
    "            if len(selected_queries) > 1:\n",
    "                console.print(\"\\n[bold cyan]Generating combined answer based on multiple query results...[/bold cyan]\")\n",
    "                final_answer = self.generate_combined_answer(\n",
    "                    query, \n",
    "                    query_docs_map,\n",
    "                    fallback_to_llm=True  # Always enable fallback\n",
    "                )\n",
    "            else:\n",
    "                # For single query, rerank and use the traditional approach\n",
    "                console.print(\"\\n[bold cyan]Reranking documents based on relevance to the original query...[/bold cyan]\")\n",
    "                reranked_docs = self.rerank_docs(query, docs)\n",
    "                \n",
    "                # Apply relevance threshold\n",
    "                filtered_docs = [\n",
    "                    doc for doc in reranked_docs if doc.score >= self.relevance_threshold\n",
    "                ]\n",
    "                \n",
    "                # Display relevance information\n",
    "                has_relevant_docs = len(filtered_docs) > 0\n",
    "                if not has_relevant_docs:\n",
    "                    console.print(\"[yellow]No documents met the relevance threshold. Using all retrieved documents but may fall back to LLM knowledge.[/yellow]\")\n",
    "                    filtered_docs = reranked_docs\n",
    "                \n",
    "                # Generate the final answer with fallback enabled\n",
    "                console.print(\"\\n[bold cyan]Generating answer...[/bold cyan]\")\n",
    "                final_answer = self.generate_final_answer(\n",
    "                    query, \n",
    "                    filtered_docs, \n",
    "                    fallback_to_llm=True  # Always enable fallback\n",
    "                )\n",
    "            \n",
    "            return final_answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in search pipeline: {e}\")\n",
    "            # Fall back to direct LLM answer if the pipeline fails\n",
    "            console.print(\"[bold red]Error in search pipeline. Falling back to LLM.[/bold red]\")\n",
    "            \n",
    "            chat_history = self.memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "            chat_context = \"\"\n",
    "            if chat_history:\n",
    "                chat_context = \"Previous conversation:\\n\"\n",
    "                for message in chat_history[-3:]:\n",
    "                    if hasattr(message, \"content\"):\n",
    "                        role = \"Human\" if isinstance(message, HumanMessage) else \"Assistant\"\n",
    "                        chat_context += f\"{role}: {message.content}\\n\"\n",
    "            \n",
    "            # Create a direct LLM response as emergency fallback\n",
    "            emergency_prompt = PromptTemplate(\n",
    "                input_variables=[\"question\", \"chat_history\"],\n",
    "                template=\"\"\"You are a helpful scientific research assistant.\n",
    "                \n",
    "                There was an error retrieving information from our research database.\n",
    "                Please answer the question based on your general knowledge.\n",
    "                \n",
    "                {chat_history}\n",
    "                \n",
    "                Question: {question}\n",
    "                \n",
    "                Answer:\"\"\"\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                answer = (emergency_prompt | self.llm).invoke({\n",
    "                    \"question\": query,\n",
    "                    \"chat_history\": chat_context\n",
    "                })\n",
    "                return answer\n",
    "            except:\n",
    "                return \"I apologize, but I'm having technical difficulties. Please try again with a different question.\"\n",
    "\n",
    "# Example usage implementation \n",
    "def create_research_assistant(llm, vector_db, embeddings_model=None):\n",
    "    \"\"\"Factory function to create a research assistant with the specified components.\"\"\"\n",
    "    return ResearchAssistant(\n",
    "        llm=llm,\n",
    "        vector_db=vector_db,\n",
    "        embeddings_model=embeddings_model,\n",
    "        relevance_threshold=0.5,  # Lower threshold to be more lenient with document relevance\n",
    "        max_docs_per_query=3,\n",
    "        always_use_fallback=True  # Always use LLM fallback for general knowledge\n",
    "    )\n",
    "\n",
    "# Interactive command-line interface\n",
    "def run_cli(assistant):\n",
    "    \"\"\"Run an interactive CLI for the research assistant.\"\"\"\n",
    "    console.print(\"[bold green]Research Assistant CLI[/bold green]\")\n",
    "    console.print(\"Type 'exit' to quit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\n[bold blue]Ask a question:[/bold blue] \")\n",
    "        if question.lower() in ('exit', 'quit', 'q'):\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            answer = assistant.answer_query(question)\n",
    "            console.print(\"\\n[bold green]Answer:[/bold green]\")\n",
    "            console.print(Panel(Markdown(answer), width=100))\n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Error:[/bold red] {e}\")\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # You would need to define these before using:\n",
    "    # from langchain.llms import ChatOpenAI\n",
    "    # from langchain.vectorstores import Chroma\n",
    "    # \n",
    "    # llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    # vector_db = Chroma(embedding_function=...)\n",
    "    # \n",
    "    # assistant = create_research_assistant(llm, vector_db)\n",
    "    # run_cli(assistant)\n",
    "    print(\"Import this module to use the ResearchAssistant class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab1fa817-a9b3-4feb-807a-570a2b51eece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:37:52,676 - SageRAG - INFO - Research Assistant initialized successfully\n"
     ]
    }
   ],
   "source": [
    "assistant = create_research_assistant(llm, vector_db, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a6699f1-9f6b-4264-82e7-0c363463319f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query What is operating system ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:38:48,444 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Evaluating your query...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mEvaluating your query\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:38:48,916 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Query Rating: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">/</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mQuery Rating: \u001b[0m\u001b[1;32m4\u001b[0m\u001b[1;32m/\u001b[0m\u001b[1;32m5\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Explanation: The query is clear and concise, with a good balance of specificity and relevance. However, it lacks \n",
       "technical precision and could potentially retrieve more general information on operating systems, rather than \n",
       "scientific research content. This makes it moderately effective for retrieving results from a vector search.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Explanation: The query is clear and concise, with a good balance of specificity and relevance. However, it lacks \n",
       "technical precision and could potentially retrieve more general information on operating systems, rather than \n",
       "scientific research content. This makes it moderately effective for retrieving results from a vector search.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:38:50,718 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">This appears to be a general knowledge question. Proceeding with simplified search.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mThis appears to be a general knowledge question. Proceeding with simplified search.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Generating improved query variations...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mGenerating improved query variations\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:38:51,061 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-04-17 13:38:53,105 - langsmith.client - WARNING - Failed to send compressed multipart ingest: Connection error caused failure to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. Please confirm your internet connection. ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'api.smith.langchain.com\\', port=443): Max retries exceeded with url: /runs/multipart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x3190d48d0>: Failed to resolve \\'api.smith.langchain.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))'))\n",
      "Content-Length: 1176\n",
      "API Key: lsv2_********************************************82\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:38:57,796 - SageRAG - ERROR - Error computing similarity: 'HuggingFaceEmbeddings' object has no attribute 'encode'\n",
      "2025-04-17 13:38:57,798 - SageRAG - ERROR - Error computing similarity: 'HuggingFaceEmbeddings' object has no attribute 'encode'\n",
      "2025-04-17 13:38:57,799 - SageRAG - ERROR - Error computing similarity: 'HuggingFaceEmbeddings' object has no attribute 'encode'\n",
      "2025-04-17 13:38:57,804 - SageRAG - ERROR - Error computing similarity: 'HuggingFaceEmbeddings' object has no attribute 'encode'\n",
      "2025-04-17 13:38:57,805 - SageRAG - ERROR - Error computing similarity: 'HuggingFaceEmbeddings' object has no attribute 'encode'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Available search queries:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mAvailable search queries:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">.</span> Original: What is operating system ?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m0\u001b[0m\u001b[1m.\u001b[0m Original: What is operating system ?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Confidence: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;32mConfidence: \u001b[0m\u001b[1;32m1.00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: **Technical terminology for better vector matching**: <span style=\"color: #008000; text-decoration-color: #008000\">\"Query OS classification: Investigate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">definitions of computer system architecture, specifically those employing process management and resource </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">allocation algorithms.\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m1\u001b[0m: **Technical terminology for better vector matching**: \u001b[32m\"Query OS classification: Investigate \u001b[0m\n",
       "\u001b[32mdefinitions of computer system architecture, specifically those employing process management and resource \u001b[0m\n",
       "\u001b[32mallocation algorithms.\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.70</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.70\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: **Breaking down complex queries into clearer formulations**: <span style=\"color: #008000; text-decoration-color: #008000\">\"Exploring the concept of a 'computer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">operating system': Describe the primary functions, including (i) process scheduling, (ii) memory management, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(iii) I/O handling, in relation to its impact on computer performance and user experience.\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m2\u001b[0m: **Breaking down complex queries into clearer formulations**: \u001b[32m\"Exploring the concept of a 'computer \u001b[0m\n",
       "\u001b[32moperating system': Describe the primary functions, including \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi\u001b[0m\u001b[32m)\u001b[0m\u001b[32m process scheduling, \u001b[0m\u001b[32m(\u001b[0m\u001b[32mii\u001b[0m\u001b[32m)\u001b[0m\u001b[32m memory management, and \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32miii\u001b[0m\u001b[32m)\u001b[0m\u001b[32m I/O handling, in relation to its impact on computer performance and user experience.\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.70</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.70\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: **Adding relevant synonyms or related concepts**: <span style=\"color: #008000; text-decoration-color: #008000\">\"Inquire about the characteristics of an 'operating</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">system' (e.g., OS), particularly those involving multi-user, multi-tasking, and multi-processing support, as well </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">as their implications for security, stability, and efficiency.\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m3\u001b[0m: **Adding relevant synonyms or related concepts**: \u001b[32m\"Inquire about the characteristics of an 'operating\u001b[0m\n",
       "\u001b[32msystem' \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., OS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, particularly those involving multi-user, multi-tasking, and multi-processing support, as well \u001b[0m\n",
       "\u001b[32mas their implications for security, stability, and efficiency.\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.70</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.70\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>: **Varying syntax while preserving semantic meaning**: <span style=\"color: #008000; text-decoration-color: #008000\">\"Describe the nature of a computer operating </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">system: What are its fundamental components? How do they interact with hardware resources? What role does it play </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in facilitating efficient computation and user interaction?\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m4\u001b[0m: **Varying syntax while preserving semantic meaning**: \u001b[32m\"Describe the nature of a computer operating \u001b[0m\n",
       "\u001b[32msystem: What are its fundamental components? How do they interact with hardware resources? What role does it play \u001b[0m\n",
       "\u001b[32min facilitating efficient computation and user interaction?\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.70</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.70\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">.</span> Rewrite <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>: **Including key entities and relationships from the original query**: <span style=\"color: #008000; text-decoration-color: #008000\">\"Examine the relationship </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">between an 'operating system' (OS) and its constituent parts, including device drivers, kernel modules, and system </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">calls: How do these components interact to enable efficient execution of applications and manage system resources?\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m5\u001b[0m\u001b[1m.\u001b[0m Rewrite \u001b[1;36m5\u001b[0m: **Including key entities and relationships from the original query**: \u001b[32m\"Examine the relationship \u001b[0m\n",
       "\u001b[32mbetween an 'operating system' \u001b[0m\u001b[32m(\u001b[0m\u001b[32mOS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and its constituent parts, including device drivers, kernel modules, and system \u001b[0m\n",
       "\u001b[32mcalls: How do these components interact to enable efficient execution of applications and manage system resources?\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Confidence: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.70</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;33mConfidence: \u001b[0m\u001b[1;33m0.70\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Select queries to use:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mSelect queries to use:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Enter the numbers of the queries you want to use <span style=\"font-weight: bold\">(</span>comma-separated, e.g., <span style=\"color: #008000; text-decoration-color: #008000\">'0,2,3'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Enter the numbers of the queries you want to use \u001b[1m(\u001b[0mcomma-separated, e.g., \u001b[32m'0,2,3'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Or press Enter to use all queries\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Or press Enter to use all queries\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:39:02,688 - langsmith.client - WARNING - Failed to send compressed multipart ingest: Connection error caused failure to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. Please confirm your internet connection. ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'api.smith.langchain.com\\', port=443): Max retries exceeded with url: /runs/multipart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x3190c2f90>: Failed to resolve \\'api.smith.langchain.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))'))\n",
      "Content-Length: 5615\n",
      "API Key: lsv2_********************************************82\n",
      "2025-04-17 13:39:12,298 - langsmith.client - WARNING - Failed to send compressed multipart ingest: Connection error caused failure to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. Please confirm your internet connection. ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'api.smith.langchain.com\\', port=443): Max retries exceeded with url: /runs/multipart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x31909c850>: Failed to resolve \\'api.smith.langchain.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))'))\n",
      "Content-Length: 6092\n",
      "API Key: lsv2_********************************************82\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  0,1,3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Selected </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> queries for retrieval.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mSelected \u001b[0m\u001b[1;32m3\u001b[0m\u001b[1;32m queries for retrieval.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------------------------------------------------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------------------------------------------------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Retrieving relevant documents...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mRetrieving relevant documents\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Query:</span> <span style=\"color: #008000; text-decoration-color: #008000\">'What is operating system ?'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;34mQuery:\u001b[0m \u001b[32m'What is operating system ?'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.5895</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.5895\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ ● Operating System ConceptsbyAvi Silberschatz, GregGagne, and Peter Baer Galvin                  │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ ● Operating System ConceptsbyAvi Silberschatz, GregGagne, and Peter Baer Galvin                  │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2104.09487v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2104.09487v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.4490</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.4490\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Android OS                                                                                       │\n",
       "│ Case Study                                                                                       │\n",
       "│ OPERATING  SYSTEMS                                                                               │\n",
       "│ Submitted By: -                                                                                  │\n",
       "│ Mayank Goelmayank.co19@nsut.ac.in                                                                │\n",
       "│ GouravSingalgourav.co19@nsut.ac.in                                                               │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Android OS                                                                                       │\n",
       "│ Case Study                                                                                       │\n",
       "│ OPERATING  SYSTEMS                                                                               │\n",
       "│ Submitted By: -                                                                                  │\n",
       "│ Mayank Goelmayank.co19@nsut.ac.in                                                                │\n",
       "│ GouravSingalgourav.co19@nsut.ac.in                                                               │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2104.09487v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2104.09487v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3009</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3009\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ controlled by the file system are ultimately accessed via                                        │\n",
       "│ kernel-resident device d rivers.  A user process can                                             │\n",
       "│ implement any semantics it chooses fo...                                                         │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ controlled by the file system are ultimately accessed via                                        │\n",
       "│ kernel-resident device d rivers.  A user process can                                             │\n",
       "│ implement any semantics it chooses fo...                                                         │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/1403.5976v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/1403.5976v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.2779</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.2779\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Symposium on Operating Systems Principles (SOSP ’09) . 29–44.                                    │\n",
       "│ [11] Andrew Baumann, Gernot Heiser, Jonathan Appavoo, Dilma Da Silva,                            │\n",
       "│ Orran Krieger, Rob...                                                                            │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Symposium on Operating Systems Principles (SOSP ’09) . 29–44.                                    │\n",
       "│ [11] Andrew Baumann, Gernot Heiser, Jonathan Appavoo, Dilma Da Silva,                            │\n",
       "│ Orran Krieger, Rob...                                                                            │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2306.15076v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2306.15076v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.2669</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.2669\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ InternationalJournal of Computer Science &amp; Information Technology (IJCSIT) Vol 4, No 5, October  │\n",
       "│ 2012                                                                                             │\n",
       "│ 123                                                                                              │\n",
       "│ System Time statistics arecalculatedin the fo...                                                 │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ InternationalJournal of Computer Science & Information Technology (IJCSIT) Vol 4, No 5, October  │\n",
       "│ 2012                                                                                             │\n",
       "│ 123                                                                                              │\n",
       "│ System Time statistics arecalculatedin the fo...                                                 │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/1211.4839v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/1211.4839v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Query:</span> <span style=\"color: #008000; text-decoration-color: #008000\">'**Technical terminology for better vector matching**: \"Query OS classification: Investigate definitions of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">computer system architecture, specifically those employing process management and resource allocation algorithms.\"'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;34mQuery:\u001b[0m \u001b[32m'**Technical terminology for better vector matching**: \"Query OS classification: Investigate definitions of \u001b[0m\n",
       "\u001b[32mcomputer system architecture, specifically those employing process management and resource allocation algorithms.\"'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3459</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3459\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Neetu Goel,  R.B. Garg| A Comparative Study of CPU Scheduling Algorithms                         │\n",
       "│ International Journal of Graphics &amp; Image Processing |Vol 2|issue 4|November...                  │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Neetu Goel,  R.B. Garg| A Comparative Study of CPU Scheduling Algorithms                         │\n",
       "│ International Journal of Graphics & Image Processing |Vol 2|issue 4|November...                  │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/1307.4165v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/1307.4165v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3312</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3312\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ [8] D. Lo, L. Cheng, R. Govindaraju, P. Ranganathan, and C. Kozyrakis,                           │\n",
       "│ “Heracles: Improving resource efficiency at scale,” in Proceedings of the                        │\n",
       "│ 42nd ...                                                                                         │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ [8] D. Lo, L. Cheng, R. Govindaraju, P. Ranganathan, and C. Kozyrakis,                           │\n",
       "│ “Heracles: Improving resource efficiency at scale,” in Proceedings of the                        │\n",
       "│ 42nd ...                                                                                         │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2310.14741v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2310.14741v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3179</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3179\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ ● Operating System ConceptsbyAvi Silberschatz, GregGagne, and Peter Baer Galvin                  │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ ● Operating System ConceptsbyAvi Silberschatz, GregGagne, and Peter Baer Galvin                  │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2104.09487v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2104.09487v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3082</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3082\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ I., 2011. Mesos: A platform for ﬁne-grained resource sharing in the data center. In: NSDI. Vol.  │\n",
       "│ 11.                                                                                              │\n",
       "│ pp. 22–22.                                                                                       │\n",
       "│ Hussain, H., Malik, S. U. R., Hameed, A...                                                       │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ I., 2011. Mesos: A platform for ﬁne-grained resource sharing in the data center. In: NSDI. Vol.  │\n",
       "│ 11.                                                                                              │\n",
       "│ pp. 22–22.                                                                                       │\n",
       "│ Hussain, H., Malik, S. U. R., Hameed, A...                                                       │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/1705.03102v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/1705.03102v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3061</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3061\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ As can be seen from Figure 12, the observed worst case                                           │\n",
       "│ time recorded in Quest-V is within the bounds, but also                                          │\n",
       "│ close, to the prediction derived from E...                                                       │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ As can be seen from Figure 12, the observed worst case                                           │\n",
       "│ time recorded in Quest-V is within the bounds, but also                                          │\n",
       "│ close, to the prediction derived from E...                                                       │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/1310.6301v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/1310.6301v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Query:</span> <span style=\"color: #008000; text-decoration-color: #008000\">'**Adding relevant synonyms or related concepts**: \"Inquire about the characteristics of an '</span>operating \n",
       "system' <span style=\"font-weight: bold\">(</span>e.g., OS<span style=\"font-weight: bold\">)</span>, particularly those involving multi-user, multi-tasking, and multi-processing support, as well \n",
       "as their implications for security, stability, and efficiency.\"'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;34mQuery:\u001b[0m \u001b[32m'**Adding relevant synonyms or related concepts**: \"Inquire about the characteristics of an '\u001b[0moperating \n",
       "system' \u001b[1m(\u001b[0me.g., OS\u001b[1m)\u001b[0m, particularly those involving multi-user, multi-tasking, and multi-processing support, as well \n",
       "as their implications for security, stability, and efficiency.\"'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.5830</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.5830\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ ● Operating System ConceptsbyAvi Silberschatz, GregGagne, and Peter Baer Galvin                  │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ ● Operating System ConceptsbyAvi Silberschatz, GregGagne, and Peter Baer Galvin                  │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2104.09487v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2104.09487v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.4519</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.4519\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Android OS                                                                                       │\n",
       "│ Case Study                                                                                       │\n",
       "│ OPERATING  SYSTEMS                                                                               │\n",
       "│ Submitted By: -                                                                                  │\n",
       "│ Mayank Goelmayank.co19@nsut.ac.in                                                                │\n",
       "│ GouravSingalgourav.co19@nsut.ac.in                                                               │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Android OS                                                                                       │\n",
       "│ Case Study                                                                                       │\n",
       "│ OPERATING  SYSTEMS                                                                               │\n",
       "│ Submitted By: -                                                                                  │\n",
       "│ Mayank Goelmayank.co19@nsut.ac.in                                                                │\n",
       "│ GouravSingalgourav.co19@nsut.ac.in                                                               │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2104.09487v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2104.09487v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.4417</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.4417\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Symposium on Operating Systems Principles (SOSP ’09) . 29–44.                                    │\n",
       "│ [11] Andrew Baumann, Gernot Heiser, Jonathan Appavoo, Dilma Da Silva,                            │\n",
       "│ Orran Krieger, Rob...                                                                            │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Symposium on Operating Systems Principles (SOSP ’09) . 29–44.                                    │\n",
       "│ [11] Andrew Baumann, Gernot Heiser, Jonathan Appavoo, Dilma Da Silva,                            │\n",
       "│ Orran Krieger, Rob...                                                                            │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2306.15076v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2306.15076v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3319</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3319\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Yew. 2006. Live Updating Operating Systems Using Virtualization. In                              │\n",
       "│ Proceedings of the 2nd International Conference on Virtual Execution En-                         │\n",
       "│ vironment...                                                                                     │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ Yew. 2006. Live Updating Operating Systems Using Virtualization. In                              │\n",
       "│ Proceedings of the 2nd International Conference on Virtual Execution En-                         │\n",
       "│ vironment...                                                                                     │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2306.15076v1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2306.15076v1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">--- Result </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\"> ---</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m--- Result \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1m ---\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Score: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.3313</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mScore: \u001b[0m\u001b[1;31m0.3313\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ AIOS: LLM Agent Operating System                                                                 │\n",
       "│ not using AIOS widens as the number of agents increases,                                         │\n",
       "│ underscoring AIOS’s effectiveness in managing concurrent                                         │\n",
       "│ ope...                                                                                           │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────── Content Preview ─────────────────────────────────────────╮\n",
       "│ AIOS: LLM Agent Operating System                                                                 │\n",
       "│ not using AIOS widens as the number of agents increases,                                         │\n",
       "│ underscoring AIOS’s effectiveness in managing concurrent                                         │\n",
       "│ ope...                                                                                           │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Source: </span><span style=\"color: #7f7fff; text-decoration-color: #7f7fff; text-decoration: underline\">https://arxiv.org/pdf/2403.16971v3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mSource: \u001b[0m\u001b[2;4;94mhttps://arxiv.org/pdf/2403.16971v3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Total unique documents:</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mTotal unique documents:\u001b[0m \u001b[1;36m11\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Generating combined answer based on multiple query results...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mGenerating combined answer based on multiple query results\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:39:19,069 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">This appears to be a general knowledge question. Using LLM knowledge.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mThis appears to be a general knowledge question. Using LLM knowledge.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:39:19,406 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'An operating system (OS) is a software that manages computer hardware resources and provides a platform for running application software. It acts as an intermediary between computer hardware and user-level applications.\\n\\nThe primary functions of an operating system include:\\n\\n1. Process management: Creating, scheduling, and terminating tasks (programs) to optimize system performance.\\n2. Memory management: Allocating and deallocating memory for running programs.\\n3. File system management: Managing files, directories, and storage devices.\\n4. Input/Output operations: Handling user input and output, such as keyboard and mouse interactions, display output, and printer management.\\n5. Security and access control: Regulating access to computer resources based on user identity and permissions.\\n6. Networking: Enabling communication between the computer and other devices on a network.\\n\\nOperating systems can be classified into several types, including:\\n\\n1. Single-user, single-tasking operating systems (e.g., MS-DOS)\\n2. Multi-user, multi-tasking operating systems (e.g., Windows XP, macOS)\\n3. Mobile operating systems (e.g., Android, iOS)\\n\\nThe operating system software provides a layer of abstraction between the hardware and user-level applications, allowing users to interact with the computer using high-level commands and programs, rather than directly accessing low-level hardware components.\\n\\nIn summary, an operating system is essential software that manages computer resources, provides a platform for running applications, and enables users to interact with their computers in a convenient and efficient manner.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:39:28,105 - langsmith.client - WARNING - Failed to send compressed multipart ingest: Connection error caused failure to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. Please confirm your internet connection. ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'api.smith.langchain.com\\', port=443): Max retries exceeded with url: /runs/multipart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x3142727d0>: Failed to resolve \\'api.smith.langchain.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))'))\n",
      "Content-Length: 1176\n",
      "API Key: lsv2_********************************************82\n",
      "2025-04-17 13:39:37,707 - langsmith.client - WARNING - Failed to send compressed multipart ingest: Connection error caused failure to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. Please confirm your internet connection. ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'api.smith.langchain.com\\', port=443): Max retries exceeded with url: /runs/multipart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x3190cf310>: Failed to resolve \\'api.smith.langchain.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))'))\n",
      "Content-Length: 6919\n",
      "API Key: lsv2_********************************************82\n"
     ]
    }
   ],
   "source": [
    "question = input(\"Enter your query\")\n",
    "assistant.answer_query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "065f4cf3-5efd-4f03-b2c0-08167871569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "k= []\n",
    "if k:\n",
    "    preint(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cae4ba5-ecf9-445b-9c1f-45a5d6438aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
